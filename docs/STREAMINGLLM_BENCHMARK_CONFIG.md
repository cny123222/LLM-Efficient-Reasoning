# StreamingLLM Benchmark å®éªŒé…ç½®è¯´æ˜

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜åœ¨ä½¿ç”¨ `scripts/benchmark.py` è¿›è¡Œ StreamingLLM åŸºå‡†æµ‹è¯•æ—¶ï¼Œæ‰€æœ‰å®éªŒç»„çš„é…ç½®ä»¥åŠ baseline çš„æµ‹é‡æ–¹å¼ã€‚

---

## ğŸ“Š å®éªŒç»„é…ç½®

### é»˜è®¤å‚æ•°

å½“è¿è¡Œä»¥ä¸‹å‘½ä»¤æ—¶ï¼š
```bash
python scripts/benchmark.py --method streaming_llm
```

é»˜è®¤å‚æ•°ä¸ºï¼š
- `--start_size`: `4` (attention sinks æ•°é‡)
- `--recent_sizes`: `"252,508,1020"` (æœ€è¿‘ tokens æ•°é‡åˆ—è¡¨)
- `--no_baseline`: `False` (é»˜è®¤åŒ…å« baseline)
- `--no_recent_only`: `False` (é»˜è®¤åŒ…å« recent_only æ§åˆ¶ç»„)

### ç”Ÿæˆçš„å®éªŒç»„

åŸºäºé»˜è®¤å‚æ•°ï¼Œä¼šç”Ÿæˆä»¥ä¸‹å®éªŒç»„ï¼š

| å®éªŒç»„åç§° | å‹ç¼©æ–¹æ³• | é…ç½®å‚æ•° | è¯´æ˜ |
|-----------|---------|---------|------|
| **baseline** | æ— å‹ç¼© | `compress_fn: None` | å®Œå…¨ä¿ç•™æ‰€æœ‰ KV cache |
| **recent_only_256** | æ»‘åŠ¨çª—å£ | `window_size: 256` | åªä¿ç•™æœ€è¿‘ 256 ä¸ª tokens |
| **streaming_256** | StreamingLLM | `start_size: 4, recent_size: 252` | 4 ä¸ª sinks + 252 ä¸ªæœ€è¿‘ tokens |
| **recent_only_512** | æ»‘åŠ¨çª—å£ | `window_size: 512` | åªä¿ç•™æœ€è¿‘ 512 ä¸ª tokens |
| **streaming_512** | StreamingLLM | `start_size: 4, recent_size: 508` | 4 ä¸ª sinks + 508 ä¸ªæœ€è¿‘ tokens |
| **recent_only_1024** | æ»‘åŠ¨çª—å£ | `window_size: 1024` | åªä¿ç•™æœ€è¿‘ 1024 ä¸ª tokens |
| **streaming_1024** | StreamingLLM | `start_size: 4, recent_size: 1020` | 4 ä¸ª sinks + 1020 ä¸ªæœ€è¿‘ tokens |

### å®éªŒç»„è¯´æ˜

#### 1. Baselineï¼ˆåŸºçº¿ï¼‰
- **é…ç½®**ï¼š`compress_fn: None`, `kwargs: {}`
- **å«ä¹‰**ï¼šä¸ä½¿ç”¨ä»»ä½• KV cache å‹ç¼©
- **KV Cache å¤§å°**ï¼šç­‰äºè¾“å…¥åºåˆ—é•¿åº¦ï¼ˆæ— é™åˆ¶å¢é•¿ï¼‰
- **ç”¨é€”**ï¼šä½œä¸ºæ€§èƒ½å’Œè´¨é‡çš„ä¸Šç•Œå‚è€ƒ

#### 2. Recent-Onlyï¼ˆæ»‘åŠ¨çª—å£æ§åˆ¶ç»„ï¼‰
- **é…ç½®**ï¼š`compress_fn: recent_only_compress`, `window_size: total_size`
- **å«ä¹‰**ï¼šåªä¿ç•™æœ€è¿‘ N ä¸ª tokensï¼Œä¸¢å¼ƒæ‰€æœ‰å†å² tokens
- **KV Cache å¤§å°**ï¼šå›ºå®šä¸º `window_size`
- **ç”¨é€”**ï¼šä½œä¸ºç®€å•æ»‘åŠ¨çª—å£çš„å¯¹æ¯”åŸºçº¿ï¼ŒéªŒè¯ attention sinks çš„é‡è¦æ€§

#### 3. StreamingLLMï¼ˆå®éªŒç»„ï¼‰
- **é…ç½®**ï¼š`compress_fn: streaming_llm_compress`, `start_size: 4, recent_size: X`
- **å«ä¹‰**ï¼šä¿ç•™ 4 ä¸ª attention sinks + æœ€è¿‘ X ä¸ª tokens
- **KV Cache å¤§å°**ï¼šå›ºå®šä¸º `start_size + recent_size`
- **ç”¨é€”**ï¼šéªŒè¯ StreamingLLM æ–¹æ³•ç›¸æ¯”çº¯æ»‘åŠ¨çª—å£çš„ä¼˜åŠ¿

### å®éªŒç»„ç”Ÿæˆé€»è¾‘

ä»£ç ä½ç½®ï¼š`scripts/benchmark.py` çš„ `build_methods_config()` å‡½æ•°

```python
elif args.method == "streaming_llm":
    recent_sizes = [int(x) for x in args.recent_sizes.split(",")]  # [252, 508, 1020]
    
    # 1. æ·»åŠ  recent_only æ§åˆ¶ç»„ï¼ˆå¦‚æœæœªç¦ç”¨ï¼‰
    if not args.no_recent_only:
        for recent_size in recent_sizes:
            total_size = args.start_size + recent_size  # 4 + 252 = 256, etc.
            methods.append({
                "name": f"recent_only_{total_size}",
                "compress_fn": recent_only_compress,
                "kwargs": {"window_size": total_size}
            })
    
    # 2. æ·»åŠ  StreamingLLM å®éªŒç»„
    for recent_size in recent_sizes:
        total_size = args.start_size + recent_size
        methods.append({
            "name": f"streaming_{total_size}",
            "compress_fn": streaming_llm_compress,
            "kwargs": {
                "start_size": args.start_size,  # 4
                "recent_size": recent_size      # 252, 508, 1020
            }
        })
```

---

## ğŸ”¬ Baseline æµ‹é‡æ–¹å¼

### Baseline çš„å®šä¹‰

**Baseline = æ— å‹ç¼©çš„åŸå§‹æ¨¡å‹æ¨ç†**

- **KV Cache å‹ç¼©**ï¼šæ— ï¼ˆ`compress_fn: None`ï¼‰
- **KV Cache å¤§å°**ï¼šç­‰äºå®Œæ•´è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆæ— é™åˆ¶ï¼‰
- **å‹ç¼©å‚æ•°**ï¼šç©ºï¼ˆ`kwargs: {}`ï¼‰

### Baseline çš„æµ‹é‡æµç¨‹

ä»£ç ä½ç½®ï¼š`kvcompress/benchmark.py` çš„ `measure_generation_metrics()` å‡½æ•°

#### 1. è¾“å…¥å¤„ç†
```python
# Tokenize è¾“å…¥æ–‡æœ¬
input_ids = tokenizer.encode(text, return_tensors="pt")
input_ids = input_ids[:, :max_input_tokens].to(device)
```

#### 2. Prefill é˜¶æ®µï¼ˆæµ‹é‡ TTFTï¼‰
```python
# ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼ˆprefillï¼‰
outputs = model(input_ids, use_cache=True, return_dict=True)

# è·å–ç¬¬ä¸€ä¸ªç”Ÿæˆçš„ token
next_token_logits = outputs.logits[:, -1, :]
next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

# è®°å½• TTFTï¼ˆTime To First Tokenï¼‰
ttft = time.perf_counter() - first_start
```

#### 3. KV Cache å¤„ç†ï¼ˆBaseline æ— å‹ç¼©ï¼‰
```python
# è·å– KV cache
past_key_values = outputs.past_key_values

# Baseline: compress_fn ä¸º Noneï¼Œä¸è¿›è¡Œä»»ä½•å‹ç¼©
if compress_fn is not None and past_key_values is not None:
    # å‹ç¼© KV cacheï¼ˆbaseline ä¸ä¼šæ‰§è¡Œè¿™é‡Œï¼‰
    compressed_kv = compress_fn(kv_list, skip_layers=skip_layers, **compress_kwargs)
    past_key_values = to_dynamic_cache(compressed_kv)
else:
    # Baseline ç›´æ¥ä½¿ç”¨åŸå§‹ KV cacheï¼Œä¸åšä»»ä½•å¤„ç†
    pass
```

#### 4. ç”Ÿæˆé˜¶æ®µï¼ˆæµ‹é‡ TPOT å’Œ Throughputï¼‰
```python
# é€ä¸ªç”Ÿæˆåç»­ tokens
for _ in range(max_new_tokens - 1):
    outputs = model(
        next_token,
        past_key_values=past_key_values,
        use_cache=True,
        return_dict=True
    )
    
    # è·å–ä¸‹ä¸€ä¸ª token
    next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1, keepdim=True)
    generated_tokens.append(next_token)
    
    # æ›´æ–° KV cacheï¼ˆbaseline ä»ç„¶ä¸å‹ç¼©ï¼‰
    past_key_values = outputs.past_key_values
    # Baseline: compress_fn ä¸º Noneï¼Œpast_key_values ä¿æŒä¸å˜
```

#### 5. æŒ‡æ ‡è®¡ç®—
```python
total_time = time.perf_counter() - total_start
num_tokens = len(generated_tokens)

# è®¡ç®—æŒ‡æ ‡
tpot = (total_time - ttft) / (num_tokens - 1)  # æ¯ä¸ªè¾“å‡º token çš„å¹³å‡æ—¶é—´
throughput = num_tokens / total_time              # tokens/ç§’
```

### Baseline æµ‹é‡çš„æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | æµ‹é‡æ–¹å¼ |
|------|------|---------|
| **TTFT** | Time To First Token | Prefill é˜¶æ®µçš„æ—¶é—´ |
| **TPOT** | Time Per Output Token | (æ€»æ—¶é—´ - TTFT) / (ç”Ÿæˆ tokens æ•° - 1) |
| **Throughput** | ååé‡ | ç”Ÿæˆ tokens æ•° / æ€»æ—¶é—´ |
| **PPL** | Perplexityï¼ˆå›°æƒ‘åº¦ï¼‰ | åœ¨è¯„ä¼°æ–‡æœ¬ä¸Šè®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶ |
| **Accuracy** | å‡†ç¡®ç‡ | Next token é¢„æµ‹å‡†ç¡®ç‡ |
| **Cache Size** | KV Cache å¤§å° | ç­‰äºè¾“å…¥åºåˆ—é•¿åº¦ï¼ˆæ— å‹ç¼©ï¼‰ |

### Baseline çš„ç‰¹æ®Šæ€§

1. **æ— å†…å­˜é™åˆ¶**ï¼šKV cache ä¼šéšç€åºåˆ—é•¿åº¦çº¿æ€§å¢é•¿
2. **æœ€ä½³è´¨é‡**ï¼šç†è®ºä¸Šæä¾›æœ€é«˜çš„ç”Ÿæˆè´¨é‡ï¼ˆPPL æœ€ä½ï¼ŒAccuracy æœ€é«˜ï¼‰
3. **é€Ÿåº¦å‚è€ƒ**ï¼šä½œä¸ºé€Ÿåº¦çš„ä¸Šç•Œï¼ˆæ— å‹ç¼©å¼€é”€ï¼‰
4. **å¯¹æ¯”åŸºå‡†**ï¼šæ‰€æœ‰å‹ç¼©æ–¹æ³•éƒ½ä¸ä¹‹å¯¹æ¯”ï¼Œè®¡ç®—æ€§èƒ½æŸå¤±

---

## ğŸ“ˆ å®éªŒå¯¹æ¯”é€»è¾‘

### å¯¹æ¯”æŒ‡æ ‡

åœ¨ `scripts/benchmark.py` çš„ `main()` å‡½æ•°ä¸­ï¼Œä¼šè®¡ç®—æ¯ä¸ªå®éªŒç»„ç›¸å¯¹äº baseline çš„å˜åŒ–ï¼š

```python
# è®¡ç®—ç›¸å¯¹ baseline çš„å˜åŒ–ç™¾åˆ†æ¯”
throughput_imp = (avg_throughput / baseline_throughput - 1) * 100  # ååé‡æå‡
tpot_imp = (1 - avg_tpot / baseline_tpot) * 100                     # TPOT é™ä½
ppl_change = (avg_ppl / baseline_ppl - 1) * 100                     # PPL å˜åŒ–
acc_change = (avg_acc / baseline_acc - 1) * 100                     # å‡†ç¡®ç‡å˜åŒ–
```

### è¾“å‡ºç¤ºä¾‹

```
Comparison with baseline (Throughput â†‘ better, TPOT â†“ better, PPL â†“ better):
  recent_only_256: Throughput -15.2%, TPOT +18.5%, PPL +12.3%, Acc -3.1%
  streaming_256: Throughput -8.5%, TPOT +9.2%, PPL +5.1%, Acc -1.2%
  streaming_512: Throughput -3.2%, TPOT +3.5%, PPL +2.1%, Acc -0.5%
  streaming_1024: Throughput -1.1%, TPOT +1.2%, PPL +0.8%, Acc -0.2%
```

---

## ğŸ¯ å®éªŒè®¾è®¡æ„å›¾

### ä¸ºä»€ä¹ˆéœ€è¦ Baselineï¼Ÿ

1. **æ€§èƒ½ä¸Šç•Œ**ï¼šæä¾›æ— å‹ç¼©æƒ…å†µä¸‹çš„æœ€ä½³æ€§èƒ½å‚è€ƒ
2. **è´¨é‡åŸºå‡†**ï¼šä½œä¸ºç”Ÿæˆè´¨é‡ï¼ˆPPL, Accuracyï¼‰çš„ä¸Šç•Œ
3. **å¯¹æ¯”æ ‡å‡†**ï¼šæ‰€æœ‰å‹ç¼©æ–¹æ³•éƒ½ä¸ä¹‹å¯¹æ¯”ï¼Œé‡åŒ–æ€§èƒ½æŸå¤±

### ä¸ºä»€ä¹ˆéœ€è¦ Recent-Only æ§åˆ¶ç»„ï¼Ÿ

1. **éªŒè¯ Attention Sinks çš„é‡è¦æ€§**ï¼šå¯¹æ¯” StreamingLLM å’Œçº¯æ»‘åŠ¨çª—å£
2. **å…¬å¹³å¯¹æ¯”**ï¼šç¡®ä¿åœ¨ç›¸åŒçš„ KV cache å¤§å°ä¸‹å¯¹æ¯”
3. **æ–¹æ³•éªŒè¯**ï¼šè¯æ˜ä¿ç•™ attention sinks çš„ä»·å€¼

### ä¸ºä»€ä¹ˆæµ‹è¯•å¤šä¸ª Cache å¤§å°ï¼Ÿ

1. **æƒè¡¡åˆ†æ**ï¼šæ¢ç´¢é€Ÿåº¦ vs è´¨é‡çš„æƒè¡¡æ›²çº¿
2. **æœ€ä¼˜é…ç½®**ï¼šæ‰¾åˆ°åœ¨å¯æ¥å—è´¨é‡æŸå¤±ä¸‹çš„æœ€ä½³ cache å¤§å°
3. **å®ç”¨æ€§**ï¼šä¸åŒåœºæ™¯å¯èƒ½éœ€è¦ä¸åŒçš„ cache å¤§å°

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´å®éªŒï¼ˆåŒ…å«æ‰€æœ‰ç»„ï¼‰

```bash
python scripts/benchmark.py \
    --method streaming_llm \
    --start_size 4 \
    --recent_sizes 252,508,1020 \
    --model_id /mnt/disk1/models/pythia-2.8b \
    --num_samples 3 \
    --max_tokens 2000
```

### åªæµ‹è¯• StreamingLLMï¼ˆä¸åŒ…å« recent_onlyï¼‰

```bash
python scripts/benchmark.py \
    --method streaming_llm \
    --start_size 4 \
    --recent_sizes 252,508,1020 \
    --no_recent_only \
    --model_id /mnt/disk1/models/pythia-2.8b
```

### ä¸åŒ…å« Baselineï¼ˆåªå¯¹æ¯”å‹ç¼©æ–¹æ³•ï¼‰

```bash
python scripts/benchmark.py \
    --method streaming_llm \
    --start_size 4 \
    --recent_sizes 252,508,1020 \
    --no_baseline \
    --model_id /mnt/disk1/models/pythia-2.8b
```

---

## ğŸ” å…³é”®ä»£ç ä½ç½®

1. **å®éªŒç»„é…ç½®**ï¼š`scripts/benchmark.py` â†’ `build_methods_config()` (line 284-310)
2. **Baseline å®šä¹‰**ï¼š`scripts/benchmark.py` â†’ `build_methods_config()` (line 232-236)
3. **æŒ‡æ ‡æµ‹é‡**ï¼š`kvcompress/benchmark.py` â†’ `measure_generation_metrics()` (line 23-150)
4. **å¯¹æ¯”è®¡ç®—**ï¼š`scripts/benchmark.py` â†’ `main()` (line 549-567)

---

*æ–‡æ¡£ç”Ÿæˆæ—¶é—´: 2024*

