"""
Tree-based Speculative Decoding Generator (SpecInfer-style)

This module implements tree-based speculative decoding, which generates a tree
of candidate tokens using the draft model and verifies them in parallel using
tree attention with the target model.

Key Innovations over Linear Speculative Decoding:
1. Multiple candidates per position (top-k sampling)
2. Tree attention mask for parallel verification of all branches
3. Higher acceptance probability due to multiple paths
4. Better GPU utilization through larger batch verification

Algorithm:
1. Draft model generates a token tree with multiple branches
2. Tree is flattened and verified with target model using tree attention
3. Best valid path is selected (longest matching path)
4. KV cache is updated with the accepted path

References:
- SpecInfer: Accelerating Generative LLM Serving (ASPLOS 2024)
- Medusa: Simple Framework for Accelerating LLM Generation (2024)
"""

import torch
import torch.nn.functional as F
from typing import Tuple, Optional, List, Dict
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel
from transformers.cache_utils import DynamicCache

from .speculative_generator import SpeculativeGenerator
from .token_tree import TokenTree, TreeNode


class TreeSpeculativeGenerator(SpeculativeGenerator):
    """
    Tree-based Speculative Decoding Generator.
    
    This generator extends the basic SpeculativeGenerator with tree-based
    drafting, where multiple candidate tokens are generated at each position
    and verified in parallel using tree attention.
    
    Args:
        target_model: Large model for verification (e.g., Pythia-2.8B)
        draft_model: Small model for drafting (e.g., Pythia-70M)
        tokenizer: Tokenizer for the models
        tree_depth: Depth of the token tree (equivalent to K in linear)
        branch_factor: Number of branches at each node (top-k candidates)
        max_tree_nodes: Maximum number of nodes in the tree
        max_len: Maximum total sequence length
        device: Device to run on
        use_compile: Whether to use torch.compile optimization
        
    Example:
        >>> generator = TreeSpeculativeGenerator(
        ...     target_model=target_model,
        ...     draft_model=draft_model,
        ...     tokenizer=tokenizer,
        ...     tree_depth=4,
        ...     branch_factor=2,  # Binary tree
        ... )
        >>> output = generator.generate("Hello, world!", max_new_tokens=100)
    """
    
    def __init__(
        self,
        target_model: PreTrainedModel,
        draft_model: PreTrainedModel,
        tokenizer: AutoTokenizer,
        tree_depth: int = 4,
        branch_factor: int = 2,
        max_tree_nodes: int = 32,
        max_len: int = 2048,
        device: str = "cuda",
        use_compile: bool = True
    ):
        # Initialize parent with equivalent K for stats
        super().__init__(
            target_model=target_model,
            draft_model=draft_model,
            tokenizer=tokenizer,
            K=tree_depth,  # K represents depth for tree
            max_len=max_len,
            device=device,
            use_compile=use_compile
        )
        
        # Tree-specific parameters
        self.tree_depth = tree_depth
        self.branch_factor = branch_factor
        self.max_tree_nodes = max_tree_nodes
        
        # Calculate max possible nodes: sum of branch_factor^i for i=0 to tree_depth
        # For binary tree depth 4: 1 + 2 + 4 + 8 + 16 = 31
        theoretical_max = sum(branch_factor ** i for i in range(tree_depth + 1))
        if max_tree_nodes < theoretical_max:
            # Limit will be enforced by pruning
            pass
        
        # Token tree for current round
        self._token_tree: Optional[TokenTree] = None
        
        # Extended statistics for tree-based decoding
        self.tree_stats = {
            "total_tree_nodes": 0,
            "total_paths_explored": 0,
            "avg_accepted_path_length": 0.0,
            "max_path_length_achieved": 0,
        }
    
    def reset(self):
        """Reset the generator state for a new generation session."""
        super().reset()
        self._token_tree = None
        self.tree_stats = {
            "total_tree_nodes": 0,
            "total_paths_explored": 0,
            "avg_accepted_path_length": 0.0,
            "max_path_length_achieved": 0,
        }
    
    @torch.inference_mode()
    def _draft_tree_tokens(self) -> TokenTree:
        """
        Generate a tree of draft tokens using the draft model.
        
        The draft model generates multiple candidates (top-k) at each position,
        creating a tree structure. Each level of the tree is generated by
        expanding all leaves from the previous level.
        
        Returns:
            TokenTree: Tree structure containing all draft candidates
        """
        # Create new token tree
        tree = TokenTree(
            max_depth=self.tree_depth,
            branch_factor=self.branch_factor,
            max_nodes=self.max_tree_nodes,
            device=self.device
        )
        
        # Re-prefill draft model with current sequence
        draft_outputs = self.draft_model(
            input_ids=self.current_ids,
            use_cache=True,
            return_dict=True
        )
        
        # Get logits for first draft position
        first_logits = draft_outputs.logits[:, -1, :]  # [1, vocab]
        log_probs = F.log_softmax(first_logits, dim=-1)
        
        # Get top-k candidates for root level
        topk_probs, topk_tokens = torch.topk(log_probs[0], self.branch_factor)
        
        # Add root node (most likely token)
        tree.add_root(
            token_id=topk_tokens[0].item(),
            logit=topk_probs[0].item()
        )
        
        # Add sibling roots if branch_factor > 1 and we have capacity
        # For simplicity, we use a single root but expand with multiple children
        # This is the "wide tree" approach
        
        # Initialize caches for each leaf node
        # We'll process level by level
        current_leaves = [0]  # Start with root
        leaf_caches = [draft_outputs.past_key_values]  # Cache for root's parent context
        leaf_tokens = [topk_tokens[0:1]]  # Token for root [1]
        
        # Build tree level by level
        for depth in range(1, self.tree_depth + 1):
            if len(tree) >= self.max_tree_nodes:
                break
            
            new_leaves = []
            new_caches = []
            new_tokens = []
            
            # Process each current leaf
            for leaf_idx, leaf_cache, leaf_token in zip(current_leaves, leaf_caches, leaf_tokens):
                # Forward the leaf token to get predictions for next level
                draft_outputs = self.draft_model(
                    input_ids=leaf_token.unsqueeze(0),  # [1, 1]
                    past_key_values=leaf_cache,
                    use_cache=True,
                    return_dict=True
                )
                
                next_logits = draft_outputs.logits[:, -1, :]  # [1, vocab]
                log_probs = F.log_softmax(next_logits, dim=-1)
                
                # Get top-k candidates
                topk_probs, topk_tokens = torch.topk(log_probs[0], self.branch_factor)
                
                # Add children to tree (limit by remaining capacity)
                remaining_capacity = self.max_tree_nodes - len(tree)
                num_children = min(self.branch_factor, remaining_capacity)
                
                if num_children <= 0:
                    break
                
                for i in range(num_children):
                    child_idx = tree.add_node(
                        token_id=topk_tokens[i].item(),
                        parent_idx=leaf_idx,
                        logit=topk_probs[i].item()
                    )
                    new_leaves.append(child_idx)
                    
                    # Clone cache for this branch (important for independent branches)
                    # For efficiency, we share the cache prefix and only diverge on new tokens
                    new_caches.append(draft_outputs.past_key_values)
                    new_tokens.append(topk_tokens[i:i+1])
            
            if not new_leaves:
                break
            
            current_leaves = new_leaves
            leaf_caches = new_caches
            leaf_tokens = new_tokens
        
        self._token_tree = tree
        return tree
    
    @torch.inference_mode()
    def _verify_tree_tokens(
        self,
        tree: TokenTree
    ) -> Tuple[torch.Tensor, torch.Tensor, object]:
        """
        Verify all tree tokens with the target model using tree attention.
        
        The tree is flattened into a sequence and verified with a special
        attention mask that encodes the tree structure. Each token can only
        attend to its ancestors in the tree.
        
        Args:
            tree: TokenTree containing draft candidates
            
        Returns:
            target_logits: Logits from target model [num_nodes, vocab_size]
            attention_mask: Tree attention mask used
            verify_outputs: Original model outputs
        """
        # Flatten tree for verification
        tree_tokens, node_indices = tree.flatten_for_verification()
        
        # Get current cache length (prefix length)
        prefix_len = self.target_cache.get_seq_length()
        
        # Build tree attention mask
        # Shape: [num_nodes, prefix_len + num_nodes]
        tree_mask = tree.build_tree_attention_mask(prefix_len=prefix_len)
        
        # Convert to the format expected by HuggingFace models
        # HuggingFace expects: [batch, 1, seq_len, total_len] for 4D
        # or [batch, seq_len] for 2D causal
        # For tree attention, we need 4D mask
        num_nodes = len(tree_tokens)
        total_len = prefix_len + num_nodes
        
        # Create 4D attention mask [1, 1, num_nodes, total_len]
        # Convert bool mask to float: True -> 0.0, False -> -inf
        attention_mask_4d = torch.zeros(
            (1, 1, num_nodes, total_len),
            dtype=self.target_model.dtype if hasattr(self.target_model, 'dtype') else torch.float16,
            device=self.device
        )
        attention_mask_4d[0, 0] = torch.where(
            tree_mask,
            torch.tensor(0.0, device=self.device),
            torch.tensor(float('-inf'), device=self.device)
        )
        
        # Forward tree tokens through target model
        tree_tokens_input = tree_tokens.unsqueeze(0)  # [1, num_nodes]
        
        try:
            outputs = self.target_model(
                input_ids=tree_tokens_input,
                past_key_values=self.target_cache,
                attention_mask=attention_mask_4d,
                use_cache=True,
                return_dict=True
            )
        except Exception as e:
            # Fallback: some models don't support 4D attention mask
            # Use sequential verification instead
            return self._verify_tree_sequential(tree)
        
        # Extract logits for each position
        # outputs.logits shape: [1, num_nodes, vocab_size]
        target_logits = outputs.logits[0]  # [num_nodes, vocab_size]
        
        # Store the new cache (will be adjusted later based on accepted path)
        self._verify_cache = outputs.past_key_values
        
        return target_logits, tree_mask, outputs
    
    @torch.inference_mode()
    def _verify_tree_sequential(
        self,
        tree: TokenTree
    ) -> Tuple[torch.Tensor, torch.Tensor, object]:
        """
        Fallback sequential verification for models that don't support tree attention.
        
        This verifies each path independently, which is slower but more compatible.
        """
        tree_tokens, node_indices = tree.flatten_for_verification()
        num_nodes = len(tree_tokens)
        vocab_size = self.target_model.config.vocab_size
        
        # Initialize logits storage
        target_logits = torch.zeros(
            (num_nodes, vocab_size),
            dtype=torch.float32,
            device=self.device
        )
        
        # Verify each leaf path
        leaves = tree.get_leaf_nodes()
        
        for leaf_idx in leaves:
            path = tree.get_path_to_root(leaf_idx)
            path_tokens = torch.tensor(
                [tree.nodes[idx].token_id for idx in path],
                dtype=torch.long,
                device=self.device
            ).unsqueeze(0)  # [1, path_len]
            
            # Forward path with standard causal attention
            outputs = self.target_model(
                input_ids=path_tokens,
                past_key_values=self.target_cache,
                use_cache=False,  # Don't update cache yet
                return_dict=True
            )
            
            # Store logits for positions in this path
            for i, node_idx in enumerate(path):
                pos = tree.nodes[node_idx].position_in_sequence
                target_logits[pos] = outputs.logits[0, i]
        
        # Build mask for compatibility
        prefix_len = self.target_cache.get_seq_length()
        tree_mask = tree.build_tree_attention_mask(prefix_len=prefix_len)
        
        return target_logits, tree_mask, None
    
    def _select_best_path(
        self,
        tree: TokenTree,
        target_logits: torch.Tensor
    ) -> Tuple[List[int], int, torch.Tensor]:
        """
        Select the best (longest valid) path from the verified tree.
        
        A path is valid if each token matches the target model's prediction
        for its parent position. We use greedy selection to find the longest
        such path.
        
        Args:
            tree: TokenTree with draft candidates
            target_logits: Logits from target model [num_nodes, vocab_size]
            
        Returns:
            accepted_path: List of node indices for the accepted path
            num_accepted: Number of accepted tokens
            accepted_tokens: Tensor of accepted token IDs [num_accepted]
        """
        # Get predictions from target model
        target_preds = target_logits.argmax(dim=-1)  # [num_nodes]
        
        # Start from _last_target_logits for first position
        first_pred = self._last_target_logits.squeeze().argmax(dim=-1).item()
        
        # Find valid paths using DFS
        best_path = []
        best_length = 0
        
        def dfs_find_valid_path(node_idx: int, current_path: List[int], expected_token: int):
            """DFS to find longest valid path."""
            nonlocal best_path, best_length
            
            node = tree.nodes[node_idx]
            
            # Check if this node's token matches expected
            if node.token_id != expected_token:
                # Mismatch - path ends here, but we can still use target's prediction
                # Return path up to parent + target's prediction
                if len(current_path) > best_length:
                    best_path = current_path.copy()
                    best_length = len(current_path)
                return
            
            # Token matches - add to path
            current_path.append(node_idx)
            
            if node.is_leaf():
                # Reached a leaf - this is a complete valid path
                if len(current_path) > best_length:
                    best_path = current_path.copy()
                    best_length = len(current_path)
            else:
                # Continue to children
                pos = node.position_in_sequence
                next_expected = target_preds[pos].item()
                
                for child_idx in node.children_idx:
                    dfs_find_valid_path(child_idx, current_path.copy(), next_expected)
            
            # Also update best if current path is longest so far
            if len(current_path) > best_length:
                best_path = current_path.copy()
                best_length = len(current_path)
        
        # Start DFS from root
        root_idx = 0
        dfs_find_valid_path(root_idx, [], first_pred)
        
        # Handle case where even root doesn't match
        if best_length == 0:
            # Use target's prediction as the single accepted token
            # num_accepted should be 1 since we're accepting target's prediction
            accepted_tokens = torch.tensor([[first_pred]], dtype=torch.long, device=self.device)
            return [], 1, accepted_tokens
        
        # Extract accepted tokens from path
        accepted_token_ids = [tree.nodes[idx].token_id for idx in best_path]
        
        # Add bonus token if we accepted the full depth
        last_node = tree.nodes[best_path[-1]]
        if last_node.depth == self.tree_depth or last_node.is_leaf():
            # Get bonus token from target's prediction for last position
            last_pos = last_node.position_in_sequence
            bonus_token = target_preds[last_pos].item()
            accepted_token_ids.append(bonus_token)
        else:
            # Didn't reach full depth - add target's prediction for mismatch position
            last_pos = last_node.position_in_sequence
            next_target_pred = target_preds[last_pos].item()
            accepted_token_ids.append(next_target_pred)
        
        accepted_tokens = torch.tensor(
            [accepted_token_ids],
            dtype=torch.long,
            device=self.device
        )
        
        return best_path, len(accepted_tokens[0]), accepted_tokens
    
    @torch.inference_mode()
    def _update_tree_cache(
        self,
        accepted_path: List[int],
        accepted_tokens: torch.Tensor,
        original_cache_len: int
    ):
        """
        Update the KV cache with the accepted path tokens.
        
        Since tree verification may have added KV entries for non-accepted
        branches, we need to rebuild the cache with only the accepted path.
        
        Args:
            accepted_path: List of node indices in the accepted path
            accepted_tokens: Tensor of accepted token IDs [1, num_accepted]
            original_cache_len: Cache length before this round
        """
        # Rollback cache to before tree verification
        self.target_cache.crop(original_cache_len)
        
        # Forward accepted tokens to rebuild cache
        if accepted_tokens.shape[1] > 0:
            outputs = self.target_model(
                input_ids=accepted_tokens,
                past_key_values=self.target_cache,
                use_cache=True,
                return_dict=True
            )
            
            self.target_cache = outputs.past_key_values
            self._last_target_logits = outputs.logits[:, -1:, :]
    
    @torch.inference_mode()
    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 100,
        verbose: bool = False
    ) -> str:
        """
        Generate text using tree-based speculative decoding.
        
        Args:
            prompt: Input text prompt
            max_new_tokens: Maximum number of new tokens to generate
            verbose: Whether to print progress information
            
        Returns:
            Generated text (including prompt)
        """
        # Reset state
        self.reset()
        
        # Tokenize prompt
        input_ids = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=self.max_len - max_new_tokens
        ).input_ids.to(self.device)
        
        # Prefill
        self._prefill(input_ids)
        
        if verbose:
            print(f"Prefilled {input_ids.shape[1]} tokens")
            print(f"Tree config: depth={self.tree_depth}, branch_factor={self.branch_factor}")
        
        generated = 0
        eos_token_id = self.tokenizer.eos_token_id
        
        while generated < max_new_tokens:
            # Check if approaching max length
            if self.current_ids.shape[1] + self.tree_depth >= self.max_len:
                break
            
            # Record cache length before this round
            original_cache_len = self.target_cache.get_seq_length()
            
            # Phase 1: Draft tree of tokens
            tree = self._draft_tree_tokens()
            self.tree_stats["total_tree_nodes"] += len(tree)
            self.stats["total_draft_tokens"] += len(tree)
            
            if verbose:
                print(f"  Draft tree: {len(tree)} nodes, depth {tree.max_depth}")
            
            # Phase 2: Verify tree with target model
            target_logits, attention_mask, verify_outputs = self._verify_tree_tokens(tree)
            
            # Phase 3: Select best path
            accepted_path, num_accepted, accepted_tokens = self._select_best_path(
                tree, target_logits
            )
            
            self.tree_stats["total_paths_explored"] += len(tree.get_leaf_nodes())
            
            # Limit to max_new_tokens
            remaining = max_new_tokens - generated
            if num_accepted > remaining:
                accepted_tokens = accepted_tokens[:, :remaining]
                num_accepted = remaining
            
            if num_accepted == 0:
                break
            
            # Phase 4: Update cache with accepted path
            self._update_tree_cache(accepted_path, accepted_tokens, original_cache_len)
            
            # Update sequence
            self.current_ids = torch.cat([self.current_ids, accepted_tokens], dim=-1)
            self.stats["total_accepted"] += num_accepted
            self.stats["total_tokens"] += num_accepted
            self.stats["total_rounds"] += 1
            generated += num_accepted
            
            # Update tree stats
            if num_accepted > self.tree_stats["max_path_length_achieved"]:
                self.tree_stats["max_path_length_achieved"] = num_accepted
            
            if verbose:
                print(f"Round {self.stats['total_rounds']}: accepted {num_accepted} tokens, total: {generated}")
            
            # Check for EOS
            if accepted_tokens[0, -1].item() == eos_token_id:
                if verbose:
                    print("Generated EOS token, stopping.")
                break
        
        # Update average path length
        if self.stats["total_rounds"] > 0:
            self.tree_stats["avg_accepted_path_length"] = (
                self.stats["total_tokens"] / self.stats["total_rounds"]
            )
        
        return self.tokenizer.decode(self.current_ids[0], skip_special_tokens=True)
    
    def get_stats(self) -> dict:
        """Get generation statistics including tree-specific stats."""
        stats = super().get_stats()
        stats.update(self.tree_stats)
        stats["tree_depth"] = self.tree_depth
        stats["branch_factor"] = self.branch_factor
        return stats


class TreeSpeculativeGeneratorV2(TreeSpeculativeGenerator):
    """
    Optimized Tree-based Speculative Decoding Generator.
    
    This version includes additional optimizations:
    1. Adaptive tree pruning based on cumulative probability
    2. Early termination when high-confidence path is found
    3. Better cache management for partial tree reuse
    """
    
    def __init__(
        self,
        target_model: PreTrainedModel,
        draft_model: PreTrainedModel,
        tokenizer: AutoTokenizer,
        tree_depth: int = 4,
        branch_factor: int = 2,
        max_tree_nodes: int = 32,
        max_len: int = 2048,
        device: str = "cuda",
        use_compile: bool = True,
        probability_threshold: float = 0.1,  # Prune branches with prob < threshold
    ):
        super().__init__(
            target_model=target_model,
            draft_model=draft_model,
            tokenizer=tokenizer,
            tree_depth=tree_depth,
            branch_factor=branch_factor,
            max_tree_nodes=max_tree_nodes,
            max_len=max_len,
            device=device,
            use_compile=use_compile
        )
        
        self.probability_threshold = probability_threshold
    
    @torch.inference_mode()
    def _draft_tree_tokens(self) -> TokenTree:
        """
        Generate tree with probability-based pruning.
        
        Branches with cumulative probability below threshold are pruned
        to focus computation on more likely sequences.
        """
        tree = TokenTree(
            max_depth=self.tree_depth,
            branch_factor=self.branch_factor,
            max_nodes=self.max_tree_nodes,
            device=self.device
        )
        
        # Re-prefill draft model
        draft_outputs = self.draft_model(
            input_ids=self.current_ids,
            use_cache=True,
            return_dict=True
        )
        
        first_logits = draft_outputs.logits[:, -1, :]
        log_probs = F.log_softmax(first_logits, dim=-1)
        topk_probs, topk_tokens = torch.topk(log_probs[0], self.branch_factor)
        
        # Add root
        tree.add_root(topk_tokens[0].item(), topk_probs[0].item())
        
        # Track active leaves (those above probability threshold)
        active_leaves = [(0, draft_outputs.past_key_values, topk_tokens[0:1])]
        
        for depth in range(1, self.tree_depth + 1):
            if len(tree) >= self.max_tree_nodes or not active_leaves:
                break
            
            new_active_leaves = []
            
            for leaf_idx, leaf_cache, leaf_token in active_leaves:
                leaf_node = tree.nodes[leaf_idx]
                
                # Check cumulative probability threshold
                if leaf_node.cumulative_logit < torch.log(
                    torch.tensor(self.probability_threshold)
                ).item():
                    continue  # Prune this branch
                
                # Forward
                draft_outputs = self.draft_model(
                    input_ids=leaf_token.unsqueeze(0),
                    past_key_values=leaf_cache,
                    use_cache=True,
                    return_dict=True
                )
                
                next_logits = draft_outputs.logits[:, -1, :]
                log_probs = F.log_softmax(next_logits, dim=-1)
                topk_probs, topk_tokens = torch.topk(log_probs[0], self.branch_factor)
                
                remaining = self.max_tree_nodes - len(tree)
                num_children = min(self.branch_factor, remaining)
                
                for i in range(num_children):
                    child_idx = tree.add_node(
                        token_id=topk_tokens[i].item(),
                        parent_idx=leaf_idx,
                        logit=topk_probs[i].item()
                    )
                    new_active_leaves.append(
                        (child_idx, draft_outputs.past_key_values, topk_tokens[i:i+1])
                    )
            
            active_leaves = new_active_leaves
        
        self._token_tree = tree
        return tree


class TreeStreamingSpeculativeGenerator(TreeSpeculativeGenerator):
    """
    Tree-based Speculative Decoding with StreamingLLM KV Cache Compression.
    
    This generator combines tree-based drafting with StreamingLLM cache
    compression for infinite-length generation with constant memory usage
    while maintaining the throughput benefits of tree-based speculation.
    
    Args:
        target_model: Large model for verification
        draft_model: Small model for drafting
        tokenizer: Tokenizer for the models
        tree_depth: Depth of the token tree
        branch_factor: Number of branches at each node
        max_tree_nodes: Maximum number of nodes in the tree
        max_len: Maximum total sequence length
        device: Device to run on
        use_compile: Whether to use torch.compile
        start_size: Number of initial tokens to keep as attention sinks
        recent_size: Number of recent tokens to keep
        compress_threshold: Fraction of max_cache_len to trigger compression
        max_cache_len: Maximum cache length before compression
    """
    
    def __init__(
        self,
        target_model: PreTrainedModel,
        draft_model: PreTrainedModel,
        tokenizer: AutoTokenizer,
        tree_depth: int = 4,
        branch_factor: int = 2,
        max_tree_nodes: int = 32,
        max_len: int = 2048,
        device: str = "cuda",
        use_compile: bool = True,
        start_size: int = 4,
        recent_size: int = 1020,
        compress_threshold: float = 0.9,
        max_cache_len: int = 1024
    ):
        super().__init__(
            target_model=target_model,
            draft_model=draft_model,
            tokenizer=tokenizer,
            tree_depth=tree_depth,
            branch_factor=branch_factor,
            max_tree_nodes=max_tree_nodes,
            max_len=max_len,
            device=device,
            use_compile=use_compile
        )
        
        # StreamingLLM parameters
        self.start_size = start_size
        self.recent_size = recent_size
        self.max_cache_len = max_cache_len
        self.compress_threshold = compress_threshold
        
        # Effective cache size
        self._effective_cache_size = start_size + recent_size
        
        # Compression statistics
        self._compression_count = 0
        self._tokens_evicted = 0
        
        # Valid token IDs for draft model after compression
        self._valid_token_ids: Optional[torch.Tensor] = None
    
    def reset(self):
        """Reset generator state including compression stats."""
        super().reset()
        self._compression_count = 0
        self._tokens_evicted = 0
        self._valid_token_ids = None
    
    def _get_cache_length(self) -> int:
        """Get current cache length."""
        if self.target_cache is None:
            return 0
        try:
            return self.target_cache.get_seq_length()
        except:
            if len(self.target_cache) > 0:
                return self.target_cache[0][0].shape[2]
            return 0
    
    def _maybe_compress_cache(self):
        """Check and apply StreamingLLM compression if needed."""
        # Import here to avoid circular dependency
        import sys
        import os
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
        from kvcompress.methods.streaming_llm import streaming_llm_compress
        
        cache_len = self._get_cache_length()
        threshold = int(self.max_cache_len * self.compress_threshold)
        
        if cache_len > threshold:
            target_size = self.start_size + self.recent_size
            tokens_to_evict = cache_len - target_size
            
            if tokens_to_evict > 0:
                # Apply StreamingLLM compression
                compressed_list = streaming_llm_compress(
                    self.target_cache,
                    start_size=self.start_size,
                    recent_size=self.recent_size
                )
                
                # Convert back to DynamicCache
                cache = DynamicCache()
                for layer_idx, (keys, values) in enumerate(compressed_list):
                    cache.update(keys, values, layer_idx)
                self.target_cache = cache
                
                # Update statistics
                self._compression_count += 1
                self._tokens_evicted += tokens_to_evict
                
                # Update valid token IDs
                self._update_valid_token_ids()
    
    def _update_valid_token_ids(self):
        """Update valid token IDs after compression."""
        if self.current_ids is None or self.current_ids.shape[1] == 0:
            return
        
        total_len = self.current_ids.shape[1]
        
        if total_len <= self._effective_cache_size:
            self._valid_token_ids = None
            return
        
        # Build valid token IDs: initial + recent
        all_tokens = self.current_ids[0].tolist()
        initial_tokens = all_tokens[:self.start_size]
        recent_tokens = all_tokens[-self.recent_size:]
        
        valid_tokens = initial_tokens + recent_tokens
        self._valid_token_ids = torch.tensor(
            valid_tokens, device=self.device
        ).unsqueeze(0)
    
    @torch.inference_mode()
    def _draft_tree_tokens(self) -> TokenTree:
        """
        Generate tree of draft tokens, using compressed token IDs if available.
        """
        tree = TokenTree(
            max_depth=self.tree_depth,
            branch_factor=self.branch_factor,
            max_nodes=self.max_tree_nodes,
            device=self.device
        )
        
        # Use valid token IDs if cache was compressed
        prefill_ids = self._valid_token_ids if self._valid_token_ids is not None else self.current_ids
        
        # Re-prefill draft model
        draft_outputs = self.draft_model(
            input_ids=prefill_ids,
            use_cache=True,
            return_dict=True
        )
        
        first_logits = draft_outputs.logits[:, -1, :]
        log_probs = F.log_softmax(first_logits, dim=-1)
        topk_probs, topk_tokens = torch.topk(log_probs[0], self.branch_factor)
        
        tree.add_root(topk_tokens[0].item(), topk_probs[0].item())
        
        current_leaves = [0]
        leaf_caches = [draft_outputs.past_key_values]
        leaf_tokens = [topk_tokens[0:1]]
        
        for depth in range(1, self.tree_depth + 1):
            if len(tree) >= self.max_tree_nodes:
                break
            
            new_leaves = []
            new_caches = []
            new_tokens = []
            
            for leaf_idx, leaf_cache, leaf_token in zip(current_leaves, leaf_caches, leaf_tokens):
                draft_outputs = self.draft_model(
                    input_ids=leaf_token.unsqueeze(0),
                    past_key_values=leaf_cache,
                    use_cache=True,
                    return_dict=True
                )
                
                next_logits = draft_outputs.logits[:, -1, :]
                log_probs = F.log_softmax(next_logits, dim=-1)
                topk_probs, topk_tokens = torch.topk(log_probs[0], self.branch_factor)
                
                remaining = self.max_tree_nodes - len(tree)
                num_children = min(self.branch_factor, remaining)
                
                if num_children <= 0:
                    break
                
                for i in range(num_children):
                    child_idx = tree.add_node(
                        token_id=topk_tokens[i].item(),
                        parent_idx=leaf_idx,
                        logit=topk_probs[i].item()
                    )
                    new_leaves.append(child_idx)
                    new_caches.append(draft_outputs.past_key_values)
                    new_tokens.append(topk_tokens[i:i+1])
            
            if not new_leaves:
                break
            
            current_leaves = new_leaves
            leaf_caches = new_caches
            leaf_tokens = new_tokens
        
        self._token_tree = tree
        return tree
    
    @torch.inference_mode()
    def _update_tree_cache(
        self,
        accepted_path: List[int],
        accepted_tokens: torch.Tensor,
        original_cache_len: int
    ):
        """
        Update cache with accepted path, then check for compression.
        """
        # Call parent implementation
        super()._update_tree_cache(accepted_path, accepted_tokens, original_cache_len)
        
        # Check if compression is needed
        self._maybe_compress_cache()
    
    def get_stats(self) -> dict:
        """Get generation statistics including compression stats."""
        stats = super().get_stats()
        stats.update({
            "compression_count": self._compression_count,
            "tokens_evicted": self._tokens_evicted,
            "current_cache_len": self._get_cache_length(),
            "max_cache_len": self.max_cache_len,
            "effective_cache_size": self._effective_cache_size
        })
        return stats
    
    def get_compression_config(self) -> dict:
        """Get StreamingLLM compression configuration."""
        return {
            "start_size": self.start_size,
            "recent_size": self.recent_size,
            "max_cache_len": self.max_cache_len,
            "compress_threshold": self.compress_threshold,
            "effective_cache_size": self._effective_cache_size
        }


__all__ = [
    "TreeSpeculativeGenerator",
    "TreeSpeculativeGeneratorV2",
    "TreeStreamingSpeculativeGenerator"
]

