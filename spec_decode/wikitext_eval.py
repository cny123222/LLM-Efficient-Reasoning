import torch
import time
import numpy as np
import matplotlib.pyplot as plt
from threading import Thread
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from datasets import load_dataset
from tqdm import tqdm
import sys

# === å…³é”®ä¿®æ­£ï¼šå¼ºåŠ›å±è”½ Transformers çš„å•°å—¦æ—¥å¿— ===
from transformers import logging
logging.set_verbosity_error()

# ================= é…ç½®åŒºåŸŸ =================
DEVICE = "cuda:0"
TARGET_MODEL_ID = "/mnt/disk1/models/pythia-2.8b"

# å¯¹æ¯”ç»„é…ç½®
MODELS_CONFIG = {
    "70M": {
        "id": "/mnt/disk1/models/pythia-70m",
        "color": "#d62728",  # çº¢è‰²
        "marker": "o"
    },
    "160M": {
        "id": "/mnt/disk1/models/pythia-160m",
        "color": "#1f77b4",  # è“è‰²
        "marker": "s"        # æ–¹å—
    }
}

# 1. Kå€¼å– 3 åˆ° 10
K_VALUES = list(range(3, 11))

NUM_SAMPLES = 15  # æ ·æœ¬æ•°
MAX_NEW_TOKENS = 200

# ================= å·¥å…·ç±» =================
class ForwardCounter:
    def __init__(self): self.count = 0
    def __call__(self, m, i, o): self.count += 1

def setup_env():
    torch.set_default_device(DEVICE)
    try:
        import seaborn as sns
        plt.style.use('seaborn-v0_8-whitegrid')
    except:
        pass

def get_data(n):
    print(f"ğŸ“š Loading Wikitext-2...")
    try:
        # Wikitext åŠ è½½é€šå¸¸å¾ˆå¿«ï¼Œç®€å•çš„åŠ è½½æç¤ºå³å¯
        ds = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
        candidates = [x['text'] for x in ds if len(x['text']) > 200]
        print(f"âœ… Loaded {len(candidates)} candidates, selecting first {n}.")
        return candidates[:n]
    except Exception as e:
        print(f"âŒ Data load failed: {e}")
        return []

# ================= æ ¸å¿ƒæµ‹è¯•é€»è¾‘ =================
def run_benchmark(target_model, draft_model, tokenizer, prompts, k_val, desc_prefix=""):
    """
    è¿è¡Œæµ‹è¯•å¹¶è¿”å› (throughput, acceptance_rate)
    """
    total_tokens = 0
    total_time = 0
    total_steps = 0

    # é¢„çƒ­ (é™é»˜)
    dummy = tokenizer("Warmup", return_tensors="pt").to(DEVICE)
    if draft_model and k_val:
        target_model.generate(**dummy, assistant_model=draft_model, max_new_tokens=5, num_assistant_tokens=k_val)
    else:
        target_model.generate(**dummy, max_new_tokens=5)
    torch.cuda.synchronize()

    # === ä¼˜åŒ–ç‚¹ï¼šä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ï¼Œå¹¶è¾“å‡ºåˆ° stdout ===
    pbar = tqdm(prompts, desc=desc_prefix, leave=True, file=sys.stdout, ncols=100)

    for text in pbar:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=1024).to(DEVICE)
        
        counter = ForwardCounter()
        hook = target_model.register_forward_hook(counter)
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        gen_kwargs = dict(
            **inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False, 
            pad_token_id=tokenizer.eos_token_id, streamer=streamer
        )
        
        if draft_model and k_val:
            gen_kwargs["assistant_model"] = draft_model
            gen_kwargs["num_assistant_tokens"] = k_val
            
        thread = Thread(target=target_model.generate, kwargs=gen_kwargs)
        
        t0 = time.perf_counter()
        thread.start()
        res = ""
        for t in streamer: res += t
        thread.join()
        t1 = time.perf_counter()
        
        hook.remove()
        
        n_tok = len(tokenizer.encode(res, add_special_tokens=False))
        total_tokens += n_tok
        total_time += (t1 - t0)
        total_steps += max(1, counter.count)

        # === ä¼˜åŒ–ç‚¹ï¼šå®æ—¶æ˜¾ç¤ºå½“å‰é€Ÿåº¦ ===
        cur_speed = n_tok / (t1 - t0)
        pbar.set_postfix({"CurSpeed": f"{cur_speed:.1f}t/s"})

    avg_tp = total_tokens / total_time
    
    # å¥å£®çš„æ¥æ”¶ç‡è®¡ç®—å…¬å¼
    avg_acc = 0.0
    if k_val:
        tps = total_tokens / total_steps
        avg_acc = (tps - 1) / k_val
        avg_acc = min(1.0, max(0.0, avg_acc))

    return avg_tp, avg_acc

# ================= ä¸“é—¨ä¼˜åŒ–çš„ç»˜å›¾å‡½æ•° =================
def plot_comparison(results, k_vals, baseline):
    # åˆ›å»º 1 è¡Œ 2 åˆ—çš„å¸ƒå±€ï¼Œå®½å±æ˜¾ç¤º
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))
    
    # --- å›¾1ï¼šååé‡ (Throughput) ---
    ax1.set_title("Throughput Comparison (Wikitext)", fontsize=14, fontweight='bold')
    ax1.set_xlabel("Num Assistant Tokens (K)", fontsize=12)
    ax1.set_ylabel("Throughput (tokens/s)", fontsize=12)
    
    # ç»˜åˆ¶ Baseline (è™šçº¿)
    ax1.axhline(y=baseline, color='gray', linestyle='--', linewidth=2, alpha=0.7, label=f'Baseline ({baseline:.1f} t/s)')
    
    # æ”¶é›†æ•°æ®ä»¥ç¡®å®š Y è½´èŒƒå›´
    all_tp_values = [baseline]
    
    for label, data in results.items():
        cfg = MODELS_CONFIG[label]
        tp_data = data['throughput']
        all_tp_values.extend(tp_data)
        
        # ç»˜åˆ¶æ›²çº¿
        ax1.plot(k_vals, tp_data, 
                 color=cfg['color'], marker=cfg['marker'], linewidth=2.5, markersize=8, 
                 label=f'{label} Draft')
        
        # æ·»åŠ å¡«å……æ•ˆæœ
        ax1.fill_between(k_vals, baseline, tp_data, color=cfg['color'], alpha=0.1)

    # === å…³é”®ä¿®æ”¹ï¼šåŠ¨æ€ç¼©æ”¾ Y è½´ï¼Œè®©å·®è·æ›´æ˜æ˜¾ ===
    min_y = min(all_tp_values)
    max_y = max(all_tp_values)
    margin = (max_y - min_y) * 0.1 # ä¸Šä¸‹ç•™ 10% çš„ä½™é‡
    y_bottom = min(baseline, min_y) - margin
    y_top = max_y + margin
    ax1.set_ylim(y_bottom, y_top)
    
    ax1.legend(fontsize=12, loc='best')
    ax1.grid(True, linestyle='--', alpha=0.5)

    # --- å›¾2ï¼šæ¥æ”¶ç‡ (Acceptance Rate) ---
    ax2.set_title("Acceptance Rate Comparison (Wikitext)", fontsize=14, fontweight='bold')
    ax2.set_xlabel("Num Assistant Tokens (K)", fontsize=12)
    ax2.set_ylabel("Acceptance Rate", fontsize=12)
    
    for label, data in results.items():
        cfg = MODELS_CONFIG[label]
        acc_data = data['acc_rate']
        
        ax2.plot(k_vals, acc_data, 
                 color=cfg['color'], marker=cfg['marker'], linewidth=2.5, markersize=8,
                 label=f'{label} Draft')

    ax2.set_ylim(0, 1.05) # æ¥æ”¶ç‡å›ºå®š 0-1
    ax2.legend(fontsize=12)
    ax2.grid(True, linestyle='--', alpha=0.5)

    plt.tight_layout()
    # === å…³é”®ä¿®æ”¹ï¼šæ–‡ä»¶åè®¾ç½®ä¸º wikitext fixed ===
    filename = "speculative_comparison_wikitext.png"
    plt.savefig(filename, dpi=300)
    print(f"\nâœ… ç»˜å›¾å®Œæˆï¼å›¾ç‰‡å·²ä¿å­˜ä¸º: {filename}")

# ================= ä¸»ç¨‹åº =================
if __name__ == "__main__":
    setup_env()
    
    print(f"ğŸ”„ Loading Target: {TARGET_MODEL_ID}...")
    target_model = AutoModelForCausalLM.from_pretrained(TARGET_MODEL_ID, torch_dtype=torch.float16, device_map=DEVICE)
    tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL_ID)
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    
    prompts = get_data(NUM_SAMPLES)
    if not prompts: exit()
    
    # å­˜å‚¨ç»“æœå®¹å™¨
    results = {
        "70M": {"throughput": [], "acc_rate": []},
        "160M": {"throughput": [], "acc_rate": []}
    }
    
    # 1. å…ˆæµ‹ Baseline (åªæµ‹ä¸€æ¬¡)
    print("\nğŸ Running Baseline...")
    baseline_speed, _ = run_benchmark(target_model, None, tokenizer, prompts, None, desc_prefix="Baseline")
    print(f"   -> Baseline Speed: {baseline_speed:.2f} t/s")

    # 2. å¾ªç¯æµ‹è¯•ä¸¤ä¸ªæ¨¡å‹
    for label in ["70M", "160M"]:
        cfg = MODELS_CONFIG[label]
        print(f"\nğŸ§ª Testing {label} Draft ({cfg['id']})...")
        
        draft_model = AutoModelForCausalLM.from_pretrained(cfg['id'], torch_dtype=torch.float16, device_map=DEVICE)
        
        # === ä¼˜åŒ–ç‚¹ï¼šæ‰å¹³åŒ–å¾ªç¯ï¼Œä¸å†ä½¿ç”¨åµŒå¥— tqdm ===
        for k in K_VALUES:
            # ä¼ é€’ desc_prefix ä»¥è·å¾—ç‹¬ç«‹çš„è¿›åº¦æ¡
            tp, acc = run_benchmark(target_model, draft_model, tokenizer, prompts, k, desc_prefix=f"Step K={k}")
            
            results[label]["throughput"].append(tp)
            results[label]["acc_rate"].append(acc)
            
            # æ‰“å°å½“å‰ K çš„æ±‡æ€»ï¼Œé˜²æ­¢è¿›åº¦æ¡æ»šèµ°åä¿¡æ¯ä¸¢å¤±
            print(f"      K={k}: Speed={tp:.1f} t/s | Acc={acc:.2f}")
        
        del draft_model
        torch.cuda.empty_cache()
        print(f"   Done {label}!")

    # 3. ç»˜å›¾
    plot_comparison(results, K_VALUES, baseline_speed)