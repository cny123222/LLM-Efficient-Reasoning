# 模块功能说明

本文档说明 `precision/` 和 `spec_decode/` 两个模块分别实现的功能。

---

## 📊 `precision/` - 模型量化精度评估

### 核心功能
**评估不同数值精度（FP32/FP16/BF16）对模型性能的影响**

### 实现内容

#### 1. **精度对比测试**
- **测试精度类型**：
  - `FP32` (float32) - 全精度，32位浮点数
  - `FP16` (float16) - 半精度，16位浮点数
  - `BF16` (bfloat16) - 半精度，16位浮点数（Google Brain Float）

#### 2. **评估指标**
脚本会全面评估以下5个关键指标：

| 指标 | 说明 | 单位 |
|------|------|------|
| **Throughput** | 吞吐量（生成速度） | tokens/sec |
| **TPOT** | Time Per Output Token（每个输出token的时间） | milliseconds |
| **TTFT** | Time To First Token（首token延迟） | milliseconds |
| **PPL** | Perplexity（困惑度，衡量生成质量） | 分数（越低越好） |
| **VRAM** | 峰值显存占用 | GB |

#### 3. **测试流程**
```
1. 加载模型（使用指定精度）
2. 计算 PPL（困惑度）评估生成质量
3. 测量生成速度（TTFT, TPOT, Throughput）
4. 记录峰值显存占用
5. 生成对比图表
```

#### 4. **输出结果**
- **图表**：`precision_benchmark_2rows.png`
  - 2行3列布局，展示5个关键指标的对比
  - 直观显示不同精度在速度、质量和内存之间的权衡

#### 5. **使用场景**
- ✅ 选择最适合的模型精度（平衡速度、质量和内存）
- ✅ 评估量化对模型性能的影响
- ✅ 为部署选择合适的精度配置

---

## 🚀 `spec_decode/` - 推测解码（Speculative Decoding）加速

### 核心功能
**使用小模型作为 draft model 加速大模型推理**

### 实现内容

#### 1. **推测解码原理**
```
传统方法：大模型逐个生成 token
推测解码：小模型快速生成 K 个候选 token → 大模型并行验证 → 接受/拒绝
```

#### 2. **测试配置**
- **Target Model（目标模型）**：`EleutherAI/pythia-2.8b` (2.8B 参数)
- **Draft Models（草稿模型）**：
  - `70M` - 70M 参数的小模型
  - `160M` - 160M 参数的小模型
- **K 值范围**：3-10（每次推测生成的 token 数量）
- **测试数据集**：
  - `pg_19_eval.py` - PG-19 长文本数据集
  - `wikitext_eval.py` - Wikitext-2 数据集

#### 3. **评估指标**

| 指标 | 说明 | 计算方式 |
|------|------|----------|
| **Throughput** | 吞吐量（生成速度） | tokens / total_time |
| **Acceptance Rate** | 接收率（draft tokens 被接受的比例） | (tokens_per_step - 1) / K |

#### 4. **核心算法流程**
```python
for each prompt:
    1. Draft Model 快速生成 K 个候选 tokens
    2. Target Model 并行验证这 K 个 tokens
    3. 计算接受率（有多少 tokens 被接受）
    4. 测量总吞吐量
```

#### 5. **关键代码逻辑**
```python
# 使用 Transformers 的 assistant_model 参数实现推测解码
gen_kwargs = {
    "assistant_model": draft_model,      # 小模型作为草稿
    "num_assistant_tokens": k_val,       # 每次推测 K 个 tokens
    "max_new_tokens": MAX_NEW_TOKENS
}
target_model.generate(**gen_kwargs)
```

#### 6. **输出结果**
- **图表**：
  - `speculative_comparison_pg19.png` - PG-19 数据集上的对比结果
  - `speculative_comparison_wikitext.png` - Wikitext 数据集上的对比结果
- **图表内容**：
  - 左图：吞吐量对比（不同 K 值下的速度提升）
  - 右图：接收率对比（draft tokens 的接受率）

#### 7. **性能分析**
- **速度提升**：通过小模型快速生成候选，大模型并行验证，理论上可以加速 2-3 倍
- **质量保证**：大模型验证确保生成质量不下降
- **最佳 K 值**：需要平衡速度提升和接收率（K 太大可能导致接收率下降）

#### 8. **使用场景**
- ✅ 加速大模型推理（特别是长文本生成）
- ✅ 在保持质量的前提下提升吞吐量
- ✅ 评估不同 draft model 大小的效果
- ✅ 找到最优的 K 值配置

---

## 📈 两个模块的关系

| 维度 | `precision/` | `spec_decode/` |
|------|-------------|----------------|
| **优化方向** | 模型量化（降低精度） | 算法优化（推测解码） |
| **主要目标** | 减少显存占用 | 提升生成速度 |
| **技术手段** | 使用 FP16/BF16 替代 FP32 | 使用小模型辅助大模型 |
| **适用场景** | 资源受限环境 | 需要高吞吐量的场景 |
| **质量影响** | 可能轻微下降（PPL 增加） | 基本无影响（大模型验证） |

---

## 🎯 总结

### `precision/` 模块
- **核心**：量化精度评估
- **价值**：帮助选择最优精度配置，平衡速度、质量和内存
- **输出**：5 个关键指标的对比图表

### `spec_decode/` 模块
- **核心**：推测解码加速
- **价值**：使用小模型加速大模型推理，显著提升吞吐量
- **输出**：不同 K 值和 draft model 的对比结果

两个模块都是 LLM 推理加速的重要技术，可以结合使用：
- 先用 `precision/` 选择合适精度（如 FP16）
- 再用 `spec_decode/` 进一步加速推理

---

*文档生成时间: 2024*

