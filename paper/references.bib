% Core speculative decoding and analyses
@misc{leviathan2023fast,
  title={Fast Inference from Transformers via Speculative Decoding}, 
  author={Yaniv Leviathan and Matan Kalman and Yossi Matias},
  year={2023},
  eprint={2211.17192},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2211.17192}, 
}

@misc{decoding_speculative,
  title={Decoding Speculative Decoding}, 
  author={Minghao Yan and Saurabh Agarwal and Shivaram Venkataraman},
  year={2025},
  eprint={2402.01528},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2402.01528}, 
}

@misc{draft_tradeoff,
  title={Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding}, 
  author={Heming Xia and Zhe Yang and Qingxiu Dong and Peiyi Wang and Yongqi Li and Tao Ge and Tianyu Liu and Wenjie Li and Zhifang Sui},
  year={2024},
  eprint={2401.07851},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2401.07851}, 
}

% Tree-based speculative decoding and pruning
@inproceedings{specinfer, 
  series={ASPLOS ’24},
  title={SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification},
  url={http://dx.doi.org/10.1145/3620666.3651335},
  DOI={10.1145/3620666.3651335},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  publisher={ACM},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and Shi, Chunan and Chen, Zhuoming and Arfeen, Daiyaan and Abhyankar, Reyna and Jia, Zhihao},
  year={2024},
  month=apr, pages={932–949},
  collection={ASPLOS ’24} 
}

@misc{opt_tree,
  title={OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure}, 
  author={Jikai Wang and Yi Su and Juntao Li and Qingrong Xia and Zi Ye and Xinyu Duan and Zhefeng Wang and Min Zhang},
  year={2025},
  eprint={2406.17276},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2406.17276}, 
}

@misc{propd,
  title={ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding}, 
  author={Shuzhang Zhong and Zebin Yang and Meng Li and Ruihao Gong and Runsheng Wang and Ru Huang},
  year={2024},
  eprint={2402.13485},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2402.13485}, 
}

@misc{dyspec,
  title={DySpec: Faster Speculative Decoding with Dynamic Token Tree Structure}, 
  author={Yunfan Xiong and Ruoyu Zhang and Yanzeng Li and Tianhao Wu and Lei Zou},
  year={2024},
  eprint={2410.11744},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2410.11744}, 
}

@misc{cast,
  title={Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models}, 
  author={Yinrong Hong and Zhiquan Tan and Kai Hu},
  year={2025},
  eprint={2510.26577},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2510.26577}, 
}

@misc{medusa,
  title={Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads}, 
  author={Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},
  year={2024},
  eprint={2401.10774},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2401.10774}, 
}

% Complementary inference optimizations (optional, cited in broader discussion)
@misc{flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
  author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
  year={2022},
  eprint={2205.14135},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2205.14135}, 
}

@misc{vllm,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention}, 
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  year={2023},
  eprint={2309.06180},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2309.06180}, 
}

@misc{streamingllm,
  title={Efficient Streaming Language Models with Attention Sinks}, 
  author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  year={2024},
  eprint={2309.17453},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2309.17453}, 
}

% Web references used in the introduction (kept for completeness)
@misc{nvidia_speculative,
  title        = {An Introduction to Speculative Decoding for Reducing Latency in {AI} Inference},
  author       = {{NVIDIA Developer Blog}},
  year         = {2023},
  howpublished = {\url{https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/}},
  note         = {Accessed: 2026-01-03}
}

@misc{speculative_intro,
  title        = {Looking Back at Speculative Decoding},
  author       = {{Google Research Blog}},
  year         = {2023},
  howpublished = {\url{https://research.google/blog/looking-back-at-speculative-decoding/}},
  note         = {Accessed: 2026-01-03}
}

@misc{transformer_inference,
  title        = {Transformer Inference: Techniques for Faster {AI} Models},
  author       = {{Premai Blog}},
  year         = {2024},
  howpublished = {\url{https://www.premai.io/blog/transformer-inference-techniques-for-faster-ai-models}},
  note         = {Accessed: 2026-01-03}
}

@misc{llm_bottleneck,
  title        = {Decoding Real-Time {LLM} Inference: A Guide to the Latency vs Throughput Bottleneck},
  author       = {{Medium}},
  year         = {2024},
  howpublished = {\url{https://medium.com/learnwithnk/decoding-real-time-llm-inference-a-guide-to-the-latency-vs-throughput-bottleneck-c1ad96442d50}},
  note         = {Accessed: 2026-01-03}
}

% Robustness and systems perspectives (from related_work.md)
@misc{spin,
  title={SPIN: Accelerating Large Language Model Inference with Heterogeneous Speculative Models}, 
  author={Fahao Chen and Peng Li and Tom H. Luan and Zhou Su and Jing Deng},
  year={2025},
  eprint={2503.15921},
  archivePrefix={arXiv},
  primaryClass={cs.DC},
  url={https://arxiv.org/abs/2503.15921}, 
}

@misc{owl,
  title={OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs}, 
  author={Jaeseong Lee and seung-won hwang and Aurick Qiao and Gabriele Oliaro and Ye Wang and Samyam Rajbhandari},
  year={2025},
  eprint={2510.07535},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2510.07535}, 
}

@misc{judge_decoding,
  title={Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment}, 
  author={Gregor Bachmann and Sotiris Anagnostidis and Albert Pumarola and Markos Georgopoulos and Artsiom Sanakoyeu and Yuming Du and Edgar Schönfeld and Ali Thabet and Jonas Kohler},
  year={2025},
  eprint={2501.19309},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2501.19309}, 
}

@misc{traversal_verification,
  title={Traversal Verification for Speculative Tree Decoding}, 
  author={Yepeng Weng and Qiao Hu and Xujie Chen and Li Liu and Dianwen Mei and Huishi Qiu and Jiang Tian and Zhongchao Shi},
  year={2025},
  eprint={2505.12398},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2505.12398}, 
}

@inproceedings{deft,
  title={De{FT}: Decoding with Flash Tree-attention for Efficient Tree-structured {LLM} Inference},
  author={Jinwei Yao and Kaiqi Chen and Kexun Zhang and Jiaxuan You and Binhang Yuan and Zeke Wang and Tao Lin},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=2c7pfOqu9k}
}

% Adaptive and confidence-aware methods (from related_work_new.md)
@misc{cm_asd,
  title={Confidence-Modulated Speculative Decoding for Large Language Models}, 
  author={Jaydip Sen and Subhasis Dasgupta and Hetvi Waghela},
  year={2025},
  eprint={2508.15371},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2508.15371}, 
}

@misc{adaeagle,
  title={AdaEAGLE: Optimizing Speculative Decoding via Explicit Modeling of Adaptive Draft Structures}, 
  author={Situo Zhang and Hankun Wang and Da Ma and Zichen Zhu and Lu Chen and Kunyao Lan and Kai Yu},
  year={2024},
  eprint={2412.18910},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2412.18910}, 
}

@misc{eagle,
  title={EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty}, 
  author={Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang},
  year={2025},
  eprint={2401.15077},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2401.15077}, 
}

@misc{eagle2,
  title={EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees}, 
  author={Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang},
  year={2024},
  eprint={2406.16858},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2406.16858}, 
}

@misc{eagle3,
  title={EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test}, 
  author={Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang},
  year={2025},
  eprint={2503.01840},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2503.01840}, 
}

@misc{cas_spec,
  title={CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs}, 
  author={Zhiyuan Ning and Jiawei Shao and Ruge Xu and Xinfei Guo and Jun Zhang and Chi Zhang and Xuelong Li},
  year={2025},
  eprint={2510.26843},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2510.26843}, 
}

@misc{adasd,
  title={AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference}, 
  author={Kuan-Wei Lu and Ding-Yong Hong and Pangfeng Liu},
  year={2025},
  eprint={2512.11280},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2512.11280}, 
}

@misc{rasd,
  title={RASD: Retrieval-Augmented Speculative Decoding}, 
  author={Guofeng Quan and Wenfeng Feng and Chuzhan Hao and Guochao Jiang and Yuewei Zhang and Hao Wang},
  year={2025},
  eprint={2503.03434},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2503.03434}, 
}

% Additional related work (from internal survey notes; metadata fetched from arXiv)
@misc{arxiv_2508_08192,
  title={Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions}, 
  author={Bangsheng Tang and Carl Chengyan Fu and Fei Kou and Grigory Sizov and Haoci Zhang and Jason Park and Jiawen Liu and Jie You and Qirui Yang and Sachin Mehta and Shengyong Cai and Xiaodong Wang and Xingyu Liu and Yunlu Li and Yanjun Zhou and Wei Wei and Zhiwei Zhao and Zixi Qi and Adolfo Victoria and Aya Ibrahim and Bram Wasti and Changkyu Kim and Daniel Haziza and Fei Sun and Giancarlo Delfin and Emily Guo and Jialin Ouyang and Jaewon Lee and Jianyu Huang and Jeremy Reizenstein and Lu Fang and Quinn Zhu and Ria Verma and Vlad Mihailescu and Xingwen Guo and Yan Cui and Ye Hu and Yejin Lee},
  year={2025},
  eprint={2508.08192},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2508.08192}, 
}

@misc{arxiv_2402_15678,
  title={Minions: Accelerating Large Language Model Inference with Aggregated Speculative Execution}, 
  author={Siqi Wang and Hailong Yang and Xuezhu Wang and Tongxuan Liu and Pengbo Wang and Xuning Liang and Kejie Ma and Tianyu Feng and Xin You and Yongjun Bao and Yi Liu and Zhongzhi Luan and Depei Qian},
  year={2024},
  eprint={2402.15678},
  archivePrefix={arXiv},
  primaryClass={cs.DC},
  url={https://arxiv.org/abs/2402.15678}, 
}

@misc{arxiv_2410_13344,
  title={Cerberus: Efficient Inference with Adaptive Parallel Decoding and Sequential Knowledge Enhancement}, 
  author={Yuxuan Liu and Wenyuan Li and Laizhong Cui and Hailiang Yang},
  year={2024},
  eprint={2410.13344},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2410.13344}, 
}

@misc{arxiv_2505_15141,
  title={BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms}, 
  author={Yunlong Hou and Fengzhuo Zhang and Cunxiao Du and Xuan Zhang and Jiachun Pan and Tianyu Pang and Chao Du and Vincent Y. F. Tan and Zhuoran Yang},
  year={2025},
  eprint={2505.15141},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2505.15141}, 
}

% Models and frameworks
@misc{pythia,
  title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
  author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
  year={2023},
  eprint={2304.01373},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2304.01373}, 
}

@misc{pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.01703}, 
}

@misc{transformers,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  year={2020},
  eprint={1910.03771},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1910.03771}, 
}

% Datasets
@misc{rae2019compressive,
  title={Compressive Transformers for Long-Range Sequence Modelling}, 
  author={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Timothy P. Lillicrap},
  year={2019},
  eprint={1911.05507},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1911.05507}, 
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models}, 
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1609.07843}, 
}


