\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025
\PassOptionsToPackage{numbers,sort&compress}{natbib}

% The authors should use one of these tracks.
% Before accepting by the NeurIPS conference, select one of the options below.
% 0. "default" for submission
% \usepackage{neurips_2025}
% Using "final" option to show author names (not for actual submission)
\usepackage[preprint]{neurips_2025}
% the "default" option is equal to the "main" option, which is used for the Main Track with double-blind reviewing.
% 1. "main" option is used for the Main Track
%  \usepackage[main]{neurips_2025}
% 2. "position" option is used for the Position Paper Track
%  \usepackage[position]{neurips_2025}
% 3. "dandb" option is used for the Datasets & Benchmarks Track
 % \usepackage[dandb]{neurips_2025}
% 4. "creativeai" option is used for the Creative AI Track
%  \usepackage[creativeai]{neurips_2025}
% 5. "sglblindworkshop" option is used for the Workshop with single-blind reviewing
 % \usepackage[sglblindworkshop]{neurips_2025}
% 6. "dblblindworkshop" option is used for the Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop]{neurips_2025}

% After being accepted, the authors should add "final" behind the track to compile a camera-ready version.
% 1. Main Track
 % \usepackage[main, final]{neurips_2025}
% 2. Position Paper Track
%  \usepackage[position, final]{neurips_2025}
% 3. Datasets & Benchmarks Track
 % \usepackage[dandb, final]{neurips_2025}
% 4. Creative AI Track
%  \usepackage[creativeai, final]{neurips_2025}
% 5. Workshop with single-blind reviewing
%  \usepackage[sglblindworkshop, final]{neurips_2025}
% 6. Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop, final]{neurips_2025}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote.
% For workshops (5., 6.), the authors should add the name of the workshop, "\workshoptitle" command is used to set the workshop title.
% \workshoptitle{WORKSHOP TITLE}

% "preprint" option is used for arXiv or other preprint submissions
 % \usepackage[preprint]{neurips_2025}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hypertexnames=false]{hyperref}       % hyperlinks (avoid duplicate destination warnings)
\usepackage{placeins}       % \FloatBarrier to keep floats out of references
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{multirow}       % multirow cells in tables
\usepackage{graphicx}       % figures
\usepackage{amsmath}        % math
\usepackage{amssymb}        % math symbols
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{algorithm}     % float wrapper
\usepackage{algpseudocode} % algorithmicx style (supports \Call)

% Float placement/spacing is defined by `neurips_2025.sty`.
% Avoid overriding it here so the style file remains the single source of truth.

% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote. 
\title{DynaTree: Confidence-Aware Adaptive Tree Speculative Decoding for Efficient LLM Inference}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Nuoyan Chen$^*$ \quad
  Jiamin Liu$^*$ \quad
  Zhaocheng Li\thanks{Equal contribution.} \\
  School of Computer Science\\
  Shanghai Jiao Tong University\\
  \texttt{\{cny123222, logic-1.0, lzc050419\}@sjtu.edu.cn}
}


\begin{document}


\maketitle


\begin{abstract}
 Autoregressive decoding in large language models (LLMs) is fundamentally sequential and therefore underutilizes modern accelerator parallelism during token generation. Speculative decoding mitigates this bottleneck by letting a lightweight draft model propose multiple tokens that are verified in parallel by the target model; however, linear variants explore only a single draft chain per step and can waste substantial computation when early tokens are rejected, while existing tree-based approaches often employ \emph{fixed} structures that cannot adapt to varying draft model confidence. We propose \textbf{DynaTree}, a training-free tree-based speculative decoding framework with \emph{confidence-aware} adaptive drafting that dynamically adjusts tree breadth and depth under an explicit node budget, combined with probability-threshold pruning to strictly manage computational overhead. In the greedy-decoding setting, DynaTree is exact: it generates the same token sequence as the target model (no distributional bias). Experiments with Pythia-2.8B (target) and Pythia-70M (draft) show that DynaTree improves throughput over autoregressive decoding, linear speculative decoding, and tuned fixed-tree baselines on both WikiText-2 and PG-19. For generation length \(T=1500\) new tokens, DynaTree reaches 219.5~tokens/s on WikiText-2 and 194.9~tokens/s on PG-19, yielding a 9.4\% and 5.1\% throughput increase over tuned fixed-tree baselines, respectively.
\end{abstract}


\section{Introduction}

Autoregressive decoding remains the default generation mode for large language models (LLMs), but it is inherently sequential: each token requires a forward pass conditioned on the full prefix. While transformer inference can exploit parallelism during prefill, the decode stage offers limited parallelism and is often bottlenecked by memory traffic and per-token kernel launch overhead~\cite{flashattention,vllm}.

Speculative decoding mitigates this bottleneck by separating \emph{proposal} and \emph{verification}~\cite{leviathan2023fast}. A lightweight draft model proposes candidate tokens and the target model verifies them in parallel; when verification succeeds, multiple tokens are committed per iteration. With rejection sampling, speculative decoding preserves the exact output distribution of the target model~\cite{decoding_speculative}.

Most deployed speculative decoders are \emph{linear}: the draft model proposes a single chain of \(K\) tokens. This design is brittle under draft--target mismatch: a rejection at an early position discards all downstream draft tokens, wasting both draft computation and target-model verification work~\cite{decoding_speculative}. Tree-based speculation offers a natural remedy by exploring multiple continuations in parallel and verifying a token tree with a structured attention mask, increasing the probability that at least one branch matches the target model's greedy continuation~\cite{specinfer,opt_tree}.

However, existing tree-based approaches typically employ \emph{fixed} tree shapes with predetermined depth and branching factor~\cite{specinfer,medusa}. A fixed structure cannot adapt to varying draft confidence across contexts: it may over-explore when the draft distribution is peaked, and under-explore when the next-token distribution is flat, creating an \emph{efficiency gap}. Recent adaptive methods adjust draft length or verification thresholds~\cite{cm_asd,adaeagle,cas_spec}, but most focus on linear speculation rather than restructuring the tree itself.

We propose \textbf{DynaTree}, a training-free tree speculative decoder that constructs a draft token tree with \emph{confidence-aware} adaptive drafting under an explicit node budget. DynaTree combines \emph{Dynamic Tree Breadth}, \emph{Dynamic Tree Depth}, and \emph{History Adaptation} to stabilize verification efficiency across iterations. Using tree attention, all drafted nodes are verified in a single target-model forward pass. Empirically, DynaTree improves throughput over autoregressive decoding, linear speculation, and tuned fixed-tree baselines on both WikiText-2 and PG-19 (Table~\ref{tab:main-results}).

In summary, our contributions are:
\begin{itemize}
  \item We propose \textbf{DynaTree}, a \textbf{training-free} tree speculative decoding framework that adaptively allocates verification budget via confidence-aware adjustments to tree breadth and depth under an explicit node budget. Unlike Medusa~\cite{medusa} or the EAGLE family~\cite{eagle,eagle2,eagle3}, which require training auxiliary modules and/or architectural modifications, DynaTree is plug-and-play for existing target--draft model pairs.
  
  \item We introduce a lightweight three-component adaptation mechanism (\emph{Dynamic Tree Breadth}, \emph{Dynamic Tree Depth}, and \emph{History Adaptation}) that integrates with tree-attention verification and probability-threshold pruning.
  
  \item Experiments on WikiText-2 and PG-19 demonstrate consistent throughput and latency improvements over autoregressive decoding, linear speculative decoding, and tuned fixed-tree baselines, with up to 1.70$\times$ speedup at \(T=1500\).
\end{itemize}

Figure~\ref{fig:decode-comparison} provides a schematic comparison of autoregressive decoding, linear speculative decoding, and tree-based speculative decoding.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.92\linewidth]{../figures/decode-v2.png}
  \caption{\textbf{Comparison of three decoding paradigms.} \textbf{(a) Autoregressive (AR):} the target LLM generates one token per iteration sequentially. \textbf{(b) Linear speculative decoding:} a draft model proposes a length-\(K\) chain that is verified by the target LLM; once an early token is rejected, all subsequent drafted tokens in the chain are discarded, reducing effective progress per iteration. \textbf{(c) Tree-based speculative decoding (e.g., DynaTree):} the draft model proposes a token tree and the target LLM verifies all candidates in parallel (one forward pass with a structured attention mask), then commits the longest greedy-consistent path (plus a bonus token), improving robustness to draft mismatch.}
  \label{fig:decode-comparison}
\end{figure}

\section{Related Work}

\subsection{Speculative Decoding}

Speculative decoding improves generation efficiency by using a small draft model to propose candidates that are verified by the target model, while preserving target-model correctness under the appropriate acceptance rule~\cite{leviathan2023fast,decoding_speculative,draft_tradeoff}. Practical speedups depend critically on the draft--target mismatch: early rejections waste draft work and reduce effective progress per verification round, motivating policies that adapt drafting aggressiveness to context. From a systems perspective, decoding is often memory-bound and sensitive to batching, context length, and KV-cache behavior~\cite{flashattention,vllm,spin,owl}. Recent studies further highlight serving-oriented constraints and robustness under realistic workloads~\cite{arxiv_2508_08192,judge_decoding}.

\subsection{Tree-Based Speculative Decoding}

Tree-based speculative decoding generalizes linear drafting by verifying a token tree with a structured attention mask in a single target-model pass~\cite{specinfer,opt_tree,medusa,traversal_verification}. SpecInfer~\cite{specinfer} pioneered practical token-tree verification, and subsequent work refined verification and implementation efficiency, e.g., via traversal-based verification and optimized tree-attention kernels~\cite{traversal_verification,deft}. OPT-Tree~\cite{opt_tree} searches for tree structures that improve the expected committed prefix length under a verification budget, while the EAGLE line explores alternative speculative designs, including dynamic draft trees for faster inference~\cite{eagle,eagle2,eagle3}. In contrast to fixed tree shapes, our focus is on training-free, confidence-aware adaptation of both tree breadth and depth within a strict node budget.

Adaptive speculative decoding modulates drafting aggressiveness using confidence signals or learned predictors~\cite{cm_asd,adaeagle,cas_spec}. In contrast to adaptive \emph{length}-only policies, DynaTree adapts the tree structure itself by jointly controlling \emph{Dynamic Tree Breadth} and \emph{Dynamic Tree Depth} and incorporating \emph{History Adaptation}, while remaining training-free.

\subsection{Dynamic Pruning Strategies}

Exponential candidate growth necessitates pruning to balance exploration against verification cost. Prior work studies early pruning and confidence-guided expansion for token trees~\cite{propd,dyspec}, as well as cost-aware tree construction and pruning that accounts for verification overhead~\cite{cast,opt_tree}. Retrieval-augmented heuristics can further bias exploration toward promising continuations~\cite{rasd}, and complementary lines adapt speculative hyperparameters online via bandit-style selection policies or threshold updates~\cite{arxiv_2505_15141,adasd}. DynaTree instantiates this trade-off with \emph{Adaptive Pruning} (probability thresholding under a strict node budget) and focuses on training-free, confidence-aware restructuring of the draft tree.


\section{Methodology}
\label{method}

\subsection{Problem Setup and Notation}
Let \(M_T\) denote a target autoregressive language model and \(M_D\) a smaller draft model over vocabulary \(\mathcal{V}\). We write \(p_T(\cdot\mid\cdot)\) and \(p_D(\cdot\mid\cdot)\) for their next-token distributions. Given a prefix (prompt) \(x_{1:t}\), greedy decoding with \(M_T\) produces tokens \(y_{t+1}, y_{t+2}, \dots\) via
\[
y_{i} \;=\; \arg\max_{v \in \mathcal{V}} p_T(v \mid x_{1:i-1}).
\]
Speculative decoding accelerates generation by proposing candidate tokens with \(M_D\) and verifying them with \(M_T\). In this paper we focus on greedy decoding; correctness requires that a token is committed only if it matches the target model's greedy argmax under the corresponding conditioning context.

\subsection{Overview of DynaTree}
DynaTree generalizes linear speculative decoding from a single draft chain to a \emph{draft token tree}. In each iteration, it constructs a candidate tree with \(M_D\) and verifies all drafted nodes in a single forward pass of \(M_T\) using tree attention. DynaTree's key innovation is \emph{confidence-aware adaptive drafting} that allocates verification budget based on draft uncertainty:
\textbf{(i) Dynamic Tree Breadth} selects a per-node branching factor from \(\{B_{\min},B_{\mathrm{mid}},B_{\max}\}\) using confidence thresholds \((\tau_h,\tau_\ell)\);
\textbf{(ii) Dynamic Tree Depth} gates expansion using cumulative path probability with early stopping and selective deep expansion up to \(D_{\max}\);
and \textbf{(iii) History Adaptation} updates a small subset of drafting hyperparameters online using recent acceptance statistics.
Throughout, we use \textbf{Adaptive Pruning} to denote the combination of probability-threshold pruning with a strict node-budget cap (Figure~\ref{fig:our-approach}c).

Figure~\ref{fig:arch} illustrates the end-to-end pipeline of a single DynaTree iteration.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=1.0\linewidth]{../figures/dynatree-v10.png}
  \caption{\textbf{One iteration of DynaTree decoding (end-to-end pipeline).} (1)~\textbf{Adaptive Tree Drafting:} the draft model expands a candidate token tree under node budget \(N_{\max}\) with confidence-aware breadth/depth and probability-threshold pruning. (2)~\textbf{Serialization \& Masking:} the pruned tree is serialized (BFS order) and converted into a tree-attention mask. (3)~\textbf{Parallel Verification:} the target model verifies all drafted candidates in a single forward pass. (4)~\textbf{Path Selection:} select the longest greedy-consistent (accepted) path and commit it (plus a bonus token). (5)~\textbf{Update Context + KV:} committed tokens update the context and target-model KV cache. (6)~\textbf{Adjust Parameters based on History:} recent acceptance statistics adjust drafting hyperparameters for the next iteration.}
  \label{fig:arch}
\end{figure}

\subsection{Adaptive Tree Drafting}
Figure~\ref{fig:our-approach} provides a schematic view of DynaTree's three adaptive mechanisms: Dynamic Tree Breadth, Dynamic Tree Depth, and Adaptive Pruning.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.85\linewidth]{../figures/our_approach.png}
  \caption{\textbf{Illustration of DynaTree's adaptive mechanisms.} \textbf{(a) Dynamic Tree Breadth:} choose per-node branching \(B(u)\in\{B_{\min},B_{\mathrm{mid}},B_{\max}\}\) based on local confidence \(c(u)\). \textbf{(b) Dynamic Tree Depth:} control expansion using cumulative path probability \(p(u)\) with early stopping (\(p(u)<\rho_{\mathrm{stop}}\)), a base depth \(D_0\), and selective deep expansion beyond \(D_0\) only when \(p(u)>\rho_{\mathrm{deep}}\), up to a hard maximum depth \(D_{\max}\). \textbf{(c) Adaptive Pruning:} stop expanding nodes whose \(p(u)<\tau\) and cap the tree size by a strict node budget \(N_{\max}\).}
  \label{fig:our-approach}
\end{figure}

We maintain a token tree \(\mathcal{T}=(\mathcal{N}, \mathcal{E})\) rooted at the current prefix \(x_{1:t}\), where each node \(u \in \mathcal{N}\) corresponds to a drafted token. Each node \(u\) is associated with: (i) token \(z_u \in \mathcal{V}\); (ii) parent \(\pi(u)\); (iii) depth \(d(u)\) from the root; (iv) draft log-probability \(\ell_u = \log p_D(z_u \mid \mathrm{ctx}(\pi(u)))\), where \(\mathrm{ctx}(\pi(u))\) denotes the unique root-to-\(\pi(u)\) token context; and (v) cumulative log-probability \(\bar{\ell}_u = \sum_{v \in \mathrm{path}(u)} \ell_v\), where \(\mathrm{path}(u)\) denotes the nodes on the root-to-\(u\) path. We also use the cumulative path probability \(p(u)=\exp(\bar{\ell}_u)\).

\paragraph{Dynamic Tree Breadth.}
Starting from the current prefix \(x_{1:t}\), we construct \(\mathcal{T}\) in a breadth-first manner under a strict node budget \(N_{\max}\). For any expandable node \(u\), let \(q_D(\cdot \mid u)\) denote the draft distribution conditioned on the unique root-to-\(u\) prefix, and define the local confidence
\[
c(u)\;=\;\max_{v\in\mathcal{V}} q_D(v\mid u).
\]
We select a \emph{per-node} branching factor via a confidence rule
\[
B(u)=
\begin{cases}
B_{\min}, & c(u)\ge \tau_h,\\
B_{\mathrm{mid}}, & \tau_\ell \le c(u) < \tau_h,\\
B_{\max}, & c(u)<\tau_\ell,
\end{cases}
\]
where \(0<\tau_\ell<\tau_h<1\) are confidence thresholds and \(1\le B_{\min}\le B_{\mathrm{mid}}\le B_{\max}\) are integer branch bounds.
and expand \(u\) by adding the \(B(u)\) highest-probability children under \(q_D(\cdot\mid u)\). To adapt depth, we use the cumulative path probability \(p(u)=\exp(\bar{\ell}_u)\): low-probability branches are terminated early, while high-probability paths may be expanded beyond a base depth. Concretely, a node at depth \(d(u)\) is eligible for expansion only if it satisfies the depth-gating rule (defined in the next paragraph) and \(|\mathcal{N}|<N_{\max}\). Implementation details for cache reuse during expansion are deferred to Appendix~\ref{app:algo}.

\paragraph{Dynamic Tree Depth.}
Let \(D_0\) denote a base depth and \(D_{\max}\) a hard maximum depth. We gate expansion using two probability thresholds \(\rho_{\mathrm{stop}} < \rho_{\mathrm{deep}}\) on the cumulative path probability \(p(u)\). A node \(u\) at depth \(d(u)\) is expandable if and only if
\[
d(u) < D_{\max}
\quad\wedge\quad
p(u)\ge \rho_{\mathrm{stop}}
\quad\wedge\quad
\Bigl(d(u)<D_0 \;\;\vee\;\; p(u)>\rho_{\mathrm{deep}}\Bigr).
\]
We assume \(1\le D_0 < D_{\max}\) and \(0<\rho_{\mathrm{stop}}<\rho_{\mathrm{deep}}<1\).
The first condition enforces a hard depth limit; the second implements \emph{early stopping} by terminating branches whose joint draft probability is too small; and the third allows \emph{deep expansion} beyond \(D_0\) only along sufficiently likely paths. In practice, \(\rho_{\mathrm{stop}}\) and \(\rho_{\mathrm{deep}}\) are tuned on a held-out set (Section~\ref{experiments}).

\paragraph{Adaptive Pruning under a node budget.}
To reduce wasted verification on unlikely branches, we further prune any leaf \(u\) whose cumulative probability falls below a global threshold \(\tau\in(0,1)\):
\[
p(u) \;<\; \tau \quad \Longrightarrow \quad \text{prune } u.
\]
This rule focuses the target-model verification budget on paths that are jointly plausible under the draft model. Additionally, we enforce a strict node budget \(N_{\max}\) during construction; when \(|\mathcal{N}|=N_{\max}\), expansion stops and remaining frontier nodes are treated as leaves.

\paragraph{Historical adjustment.}
Finally, DynaTree adapts a small subset of drafting hyperparameters online using recent verification outcomes. Let \(a_r\in[0,1]\) denote the per-iteration acceptance statistic at iteration \(r\) (fraction of drafted tokens committed in that iteration), and let \(\bar{a}_t\) be the sliding-window mean over the last \(W\) iterations:
\[
\bar{a}_t \;=\; \frac{1}{W}\sum_{i=0}^{W-1} a_{t-i}.
\]
Here \(W\) is a fixed window size.
We use a simple proportional-control update around a target acceptance level \(a^\star\in(0,1)\), forming a feedback control loop that stabilizes the acceptance behavior around a desired operating point. For example, for base depth \(D_0\) and high-confidence threshold \(\tau_h\),
\[
D_0 \leftarrow \mathrm{clip}\!\left(D_0 + \eta_D(\bar{a}_t-a^\star),\,1,\,D_{\max}-1\right),\quad
\tau_h \leftarrow \mathrm{clip}\!\left(\tau_h - \eta_h(\bar{a}_t-a^\star),\,0,\,1\right),
\]
with step sizes \(\eta_D,\eta_h>0\) and \(\mathrm{clip}(\cdot)\) enforcing valid ranges. Intuitively, if recent acceptance is high (\(\bar{a}_t>a^\star\)), we increase drafting aggressiveness; otherwise we become more conservative. The exact parameter subset and schedule follow the same principle and are provided in Appendix~\ref{app:algo}.

% (Moved long pseudocode to the appendix to save space in the main paper.)
We provide full pseudocode for one DynaTree iteration in Appendix~\ref{app:algo}.

\subsection{Tree Attention for Parallel Verification}
To verify all drafted tokens in one target-model forward pass, we \emph{serialize} the tree in breadth-first order (BFS)---equivalently, \emph{flatten} it into an ordered node list \((u_1,\dots,u_n)\) and token sequence \(z_{1:n}\) where \(z_i=z_{u_i}\). Let \(\mathrm{pos}(u_i)=i\). We then construct a boolean tree-attention mask \(\mathbf{A} \in \{0,1\}^{n \times (t+n)}\) such that each drafted token attends to: (i) all prefix tokens \(x_{1:t}\), and (ii) only its ancestors (including itself) among drafted nodes:
\[
\mathbf{A}_{i,j} =
\begin{cases}
1, & 1 \le j \le t,\\
1, & j=t+\mathrm{pos}(v) \text{ for some } v \in \mathrm{Anc}(u_i)\cup\{u_i\},\\
0, & \text{otherwise.}
\end{cases}
\]
This mask ensures the conditional distribution computed at each node matches the distribution of sequential decoding along its unique root-to-node path, while enabling parallel verification across different branches~\cite{specinfer,opt_tree}.

\subsection{Greedy Path Selection and Cache Update}
\paragraph{Verification signals.}
Let \(\hat{y}_{t+1} = \arg\max p_T(\cdot \mid x_{1:t})\) be the target model's greedy next token from the prefix (available from the prefix logits). For each tree node \(u\) with serialized (flattened) position \(i\), the target forward pass outputs logits \(\mathbf{s}_i\), whose argmax \(\hat{y}(u)=\arg\max \mathbf{s}_i\) corresponds to the greedy \emph{next-token} prediction after consuming the path to \(u\).

\paragraph{Longest valid path.}
DynaTree commits the longest path \(u_0 \rightarrow u_1 \rightarrow \cdots \rightarrow u_m\) such that the drafted token at each node matches the target greedy prediction from its parent context:
\[
z_{u_0}=\hat{y}_{t+1},\quad
z_{u_{k}}=\hat{y}(u_{k-1}) \;\; \text{for } k=1,\dots,m.
\]
If no drafted token matches the first greedy prediction, we fall back to committing \(\hat{y}_{t+1}\) (one token progress). After committing the matched draft tokens, we append one \emph{bonus} token \(\hat{y}(u_m)\) from the target model, mirroring the greedy speculative decoding convention and ensuring steady progress.

\paragraph{KV-cache management.}
Tree verification may populate key-value states for branches that are ultimately not committed. To maintain consistency with sequential decoding, we must restore the cache to the state corresponding to the committed prefix. Concretely, after identifying the committed path, we: (i)~discard all cached key-value pairs beyond the original prefix length \(t\); and (ii)~perform a forward pass of the committed tokens through the target model to populate the cache correctly for the next iteration. This ensures that subsequent iterations start from an identical cache state as sequential greedy decoding would produce.
While this rollback-and-rebuild introduces extra target-model work, its cost scales only with the number of committed tokens in that iteration (rather than the full drafted tree) and is amortized by committing multiple tokens per verification round.

\subsection{Correctness for Greedy Decoding}
We sketch the correctness argument for greedy decoding (the setting used throughout our experiments). The tree attention mask guarantees that for any node \(u\), the target logits at \(u\) are computed from exactly the same conditioning context as in sequential decoding along the root-to-\(u\) path. DynaTree commits a drafted token \emph{only if} it equals the target greedy argmax under that context. Therefore, every committed token matches the token that greedy decoding with \(M_T\) would produce at that position. The cache rollback-and-rebuild step ensures the subsequent iteration starts from an identical KV state. Consequently, DynaTree generates exactly the same token sequence as greedy decoding with the target model, while reducing the number of expensive target-model forward passes by verifying many candidate tokens in parallel.

\subsection{Complexity Discussion}
Let \(n=|\mathcal{N}|\le N_{\max}\) be the number of drafted nodes. Drafting requires \(O(n)\) one-token forward passes of the draft model (with cache reuse across expansions). Verification requires a single target-model forward pass over \(n\) tokens with a structured attention mask. Dynamic pruning reduces \(n\) in uncertain regions by discarding low-probability branches, improving the trade-off between draft overhead and verification parallelism.


\section{Experiments}
\label{experiments}

\subsection{Experimental Setup}
\paragraph{Models.}
We evaluate DynaTree using models from the Pythia family~\cite{pythia}. Our target model \(M_T\) is Pythia-2.8B (2.8B parameters) and our draft model \(M_D\) is Pythia-70M (70M parameters). Throughout, we use deterministic greedy decoding so the generated sequence is uniquely determined by the model and prefix.

\paragraph{Hardware and software.}
All experiments run on a single NVIDIA A100 GPU with batch size \(1\) (one prompt per run). We implement DynaTree in PyTorch~\cite{pytorch} on top of HuggingFace Transformers~\cite{transformers}. Across methods, we reuse KV caches where applicable and synchronize GPU execution for timing to obtain comparable wall-clock measurements.

\paragraph{Workloads and data preprocessing.}
We report results on WikiText-2~\citep{merity2016pointer} and PG-19~\citep{rae2019compressive}. For each sampled prompt, we generate \(T\) new tokens with greedy decoding; unless stated otherwise, \(T=1500\). To control prefill cost, prompts are truncated to a maximum length \(L_{\max}\) (\(L_{\max}=800\) for WikiText-2 and \(L_{\max}=1000\) for PG-19). We evaluate \(N=10\) prompts and discard the first \(W=2\) runs as warmup, reporting mean and standard deviation over the remaining runs. Appendix~\ref{app:exp-config} summarizes the common settings.

\subsection{Evaluation Metrics}
We use \textbf{throughput} (tokens/s) as the primary metric, computed as \(T\) divided by the wall-clock decoding time (excluding warmup). We report \textbf{speedup} relative to autoregressive decoding, \(\mathrm{speedup}=\mathrm{TPS}/\mathrm{TPS}_{\mathrm{AR}}\), under the same dataset and prompt-length cap. To characterize verification efficiency, we report the \textbf{acceptance rate} \(a\) (fraction of drafted tokens matching the target model's greedy predictions under the corresponding conditioning contexts) and the average \textbf{tokens per iteration} \(\bar{L}\) (tokens committed per verification round). For tree-based methods, we additionally report the \textbf{average committed path length} \(\bar{\ell}\) (mean depth of the greedy-consistent committed path before the bonus token). For latency, we include \textbf{time-to-first-token} (TTFT) and \textbf{time-per-output-token} (TPOT), averaged over prompts.

\subsection{Baselines}
We compare against the following baselines under identical greedy decoding settings:
\begin{itemize}
  \item \textbf{Autoregressive (AR):} Standard greedy decoding with the target model, serving as the performance baseline.
  \item \textbf{Linear speculative decoding:} A linear-chain speculative decoder that drafts \(K\) tokens with the draft model and verifies them with the target model in parallel, committing the longest greedy-consistent prefix~\citep{leviathan2023fast}.
  \item \textbf{Fixed Tree:} A static tree speculative decoder with fixed depth \(D\), fixed branching factor \(B\), and node budget \(N_{\max}\), representing a non-adaptive tree baseline. This baseline captures the core \emph{static-tree} configuration strategy used by tree-verification decoders (e.g., SpecInfer~\cite{specinfer}) without incorporating their method-specific architectural or system optimizations. We report the configuration used wherever results are shown.
  \item \textbf{DynaTree:} Our full method that augments the fixed-tree backbone with (i)~\emph{Dynamic Tree Breadth}, (ii)~\emph{Dynamic Tree Depth}, and (iii)~\emph{History Adaptation}. We ablate these components in Section~\ref{sec:ablation}.
\end{itemize}

\subsection{Main Results}
\label{main-results}
Table~\ref{tab:main-results} reports end-to-end throughput on WikiText-2 and PG-19 for \(T=1500\). DynaTree achieves the best throughput on both datasets, improving over autoregressive decoding, linear speculative decoding, and a fixed-tree baseline. On WikiText-2, DynaTree reaches 219.5~tokens/s versus 200.7~tokens/s for the static tree, demonstrating that adapting the draft structure can improve the draft--verify trade-off over a fixed configuration under the same evaluation protocol.
To ensure a fair comparison, the fixed-tree baseline configuration is selected via a grid search under the same evaluation protocol and reported as the best (or near-best) setting in the sweep (Appendix, Figure~\ref{fig:fixed-tree-sweep}).
In Tables~\ref{tab:main-results}--\ref{tab:latency-metrics}, the Fixed Tree baseline uses \(D=8\), \(B=3\), and pruning threshold \(\tau=0.1\).
In other words, our Fixed Tree baseline is not a weak strawman: it represents the best-performing static configuration found by an exhaustive sweep under the paper protocol. Even against this tuned baseline, DynaTree improves throughput by +9.4\% on WikiText-2 and +5.1\% on PG-19.

\begin{table}[tbp]
\centering
\caption{\textbf{Main results (\(T=1500\)).} Throughput (tokens/s) and speedup relative to autoregressive decoding on WikiText-2 and PG-19. Values are mean\(\pm\)std over prompts (excluding warmup). For linear speculation, we use \(K=8\) on WikiText-2 and \(K=5\) on PG-19.}
\label{tab:main-results}
\small
\begin{tabular}{lcccc}
\toprule
Method & \multicolumn{2}{c}{WikiText-2} & \multicolumn{2}{c}{PG-19} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& Throughput (tokens/s) & Speedup & Throughput (tokens/s) & Speedup \\
\midrule
AR & 133.4\(\pm\)0.5 & 1.00\(\times\) & 114.8\(\pm\)20.6 & 1.00\(\times\) \\
Linear Spec & 196.1\(\pm\)37.8 & 1.47\(\times\) & 144.9\(\pm\)28.6 & 1.26\(\times\) \\
Fixed Tree & 200.7\(\pm\)41.7 & 1.50\(\times\) & 185.5\(\pm\)33.2 & 1.62\(\times\) \\
\textbf{DynaTree} & \textbf{219.5\(\pm\)22.2} & \textbf{1.64\(\times\)} & \textbf{194.9\(\pm\)35.6} & \textbf{1.70\(\times\)} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.80\linewidth]{../figures/main_results_bars.pdf}
  \caption{\textbf{Throughput and speedup across datasets (\(T=1500\)).} Each method is shown with two bars (WikiText-2 vs.\ PG-19). DynaTree consistently improves over autoregressive and linear speculative decoding on both datasets.}
  \label{fig:main-results}
\end{figure}

Figure~\ref{fig:main-results} visualizes throughput and speedup. Linear speculative decoding can attain high acceptance when the draft closely matches the target; however, an early mismatch truncates the committed prefix and wastes the remaining drafted tokens in the chain. Tree-based drafting mitigates this failure mode by exploring multiple continuations in parallel and committing the longest greedy-consistent prefix found in the candidate set. DynaTree further reallocates verification budget across the tree based on uncertainty, reducing wasted verification in low-confidence regions while preserving longer committed prefixes in high-confidence regions.

\paragraph{Latency breakdown analysis.}
Table~\ref{tab:latency-metrics} reports TTFT and TPOT for \(T=1500\). Across datasets, speculative decoding reduces TPOT by amortizing a target-model verification over multiple output tokens; DynaTree achieves the lowest TPOT among compared methods. TTFT reflects both prefill and the first decode iteration and is therefore sensitive to implementation and cache reuse; under our implementation, speculative methods also reduce TTFT.

\begin{table}[tbp]
\centering
\caption{\textbf{Latency metrics (\(T=1500\)).} We report TTFT (latency to first output token) and TPOT (average per-token latency) on WikiText-2 and PG-19. Values are mean\(\pm\)std over prompts (excluding warmup). For linear speculation, we use \(K=8\) on WikiText-2 and \(K=5\) on PG-19.}
\label{tab:latency-metrics}
\small
\begin{tabular}{lcccc}
\toprule
Method & \multicolumn{2}{c}{WikiText-2} & \multicolumn{2}{c}{PG-19} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& TTFT (ms) & TPOT (ms) & TTFT (ms) & TPOT (ms) \\
\midrule
AR & 18.9\(\pm\)6.1 & 7.48\(\pm\)0.02 & 21.8\(\pm\)8.7 & 9.00\(\pm\)1.69 \\
Linear Spec & 14.6\(\pm\)4.0 & 5.32\(\pm\)1.24 & 11.1\(\pm\)3.3 & 7.17\(\pm\)1.42 \\
Fixed Tree & 14.6\(\pm\)4.2 & 5.30\(\pm\)1.63 & 9.7\(\pm\)0.4 & 5.55\(\pm\)0.95 \\
\textbf{DynaTree} & 14.4\(\pm\)4.0 & \textbf{4.59\(\pm\)0.47} & 9.6\(\pm\)0.6 & \textbf{5.30\(\pm\)1.02} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Verification efficiency.}
To contextualize throughput gains, Table~\ref{tab:verification-efficiency} reports auxiliary verification statistics: tokens committed per verification round (\(\bar{L}\)), average committed path length (\(\bar{\ell}\)), and the number of verification rounds required to generate \(T=1500\) tokens. Relative to autoregressive decoding, speculative methods reduce the number of target-model rounds by committing multiple tokens per round; tree-based methods further increase robustness to draft mismatches by verifying multiple continuations. DynaTree improves efficiency by adapting the draft structure online, yielding fewer rounds and larger \(\bar{L}\) under the same node budget.

\begin{table}[tbp]
\centering
\caption{\textbf{Verification efficiency metrics (\(T=1500\)).} We report acceptance rate (\textbf{Accept.}), tokens committed per verification iteration (\(\bar{L}\)), average committed path length before the bonus token (\(\bar{\ell}\)), and the total number of verification iterations (\#Iter.) needed to generate \(T\) tokens. Due to the bonus-token convention in tree verification, \textbf{Accept.} can slightly exceed 100\% for tree-based methods. Values are mean across prompts (excluding warmup).}
\label{tab:verification-efficiency}
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccccccc}
\toprule
Method & \multicolumn{4}{c}{WikiText-2} & \multicolumn{4}{c}{PG-19} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
& Accept. & \(\bar{L}\) & \(\bar{\ell}\) & \#Iter. & Accept. & \(\bar{L}\) & \(\bar{\ell}\) & \#Iter. \\
\midrule
AR & -- & 1.00 & -- & 1500 & -- & 1.00 & -- & 1500 \\
Linear Spec & 88.2\% & 6.82 & 7.05 & 220 & 92.5\% & 4.56 & 4.63 & 329 \\
Fixed Tree & 71.0\% & 6.79 & 7.10 & 221 & 64.3\% & 6.20 & 6.43 & 242 \\
\textbf{DynaTree} & \textbf{102.2\%} & \textbf{7.08} & \textbf{7.15} & \textbf{212} & \textbf{92.2\%} & \textbf{6.17} & \textbf{6.45} & \textbf{243} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Discussion.}
Two trends help explain the observed speedups. First, compared with linear drafting, tree-based methods are less sensitive to early mismatches: when the draft model diverges, alternative branches can still contain a greedy-consistent continuation, which increases the expected committed prefix length per verification step. Second, compared with a fixed tree, DynaTree allocates more of the node budget to regions where the draft model is confident while curtailing wasteful expansion in uncertain regions. This adaptivity improves verification efficiency (Table~\ref{tab:verification-efficiency}) and translates into higher end-to-end throughput (Table~\ref{tab:main-results}).
Finally, while our experiments target latency-critical single-request decoding (batch size \(=1\)), extending adaptive tree construction and verification to high-throughput continuous batching (e.g., in production serving engines) introduces additional systems challenges, such as coordinating per-request tree shapes and managing KV-cache allocation and fragmentation; we leave a thorough batched-serving study to future work.

\subsection{Ablation Study}
\label{sec:ablation}
We provide a progressive ablation on WikiText-2 under the same \(T=1500\) setting as the main benchmark (Table~\ref{tab:ablation}), measuring the contribution of each adaptive component relative to a fixed-tree backbone. Since \emph{Dynamic Tree Breadth} and \emph{Dynamic Tree Depth} are coupled in how they trade verification breadth against the length of the committed path, we report them jointly as \emph{Dynamic Tree Breadth \& Depth}. We then add \emph{History Adaptation}, which adjusts drafting hyperparameters based on recent verification outcomes.

\begin{table}[tbp]
\centering
\caption{\textbf{Ablation-style progressive comparison on WikiText-2 (\(T=1500\)).} We compare a fixed tree (\(D=5,B=2\)) with variants that add Dynamic Tree Breadth \& Depth and History Adaptation. Speedup is computed relative to the autoregressive baseline.}
\label{tab:ablation}
\small
\begin{tabular}{lccc}
    \toprule
Variant & Throughput (tokens/s) & Speedup & \(\Delta\) vs Fixed \\
    \midrule
Fixed Tree (\(D=5,B=2\)) & 188.0\(\pm\)16.5 & 1.42\(\times\) & 0.0\% \\
+ Dynamic Tree Breadth \& Depth & 213.2\(\pm\)26.1 & 1.61\(\times\) & +13.4\% \\
\textbf{+ History Adaptation} & \textbf{218.5\(\pm\)22.2} & \textbf{1.65\(\times\)} & \textbf{+16.2\%} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.85\linewidth]{../figures/ablation_progression.pdf}
  \caption{\textbf{Ablation progression on WikiText-2 (\(T=1500\)).} Each panel combines two complementary metrics. Left: throughput (bars) alongside the number of verification iterations (\#Iter.). Right: per-iteration progress (\(\bar{L}\), tokens/iter) alongside acceptance rate (separate axis). Dynamic Tree Breadth \& Depth increases per-step progress and reduces iterations; History Adaptation further improves per-step progress and iteration count.}
  \label{fig:ablation-progression}
\end{figure}

\paragraph{Interpretation.}
The throughput gains correlate with increased per-iteration progress (\(\bar{L}\)) and fewer verification iterations. History Adaptation provides an additional improvement by adjusting draft aggressiveness online, yielding higher throughput while keeping verification efficiency metrics stable (Figure~\ref{fig:ablation-progression}).

\subsection{Sequence Length Scaling}
\label{sec:length-scaling}
We evaluate how performance varies with generation length \(T\) on WikiText-2, comparing autoregressive decoding, linear speculative decoding, a fixed tree baseline, and DynaTree. Figure~\ref{fig:length-scaling} summarizes both end-to-end throughput and auxiliary verification statistics across \(T\).

\begin{figure}[tbp]
  \centering
  \includegraphics[width=\linewidth]{../figures/length_scaling_fourpanel.pdf}
  \caption{\textbf{Sequence length scaling on WikiText-2.} (a) Throughput (tokens/s) as a function of \(T\) for AR, linear speculative decoding, a fixed tree baseline, and DynaTree. (b--d) Auxiliary verification statistics: tokens per iteration (\(\bar{L}\)), average committed path length (\(\bar{\ell}\)), and acceptance rate. DynaTree shows superior scalability on longer sequences, partly because History Adaptation requires a short warm-up to reach a stable operating point.}
  \label{fig:length-scaling}
\end{figure}

\paragraph{Analysis.}
Across \(T\), DynaTree maintains strong throughput by sustaining larger committed progress per verification round while keeping acceptance rates relatively stable. In contrast, linear speculation is more sensitive to occasional early mismatches: a single rejection can truncate the reusable drafted chain and reduce effective progress per round. The auxiliary panels highlight how \(\bar{L}\), \(\bar{\ell}\), and acceptance jointly determine end-to-end throughput via their effect on the number of verification rounds.

\paragraph{Additional analyses.}
We place supplementary parameter sensitivity analyses in Appendix~\ref{app:additional-analyses}.


\section{Conclusion}
We presented DynaTree, a greedy-consistent tree-based speculative decoding method that drafts a candidate token tree with a lightweight model and verifies all drafted nodes in a single target-model pass using a structured attention mask. DynaTree adaptively allocates verification budget via \emph{Dynamic Tree Breadth} and \emph{Dynamic Tree Depth} and stabilizes decoding through \emph{History Adaptation}. Across WikiText-2 and PG-19 at \(T=1500\), DynaTree improves throughput and reduces per-token latency relative to autoregressive decoding, linear speculative decoding, and a fixed-tree baseline by increasing the expected number of committed tokens per verification iteration. Our evaluation is limited to deterministic greedy decoding with a single target--draft model pair on a single GPU, and does not cover batch-serving scenarios. Extending adaptive tree verification to batched serving, additional model pairs, and broader workloads, while further reducing system overhead, is a promising direction for future work. More broadly, DynaTree shows that careful algorithmic design can deliver strong acceleration \emph{without any additional training or architectural changes}.

\FloatBarrier % ensure all floats are placed before references
\bibliographystyle{plainnat}
\bibliography{references}

\appendix
% Avoid duplicate hyperref anchors when float counters reset/behave differently in the appendix.
% (pdfTeX warning: destination with the same identifier has been already used)
\makeatletter
\renewcommand{\theHfigure}{\theHsection.\arabic{figure}}
\renewcommand{\theHtable}{\theHsection.\arabic{table}}
\makeatother

\section{Experimental Configuration Details}
\label{app:exp-config}
\paragraph{Data traceability.}
All appendix figures and tables are generated from the logged JSON result files included in the repository. We do not introduce any unlogged (hand-entered) experimental numbers. When a result file contains an auxiliary \texttt{speedup} field, we recompute speedup as \(\mathrm{TPS}/\mathrm{TPS}_{\mathrm{AR}}\) under the same setting (Section~\ref{main-results}).

\paragraph{Common settings.}
Unless stated otherwise, we use WikiText-2 prompts, truncate the prompt length to \(L_{\max}=800\), and generate \(T\) new tokens with greedy decoding. We evaluate \(N=10\) prompts and discard the first \(W=2\) runs as warmup. Reported values are mean and standard deviation over the remaining runs.
\paragraph{PG-19 setting.}
For PG-19, we use the same model pair and greedy decoding, with a maximum prompt length \(L_{\max}=1000\).
\paragraph{Fixed-tree baseline.}
We use a static-tree speculative decoder with a fixed \((D,B,\tau)\) configuration and node budget \(N_{\max}=256\). We report the exact configuration used alongside the corresponding results. We additionally report a fixed-tree hyperparameter sweep under the paper protocol in Appendix~\ref{app:additional-analyses} to verify that the static-tree baseline used in the main benchmark is reasonably tuned.
\paragraph{DynaTree.}
Unless stated otherwise, the adaptive configuration uses base depth \(D_0=5\), maximum depth \(D_{\max}=8\) on WikiText-2 (\(D_{\max}=9\) on PG-19), branch bounds \((B_{\min},B_{\max})=(1,3)\), and confidence thresholds \((\tau_h,\tau_\ell)=(0.9,0.4)\). History Adaptation updates a small subset of these parameters based on recent verification outcomes.

\section{DynaTree Iteration Pseudocode}
\label{app:algo}
\paragraph{Reference implementation.}
Algorithm~\ref{alg:dynatree} provides a self-contained pseudocode listing for one greedy-consistent DynaTree iteration, matching the pipeline in Figure~\ref{fig:arch}.
\begin{algorithm}[t]
\caption{\textbf{DynaTree: one iteration (greedy-consistent).}}
\label{alg:dynatree}
\small
\begin{algorithmic}[1]
\Require Prefix tokens \(x_{1:t}\); target KV cache \(\mathcal{K}_T\); prefix next-token logits \(\mathbf{s}_{\text{last}}\);
branch bounds \(B_{\min}\le B_{\mathrm{mid}}\le B_{\max}\); confidence thresholds \(0<\tau_\ell<\tau_h<1\); base depth \(D_0\); max depth \(D_{\max}\); depth thresholds \(0<\rho_{\mathrm{stop}}<\rho_{\mathrm{deep}}<1\); pruning threshold \(\tau\); node budget \(N_{\max}\); history window \(W\).
\Ensure Committed tokens \(y_{t+1:t+L}\) and updated \(\mathcal{K}_T\).

\State \(\ell \gets \Call{SeqLen}{\mathcal{K}_T}\) \Comment{record prefix cache length}
\State \(\mathcal{T} \gets \Call{DraftTree}{x_{1:t}, B_{\min}, B_{\mathrm{mid}}, B_{\max}, \tau_\ell, \tau_h, D_0, D_{\max}, \rho_{\mathrm{stop}}, \rho_{\mathrm{deep}}, \tau, N_{\max}}\)
\State \(\mathbf{z}_{1:n} \gets \Call{BFSSerialize}{\mathcal{T}}\); \(\mathbf{A} \gets \Call{TreeAttnMask}{\mathcal{T}, \ell}\) \Comment{BFS serialization + tree attention mask (prefix + ancestors only)}
\State \(\mathbf{s}_{1:n} \gets M_T(\mathbf{z}_{1:n}; \mathbf{A}, \mathcal{K}_T)\); \(\hat{\mathbf{y}} \gets \arg\max \mathbf{s}_{1:n}\)
\State \(y_{t+1:t+L} \gets \Call{SelectCommit}{\mathcal{T}, \hat{\mathbf{y}}, \mathbf{s}_{\text{last}}}\)
\State \(\mathcal{K}_T \gets \Call{Crop}{\mathcal{K}_T,\ell}\); \(\mathcal{K}_T \gets M_T(y_{t+1:t+L}; \mathcal{K}_T)\) \Comment{rollback + rebuild}
\State \Call{UpdateHistory}{$y_{t+1:t+L}, \mathcal{T}, W$}; \Call{Adjust}{$\tau_\ell,\tau_h,D_0$} \Comment{historical adjustment}
\State \Return \(y_{t+1:t+L}\)

\Statex
\Function{\textproc{DraftTree}}{$x_{1:t}, B_{\min}, B_{\mathrm{mid}}, B_{\max}, \tau_\ell, \tau_h, D_0, D_{\max}, \rho_{\mathrm{stop}}, \rho_{\mathrm{deep}}, \tau, N_{\max}$}
  \Comment{Draft a candidate token tree with Dynamic Tree Breadth, Dynamic Tree Depth, and pruning under a node budget.}
  \State Run \(M_D\) on \(x_{1:t}\); let \(u_0\) be the \(\top 1\) token; initialize \(\mathcal{T}\) with root \(u_0\)
  \State \(\mathcal{A} \gets \{u_0\}\) \Comment{active frontier}
  \While{$|\mathcal{A}|>0$ \textbf{and} $|\mathcal{T}|<N_{\max}$}
    \State Pop an element \(u\) from \(\mathcal{A}\)
    \If{$\exp(\bar{\ell}_u) < \tau$} \State \textbf{continue} \EndIf \Comment{probability-threshold pruning}
    \If{$d(u)\ge D_{\max}$} \State \textbf{continue} \EndIf
    \If{$\exp(\bar{\ell}_u) < \rho_{\mathrm{stop}}$} \State \textbf{continue} \EndIf \Comment{early stopping}
    \If{$d(u)\ge D_0$ \textbf{and} $\exp(\bar{\ell}_u) \le \rho_{\mathrm{deep}}$} \State \textbf{continue} \EndIf \Comment{depth gating}
    \State Do one cached step of \(M_D\) from \(u\); compute confidence \(c(u)=\max \mathrm{softmax}(\mathbf{h}(u))\)
    \State Set \(B(u)\gets B_{\min}\) if \(c(u)\ge\tau_h\), \(B_{\max}\) if \(c(u)<\tau_\ell\), else \(B_{\mathrm{mid}}\)
    \State Take \(\top B(u)\) next-token candidates; add each child \(v\) to \(\mathcal{T}\) and push \(v\) into \(\mathcal{A}\) until \(N_{\max}\)
  \EndWhile
  \State \Return \(\mathcal{T}\)
\EndFunction

\Statex
\Function{\textproc{SelectCommit}}{$\mathcal{T}, \hat{\mathbf{y}}, \mathbf{s}_{\text{last}}$}
  \Comment{Select the longest greedy-consistent path (plus one bonus token).}
  \State \(first \gets \arg\max \mathbf{s}_{\text{last}}\)
  \State Find the longest path \(P\) where root token \(=\) \(first\), and for each edge \((u\!\rightarrow\!v)\), token\((v)=\hat{\mathbf{y}}[\text{pos}(u)]\)
  \If{$P=\emptyset$}
    \State \Return \([first]\)
  \Else
    \State \(y \gets\) tokens on \(P\)
    \State Append one bonus token \(\hat{\mathbf{y}}[\text{pos}(\text{last}(P))]\)
    \State \Return \(y\)
  \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\section{Additional Experimental Analyses}
\label{app:additional-analyses}
\paragraph{Scope.}
This appendix reports additional analyses that support the main claims: (i)~a fixed-tree sweep that validates the static-tree baseline is reasonably tuned; (ii)~sensitivity of DynaTree to drafting hyperparameters; (iii)~prompt-length sensitivity; and (iv)~memory footprint.

\paragraph{Implementation overhead.}
Our current implementation is a Python/PyTorch prototype. While this introduces nontrivial framework and control-flow overhead compared to highly optimized kernel-level implementations, DynaTree still achieves consistent speedups in our benchmarks, suggesting additional headroom from a more optimized implementation (e.g., fused kernels for tree attention and bookkeeping) in future work.

\subsection{Fixed-tree Hyperparameter Sweep}
We perform a fixed-tree hyperparameter sweep under the paper protocol to ensure the static-tree baseline is reasonably tuned. The sweep varies depth \(D\), branching factor \(B\), and pruning threshold \(\tau\) while holding the node budget fixed. Figure~\ref{fig:fixed-tree-sweep} summarizes how throughput-related metrics change across the grid, and illustrates that the baseline configuration reported in the main tables lies in a stable high-performing region rather than being a brittle outlier.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../figures/fixed_tree_sweep.pdf}
  \caption{\textbf{Fixed-tree hyperparameter sweep under the paper protocol (WikiText-2, \(T=1500\)).} Speedup is computed relative to autoregressive decoding under the same setting. Line plots use widened y-axis ranges to emphasize robustness across depth, branching, and threshold settings.}
  \label{fig:fixed-tree-sweep}
\end{figure}

\subsection{Parameter Sensitivity}
We study the sensitivity of DynaTree to key drafting hyperparameters on WikiText-2 at \(T=1500\) using a comprehensive sweep that varies confidence thresholds, branch bounds, depth ranges, and selected cross-combinations. Since the sweep does not enumerate a full Cartesian grid for every parameter pair, we visualize thresholds as a sparse 2D scatter and use mean\(\pm\)std plots for breadth and depth.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../figures/parameter_sensitivity.pdf}
\caption{\textbf{Sensitivity of DynaTree to drafting hyperparameters (WikiText-2, \(T=1500\)).} \textbf{(a)} Threshold sweep shown as a sparse 2D scatter over \((\tau_h,\tau_\ell)\), colored by throughput (tokens/s). \textbf{(b--c)} Breadth and depth sweeps shown as throughput mean\(\pm\)std across tested pair configurations. Horizontal lines denote the autoregressive and fixed-tree baselines measured in the same sweep run.}
  \label{fig:parameter-sensitivity}
\end{figure}

\subsection{Prompt Length Sensitivity}
We evaluate the impact of input context length by varying the maximum prompt length \(L_{\max}\) on WikiText-2 under the same \(T=1500\) generation setting as the main benchmark. Figure~\ref{fig:prompt-length-sensitivity} reports throughput and speedup as functions of \(L_{\max}\). This analysis isolates how longer prompts (higher prefill cost and longer KV caches) interact with speculative verification, under otherwise fixed decoding settings.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../figures/prompt_length_sensitivity.pdf}
  \caption{\textbf{Prompt length sensitivity on WikiText-2 (\(T=1500\)).} Throughput and speedup (relative to AR at the same \(L_{\max}\)) as functions of the maximum prompt length.}
  \label{fig:prompt-length-sensitivity}
\end{figure}

\subsection{Memory Footprint Analysis}
\label{app:memory}
An important practical consideration for speculative decoding methods is their memory overhead. Table~\ref{tab:memory-footprint} reports peak GPU memory consumption across methods on PG-19 and WikiText-2 during the \(T=1500\) benchmark corresponding to Tables~\ref{tab:main-results}--\ref{tab:latency-metrics}. Overall, speculative decoding introduces a modest additional peak allocation (about 3\% on average in this setup) to maintain the draft-model KV cache and intermediate verification structures.

\begin{table}[t]
\centering
\caption{\textbf{Peak GPU memory consumption comparison (\(T=1500\)).} Peak GPU memory (MB) during generation on PG-19 and WikiText-2 with Pythia-2.8B and Pythia-70M. Relative change is computed against the autoregressive baseline using the average of the two datasets. Linear speculative decoding uses \(K=5\) on PG-19 and \(K=8\) on WikiText-2 (matching Table~\ref{tab:main-results}).}
\label{tab:memory-footprint}
\small
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Peak Memory (MB)} & \multirow{2}{*}{Average (MB)} & \multirow{2}{*}{Rel. Change} \\
\cmidrule(lr){2-3}
& PG-19 & WikiText-2 & & \\
\midrule
AR (baseline) & 6156.5 & 6110.0 & 6133.3 & 0.00\% \\
Linear Spec & 6363.0 & 6315.3 & 6339.2 & +3.36\% \\
Fixed Tree (\(D{=}8,B{=}3,\tau{=}0.1\)) & 6354.6 & 6316.9 & 6335.7 & +3.30\% \\
\textbf{DynaTree} & \textbf{6355.0} & \textbf{6316.4} & \textbf{6335.7} & \textbf{+3.30\%} \\
\bottomrule
\end{tabular}
\end{table}

\end{document}