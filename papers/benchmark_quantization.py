#!/usr/bin/env python3
"""
INT8 é‡åŒ–æ€§èƒ½ Benchmark

å¯¹æ¯” FP16 å’Œ INT8 é‡åŒ–ä¸‹ Tree-based Speculative Decoding çš„æ€§èƒ½å·®å¼‚ã€‚

æµ‹è¯•å†…å®¹:
1. ååé‡å¯¹æ¯” (FP16 vs INT8)
2. å†…å­˜ä½¿ç”¨å¯¹æ¯”
3. æ¥å—ç‡å˜åŒ–
4. ä¸åŒ Token é•¿åº¦ä¸‹çš„æ€§èƒ½

Usage:
    python papers/benchmark_quantization.py
"""

import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import time
import gc
import json
from datetime import datetime
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

from transformers import AutoTokenizer

from spec_decode.core import (
    TreeSpeculativeGeneratorV2,
    load_model_fp16,
    load_model_int8,
    BITSANDBYTES_AVAILABLE,
    get_model_memory_footprint,
)


# é…ç½®
TARGET_MODEL_PATH = "/mnt/disk1/models/pythia-2.8b"
DRAFT_MODEL_PATH = "/mnt/disk1/models/pythia-70m"
DEVICE = "cuda"

# Tree V2 æœ€ä¼˜é…ç½®
TREE_DEPTH = 8
TREE_BRANCH = 3
TREE_THRESHOLD = 0.03

# æµ‹è¯•é…ç½®
TOKEN_LENGTHS = [100, 300, 500]
NUM_RUNS = 4
SKIP_FIRST = True

PROMPT = """Write a detailed technical explanation about the development of large language models. 
Cover the history, architecture innovations, training techniques, and future directions.
Begin your explanation:

Large language models have become"""


def cleanup():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()


def get_gpu_memory() -> Tuple[float, float]:
    """è·å– GPU å†…å­˜ä½¿ç”¨ (allocated, reserved) in MB"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**2
        reserved = torch.cuda.max_memory_allocated() / 1024**2
        return allocated, reserved
    return 0.0, 0.0


def print_header(title: str):
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def benchmark_config(
    target_model,
    draft_model,
    tokenizer,
    max_new_tokens: int,
    config_name: str,
    num_runs: int = NUM_RUNS
) -> Dict:
    """æµ‹è¯•å•ä¸ªé…ç½®çš„æ€§èƒ½"""
    
    gen = TreeSpeculativeGeneratorV2(
        target_model, draft_model, tokenizer,
        tree_depth=TREE_DEPTH,
        branch_factor=TREE_BRANCH,
        probability_threshold=TREE_THRESHOLD,
        max_tree_nodes=128,
        device=DEVICE,
        use_compile=False
    )
    
    results = []
    
    for i in range(num_runs):
        cleanup()
        gen.reset()
        
        # è®°å½•åˆå§‹å†…å­˜
        mem_before, _ = get_gpu_memory()
        
        torch.cuda.synchronize()
        start = time.perf_counter()
        
        _ = gen.generate(PROMPT, max_new_tokens=max_new_tokens)
        
        torch.cuda.synchronize()
        elapsed = time.perf_counter() - start
        
        # è®°å½•å³°å€¼å†…å­˜
        _, mem_peak = get_gpu_memory()
        
        stats = gen.get_stats()
        throughput = stats['total_tokens'] / elapsed
        
        if not SKIP_FIRST or i > 0:
            results.append({
                'tokens': stats['total_tokens'],
                'time': elapsed,
                'throughput': throughput,
                'acceptance_rate': stats.get('acceptance_rate', 0),
                'memory_peak_mb': mem_peak
            })
        
        status = "(warmup)" if SKIP_FIRST and i == 0 else ""
        print(f"    Run {i+1}: {throughput:.1f} t/s, accept={stats.get('acceptance_rate', 0):.1%} {status}")
    
    # è®¡ç®—å¹³å‡å€¼
    avg_throughput = sum(r['throughput'] for r in results) / len(results)
    avg_accept = sum(r['acceptance_rate'] for r in results) / len(results)
    avg_memory = sum(r['memory_peak_mb'] for r in results) / len(results)
    
    return {
        'config': config_name,
        'max_new_tokens': max_new_tokens,
        'avg_throughput': avg_throughput,
        'avg_acceptance_rate': avg_accept,
        'avg_memory_peak_mb': avg_memory,
        'runs': results
    }


def main():
    print_header("INT8 é‡åŒ–æ€§èƒ½ Benchmark")
    
    # æ£€æŸ¥ bitsandbytes
    if not BITSANDBYTES_AVAILABLE:
        print("\nâš ï¸  bitsandbytes æœªå®‰è£…ï¼Œè·³è¿‡ INT8 æµ‹è¯•")
        print("   å®‰è£…: pip install bitsandbytes")
        print("\nä»…è¿è¡Œ FP16 baseline æµ‹è¯•...")
        run_int8_test = False
    else:
        print("\nâœ“ bitsandbytes å¯ç”¨")
        run_int8_test = True
    
    # åŠ è½½ tokenizer
    print("\nåŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL_PATH)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    all_results = []
    
    # =========================================================================
    # FP16 Baseline
    # =========================================================================
    print_header("FP16 Baseline æµ‹è¯•")
    
    print("\nåŠ è½½ FP16 æ¨¡å‹...")
    cleanup()
    
    target_fp16 = load_model_fp16(TARGET_MODEL_PATH, device_map=DEVICE)
    draft_fp16 = load_model_fp16(DRAFT_MODEL_PATH, device_map=DEVICE)
    
    fp16_mem = get_model_memory_footprint(target_fp16)
    print(f"  Target æ¨¡å‹å†…å­˜: {fp16_mem['total_size_mb']:.1f} MB")
    print(f"  GPU å·²åˆ†é…: {fp16_mem['gpu_allocated_mb']:.1f} MB")
    
    # Warmup
    print("\nWarmup (5 runs)...")
    gen = TreeSpeculativeGeneratorV2(
        target_fp16, draft_fp16, tokenizer,
        tree_depth=TREE_DEPTH, branch_factor=TREE_BRANCH,
        probability_threshold=TREE_THRESHOLD,
        device=DEVICE, use_compile=False
    )
    for _ in range(5):
        gen.reset()
        _ = gen.generate(PROMPT, max_new_tokens=50)
    torch.cuda.synchronize()
    
    # æµ‹è¯•ä¸åŒ token é•¿åº¦
    fp16_results = []
    for max_tokens in TOKEN_LENGTHS:
        print(f"\næµ‹è¯• FP16 - {max_tokens} tokens:")
        result = benchmark_config(
            target_fp16, draft_fp16, tokenizer,
            max_tokens, f"FP16_{max_tokens}"
        )
        fp16_results.append(result)
        all_results.append(result)
        print(f"  >>> å¹³å‡: {result['avg_throughput']:.1f} t/s, æ¥å—ç‡: {result['avg_acceptance_rate']:.1%}")
    
    # é‡Šæ”¾ FP16 æ¨¡å‹
    del target_fp16, draft_fp16, gen
    cleanup()
    
    # =========================================================================
    # INT8 æµ‹è¯•
    # =========================================================================
    if run_int8_test:
        print_header("INT8 é‡åŒ–æµ‹è¯•")
        
        print("\nåŠ è½½ INT8 é‡åŒ–æ¨¡å‹...")
        cleanup()
        
        try:
            target_int8 = load_model_int8(TARGET_MODEL_PATH, device_map=DEVICE)
            draft_fp16 = load_model_fp16(DRAFT_MODEL_PATH, device_map=DEVICE)  # Draft ä¿æŒ FP16
            
            int8_mem = get_model_memory_footprint(target_int8)
            print(f"  Target æ¨¡å‹å†…å­˜ (INT8): {int8_mem['total_size_mb']:.1f} MB")
            print(f"  GPU å·²åˆ†é…: {int8_mem['gpu_allocated_mb']:.1f} MB")
            print(f"  å†…å­˜èŠ‚çœ: {(fp16_mem['gpu_allocated_mb'] - int8_mem['gpu_allocated_mb']):.1f} MB")
            
            # Warmup
            print("\nWarmup (5 runs)...")
            gen = TreeSpeculativeGeneratorV2(
                target_int8, draft_fp16, tokenizer,
                tree_depth=TREE_DEPTH, branch_factor=TREE_BRANCH,
                probability_threshold=TREE_THRESHOLD,
                device=DEVICE, use_compile=False
            )
            for _ in range(5):
                gen.reset()
                _ = gen.generate(PROMPT, max_new_tokens=50)
            torch.cuda.synchronize()
            
            # æµ‹è¯•ä¸åŒ token é•¿åº¦
            int8_results = []
            for max_tokens in TOKEN_LENGTHS:
                print(f"\næµ‹è¯• INT8 - {max_tokens} tokens:")
                result = benchmark_config(
                    target_int8, draft_fp16, tokenizer,
                    max_tokens, f"INT8_{max_tokens}"
                )
                int8_results.append(result)
                all_results.append(result)
                print(f"  >>> å¹³å‡: {result['avg_throughput']:.1f} t/s, æ¥å—ç‡: {result['avg_acceptance_rate']:.1%}")
            
            # é‡Šæ”¾ INT8 æ¨¡å‹
            del target_int8, draft_fp16, gen
            cleanup()
            
        except Exception as e:
            print(f"\nâŒ INT8 åŠ è½½å¤±è´¥: {e}")
            int8_results = []
    else:
        int8_results = []
    
    # =========================================================================
    # ç»“æœæ±‡æ€»
    # =========================================================================
    print_header("ğŸ“Š ç»“æœæ±‡æ€»")
    
    print(f"\n{'é…ç½®':<20} {'Tokens':<8} {'ååé‡':>12} {'æ¥å—ç‡':>10} {'å†…å­˜å³°å€¼':>12}")
    print("-" * 70)
    
    for r in all_results:
        config = r['config'].split('_')[0]
        tokens = r['max_new_tokens']
        print(f"{config:<20} {tokens:<8} {r['avg_throughput']:>10.1f} t/s {r['avg_acceptance_rate']:>9.1%} {r['avg_memory_peak_mb']:>10.1f} MB")
    
    # å¯¹æ¯”åˆ†æ
    if fp16_results and int8_results:
        print_header("ğŸ” FP16 vs INT8 å¯¹æ¯”")
        
        for fp16_r, int8_r in zip(fp16_results, int8_results):
            tokens = fp16_r['max_new_tokens']
            speedup = int8_r['avg_throughput'] / fp16_r['avg_throughput']
            memory_save = (fp16_r['avg_memory_peak_mb'] - int8_r['avg_memory_peak_mb']) / fp16_r['avg_memory_peak_mb'] * 100
            accept_diff = int8_r['avg_acceptance_rate'] - fp16_r['avg_acceptance_rate']
            
            print(f"\n{tokens} tokens:")
            print(f"  FP16: {fp16_r['avg_throughput']:.1f} t/s, INT8: {int8_r['avg_throughput']:.1f} t/s")
            print(f"  é€Ÿåº¦å˜åŒ–: {speedup:.2f}x ({'æå‡' if speedup > 1 else 'ä¸‹é™'})")
            print(f"  å†…å­˜èŠ‚çœ: {memory_save:.1f}%")
            print(f"  æ¥å—ç‡å˜åŒ–: {accept_diff:+.1%}")
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = f"results/quantization_benchmark_{timestamp}.json"
    
    os.makedirs("results", exist_ok=True)
    with open(result_file, 'w') as f:
        json.dump({
            'config': {
                'target_model': TARGET_MODEL_PATH,
                'draft_model': DRAFT_MODEL_PATH,
                'tree_depth': TREE_DEPTH,
                'tree_branch': TREE_BRANCH,
                'tree_threshold': TREE_THRESHOLD,
                'token_lengths': TOKEN_LENGTHS,
                'num_runs': NUM_RUNS
            },
            'results': all_results,
            'timestamp': timestamp
        }, f, indent=2)
    
    print(f"\n\nç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    # ç»“è®º
    print_header("ğŸ“ ç»“è®º")
    
    if int8_results:
        avg_speedup = sum(
            int8_r['avg_throughput'] / fp16_r['avg_throughput']
            for fp16_r, int8_r in zip(fp16_results, int8_results)
        ) / len(fp16_results)
        
        if avg_speedup > 1:
            print(f"\nâœ“ INT8 é‡åŒ–å¹³å‡æé€Ÿ: {avg_speedup:.2f}x")
        else:
            print(f"\nâš  INT8 é‡åŒ–å¹³å‡é™é€Ÿ: {avg_speedup:.2f}x")
        
        print("\nå»ºè®®:")
        if avg_speedup > 1.1:
            print("  - INT8 é‡åŒ–åœ¨è¯¥ç¡¬ä»¶ä¸Šæœ‰æ˜¾è‘—æ”¶ç›Šï¼Œæ¨èä½¿ç”¨")
        elif avg_speedup > 0.95:
            print("  - INT8 é‡åŒ–æ€§èƒ½åŸºæœ¬æŒå¹³ï¼Œå¯æ ¹æ®å†…å­˜éœ€æ±‚é€‰æ‹©")
        else:
            print("  - INT8 é‡åŒ–æœ‰æ€§èƒ½æŸå¤±ï¼Œå»ºè®®ç»§ç»­ä½¿ç”¨ FP16")
    else:
        print("\næœªèƒ½å®Œæˆ INT8 æµ‹è¯•ï¼Œè¯·å®‰è£… bitsandbytes åé‡è¯•")


if __name__ == "__main__":
    main()






