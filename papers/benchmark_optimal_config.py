#!/usr/bin/env python3
"""
åœ¨æœ€ä¼˜é…ç½® (500 tokens, D=8, B=3, t=0.03) ä¸‹å¯¹æ¯”æ‰€æœ‰ Spec Decode æ–¹æ³•

æµ‹è¯•æ–¹æ³•:
1. Baseline (çº¯è‡ªå›å½’)
2. HuggingFace Assisted Generation
3. Linear Speculative Decoding (K=5,6,7,8)
4. Tree-based Speculative Decoding V2
5. StreamingLLM + Spec Decode
6. Tree + StreamingLLM (å¦‚æœå­˜åœ¨)
"""

import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from spec_decode.core import (
    SpeculativeGenerator,
    TreeSpeculativeGeneratorV2,
    StreamingSpeculativeGenerator,
)
import time
import gc
import warnings
warnings.filterwarnings('ignore')

# é…ç½®
DEVICE = 'cuda'
TARGET_MODEL = '/mnt/disk1/models/pythia-2.8b'
DRAFT_MODEL = '/mnt/disk1/models/pythia-70m'
MAX_NEW_TOKENS = 500
NUM_RUNS = 5  # æ¯ä¸ªæ–¹æ³•è¿è¡Œæ¬¡æ•°
SKIP_FIRST = True  # è·³è¿‡é¦–æ¬¡ warmup

# Tree V2 æœ€ä¼˜å‚æ•°
TREE_DEPTH = 8
TREE_BRANCH = 3
TREE_THRESHOLD = 0.03

PROMPT = """Write a detailed technical explanation about the development of large language models. 
Cover the history, architecture innovations, training techniques, and future directions.
Begin your explanation:

Large language models have become"""


def cleanup():
    gc.collect()
    torch.cuda.empty_cache()


def print_header(title):
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def measure_method(name, run_fn, num_runs=NUM_RUNS, skip_first=SKIP_FIRST):
    """é€šç”¨æµ‹é‡å‡½æ•°"""
    results = []
    
    for i in range(num_runs):
        cleanup()
        torch.cuda.synchronize()
        
        start = time.perf_counter()
        tokens, extra_stats = run_fn()
        torch.cuda.synchronize()
        elapsed = time.perf_counter() - start
        
        tp = tokens / elapsed
        
        if not skip_first or i > 0:
            results.append({
                'tokens': tokens,
                'time': elapsed,
                'throughput': tp,
                **extra_stats
            })
        
        status = "(warmup, è·³è¿‡)" if skip_first and i == 0 else ""
        print(f"    Run {i+1}: {tokens} tokens, {elapsed:.2f}s, {tp:.1f} t/s {status}")
    
    # è®¡ç®—å¹³å‡å€¼
    avg_tp = sum(r['throughput'] for r in results) / len(results)
    avg_time = sum(r['time'] for r in results) / len(results)
    
    return {
        'name': name,
        'avg_throughput': avg_tp,
        'avg_time': avg_time,
        'runs': results
    }


def main():
    print_header("åŠ è½½æ¨¡å‹")
    print(f"  Target: {TARGET_MODEL}")
    print(f"  Draft: {DRAFT_MODEL}")
    
    tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    target_model = AutoModelForCausalLM.from_pretrained(
        TARGET_MODEL, torch_dtype=torch.float16, device_map=DEVICE
    )
    draft_model = AutoModelForCausalLM.from_pretrained(
        DRAFT_MODEL, torch_dtype=torch.float16, device_map=DEVICE
    )
    
    # å……åˆ† Warmup
    print_header("Warmup (10 runs)")
    for i in range(10):
        cleanup()
        input_ids = tokenizer(PROMPT, return_tensors='pt').input_ids.to(DEVICE)
        with torch.inference_mode():
            _ = target_model.generate(
                input_ids, max_new_tokens=50, do_sample=False,
                eos_token_id=None, assistant_model=draft_model
            )
        torch.cuda.synchronize()
        print(f"  Warmup {i+1}/10 å®Œæˆ")
    
    print_header(f"æ€§èƒ½æµ‹è¯•: {MAX_NEW_TOKENS} tokens, {NUM_RUNS} runs")
    print(f"  Tree V2 é…ç½®: D={TREE_DEPTH}, B={TREE_BRANCH}, t={TREE_THRESHOLD}")
    
    all_results = []
    
    # =========================================================================
    # 1. Baseline
    # =========================================================================
    print("\n[1/7] Baseline (çº¯è‡ªå›å½’)...")
    
    def run_baseline():
        input_ids = tokenizer(PROMPT, return_tensors='pt').input_ids.to(DEVICE)
        with torch.inference_mode():
            out = target_model.generate(
                input_ids, max_new_tokens=MAX_NEW_TOKENS, do_sample=False,
                eos_token_id=None, pad_token_id=tokenizer.pad_token_id
            )
        tokens = out.shape[1] - input_ids.shape[1]
        return tokens, {}
    
    baseline_result = measure_method("Baseline (AR)", run_baseline)
    all_results.append(baseline_result)
    baseline_tp = baseline_result['avg_throughput']
    print(f"  >>> å¹³å‡: {baseline_tp:.1f} t/s (1.00x)")
    
    # =========================================================================
    # 2. HuggingFace Assisted
    # =========================================================================
    print("\n[2/7] HuggingFace Assisted Generation...")
    
    def run_hf_assisted():
        input_ids = tokenizer(PROMPT, return_tensors='pt').input_ids.to(DEVICE)
        with torch.inference_mode():
            out = target_model.generate(
                input_ids, max_new_tokens=MAX_NEW_TOKENS, do_sample=False,
                eos_token_id=None, assistant_model=draft_model,
                pad_token_id=tokenizer.pad_token_id
            )
        tokens = out.shape[1] - input_ids.shape[1]
        return tokens, {}
    
    hf_result = measure_method("HF Assisted", run_hf_assisted)
    all_results.append(hf_result)
    print(f"  >>> å¹³å‡: {hf_result['avg_throughput']:.1f} t/s ({hf_result['avg_throughput']/baseline_tp:.2f}x)")
    
    # =========================================================================
    # 3. Linear Speculative Decoding (å¤šä¸ª K å€¼)
    # =========================================================================
    for K in [5, 6, 7, 8]:
        print(f"\n[3/7] Linear Spec Decode K={K}...")
        
        gen = SpeculativeGenerator(
            target_model, draft_model, tokenizer,
            K=K, max_len=8192, device=DEVICE, use_compile=False
        )
        
        def run_linear():
            gen.reset()
            _ = gen.generate(PROMPT, max_new_tokens=MAX_NEW_TOKENS)
            stats = gen.get_stats()
            return stats['total_tokens'], {
                'acceptance_rate': stats.get('acceptance_rate', 0),
                'tokens_per_round': stats.get('tokens_per_round', 0)
            }
        
        linear_result = measure_method(f"Linear K={K}", run_linear)
        all_results.append(linear_result)
        
        # è·å–é¢å¤–ç»Ÿè®¡
        accept_rate = sum(r.get('acceptance_rate', 0) for r in linear_result['runs']) / len(linear_result['runs'])
        tpr = sum(r.get('tokens_per_round', 0) for r in linear_result['runs']) / len(linear_result['runs'])
        
        print(f"  >>> å¹³å‡: {linear_result['avg_throughput']:.1f} t/s ({linear_result['avg_throughput']/baseline_tp:.2f}x)")
        print(f"      æ¥å—ç‡: {accept_rate:.1%}, æ¯è½® tokens: {tpr:.2f}")
    
    # =========================================================================
    # 4. Tree V2 Speculative Decoding (æœ€ä¼˜é…ç½®)
    # =========================================================================
    print(f"\n[4/7] Tree V2 (D={TREE_DEPTH} B={TREE_BRANCH} t={TREE_THRESHOLD})...")
    
    tree_gen = TreeSpeculativeGeneratorV2(
        target_model, draft_model, tokenizer,
        tree_depth=TREE_DEPTH, branch_factor=TREE_BRANCH,
        probability_threshold=TREE_THRESHOLD,
        max_tree_nodes=128, device=DEVICE, use_compile=False
    )
    
    def run_tree():
        tree_gen.reset()
        _ = tree_gen.generate(PROMPT, max_new_tokens=MAX_NEW_TOKENS)
        stats = tree_gen.get_stats()
        return stats['total_tokens'], {
            'acceptance_rate': stats.get('acceptance_rate', 0),
            'avg_path_length': stats.get('avg_path_length', 0)
        }
    
    tree_result = measure_method(f"Tree V2 D={TREE_DEPTH}B={TREE_BRANCH}t={TREE_THRESHOLD}", run_tree)
    all_results.append(tree_result)
    
    accept_rate = sum(r.get('acceptance_rate', 0) for r in tree_result['runs']) / len(tree_result['runs'])
    path_len = sum(r.get('avg_path_length', 0) for r in tree_result['runs']) / len(tree_result['runs'])
    
    print(f"  >>> å¹³å‡: {tree_result['avg_throughput']:.1f} t/s ({tree_result['avg_throughput']/baseline_tp:.2f}x)")
    print(f"      æ¥å—ç‡: {accept_rate:.1%}, å¹³å‡è·¯å¾„é•¿åº¦: {path_len:.2f}")
    
    # =========================================================================
    # 5. StreamingLLM + Spec Decode
    # =========================================================================
    for cache_len in [512, 1024]:
        print(f"\n[5/7] StreamingLLM + Spec Decode (cache={cache_len})...")
        
        stream_gen = StreamingSpeculativeGenerator(
            target_model, draft_model, tokenizer,
            K=6, max_len=8192, max_cache_len=cache_len,
            start_size=4, recent_size=cache_len-4,
            device=DEVICE, use_compile=False
        )
        
        def run_streaming():
            stream_gen.reset()
            _ = stream_gen.generate(PROMPT, max_new_tokens=MAX_NEW_TOKENS)
            stats = stream_gen.get_stats()
            return stats['total_tokens'], {
                'acceptance_rate': stats.get('acceptance_rate', 0),
                'compress_count': stats.get('compress_count', 0)
            }
        
        stream_result = measure_method(f"Streaming K=6 cache={cache_len}", run_streaming)
        all_results.append(stream_result)
        
        accept_rate = sum(r.get('acceptance_rate', 0) for r in stream_result['runs']) / len(stream_result['runs'])
        compress = sum(r.get('compress_count', 0) for r in stream_result['runs']) / len(stream_result['runs'])
        
        print(f"  >>> å¹³å‡: {stream_result['avg_throughput']:.1f} t/s ({stream_result['avg_throughput']/baseline_tp:.2f}x)")
        print(f"      æ¥å—ç‡: {accept_rate:.1%}, å‹ç¼©æ¬¡æ•°: {compress:.0f}")
    
    # =========================================================================
    # 6. Tree + StreamingLLM (å¦‚æœå­˜åœ¨)
    # =========================================================================
    try:
        from spec_decode.core import TreeStreamingSpeculativeGenerator
        
        print(f"\n[6/7] Tree + StreamingLLM (D={TREE_DEPTH} B={TREE_BRANCH} cache=1024)...")
        
        tree_stream_gen = TreeStreamingSpeculativeGenerator(
            target_model, draft_model, tokenizer,
            tree_depth=TREE_DEPTH, branch_factor=TREE_BRANCH,
            probability_threshold=TREE_THRESHOLD,
            max_tree_nodes=128, max_cache_len=1024,
            start_size=4, recent_size=1020,
            device=DEVICE, use_compile=False
        )
        
        def run_tree_streaming():
            tree_stream_gen.reset()
            _ = tree_stream_gen.generate(PROMPT, max_new_tokens=MAX_NEW_TOKENS)
            stats = tree_stream_gen.get_stats()
            return stats['total_tokens'], {
                'acceptance_rate': stats.get('acceptance_rate', 0),
                'avg_path_length': stats.get('avg_path_length', 0)
            }
        
        tree_stream_result = measure_method("Tree+Streaming", run_tree_streaming)
        all_results.append(tree_stream_result)
        
        print(f"  >>> å¹³å‡: {tree_stream_result['avg_throughput']:.1f} t/s ({tree_stream_result['avg_throughput']/baseline_tp:.2f}x)")
    except ImportError:
        print("\n[6/7] Tree + StreamingLLM - è·³è¿‡ (æ¨¡å—æœªæ‰¾åˆ°)")
    except Exception as e:
        print(f"\n[6/7] Tree + StreamingLLM - é”™è¯¯: {e}")
    
    # =========================================================================
    # ç»“æœæ±‡æ€»
    # =========================================================================
    print_header("ğŸ“Š ç»“æœæ±‡æ€»")
    
    # æŒ‰åŠ é€Ÿæ¯”æ’åº
    sorted_results = sorted(all_results, key=lambda x: x['avg_throughput'], reverse=True)
    
    print(f"\n{'æ’å':<4} {'æ–¹æ³•':<35} {'ååé‡':>12} {'åŠ é€Ÿæ¯”':>10}")
    print("-" * 65)
    
    for i, r in enumerate(sorted_results):
        speedup = r['avg_throughput'] / baseline_tp
        marker = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
        print(f"{marker}{i+1:<3} {r['name']:<35} {r['avg_throughput']:>10.1f} t/s {speedup:>8.2f}x")
    
    # å…³é”®å¯¹æ¯”
    print_header("ğŸ” å…³é”®å¯¹æ¯”åˆ†æ")
    
    # æ‰¾åˆ°å„ç±»æ–¹æ³•çš„æœ€ä½³ç»“æœ
    hf_best = next((r for r in sorted_results if 'HF' in r['name']), None)
    linear_best = max([r for r in sorted_results if 'Linear' in r['name']], key=lambda x: x['avg_throughput'], default=None)
    tree_best = next((r for r in sorted_results if 'Tree V2' in r['name']), None)
    stream_best = max([r for r in sorted_results if 'Streaming' in r['name'] and 'Tree' not in r['name']], key=lambda x: x['avg_throughput'], default=None)
    
    print(f"""
é…ç½®: {MAX_NEW_TOKENS} tokens, Tree V2 (D={TREE_DEPTH}, B={TREE_BRANCH}, t={TREE_THRESHOLD})

æ–¹æ³•å¯¹æ¯”:
  Baseline:              {baseline_tp:>6.1f} t/s (1.00x)
  HF Assisted:           {hf_best['avg_throughput'] if hf_best else 0:>6.1f} t/s ({hf_best['avg_throughput']/baseline_tp if hf_best else 0:.2f}x)
  Linear (æœ€ä½³ K):       {linear_best['avg_throughput'] if linear_best else 0:>6.1f} t/s ({linear_best['avg_throughput']/baseline_tp if linear_best else 0:.2f}x) [{linear_best['name'] if linear_best else 'N/A'}]
  Tree V2:               {tree_best['avg_throughput'] if tree_best else 0:>6.1f} t/s ({tree_best['avg_throughput']/baseline_tp if tree_best else 0:.2f}x)
  StreamingLLM (æœ€ä½³):   {stream_best['avg_throughput'] if stream_best else 0:>6.1f} t/s ({stream_best['avg_throughput']/baseline_tp if stream_best else 0:.2f}x) [{stream_best['name'] if stream_best else 'N/A'}]
""")
    
    # ç»“è®º
    print_header("ğŸ“ ç»“è®º")
    
    best = sorted_results[0]
    print(f"""
1. æœ€å¿«æ–¹æ³•: {best['name']} ({best['avg_throughput']:.1f} t/s, {best['avg_throughput']/baseline_tp:.2f}x)

2. Tree V2 vs Linear:
   - Tree V2: {tree_best['avg_throughput'] if tree_best else 0:.1f} t/s
   - Linear æœ€ä½³: {linear_best['avg_throughput'] if linear_best else 0:.1f} t/s
   - å·®å¼‚: {((tree_best['avg_throughput'] if tree_best else 0) - (linear_best['avg_throughput'] if linear_best else 0)):.1f} t/s
   - Tree V2 {'ä¼˜äº' if tree_best and linear_best and tree_best['avg_throughput'] > linear_best['avg_throughput'] else 'ä¸å¦‚'} Linear

3. HF Assisted æ˜¾è‘—é¢†å…ˆï¼Œå› ä¸º:
   - HuggingFace å†…éƒ¨ä¼˜åŒ–æ›´å½»åº•
   - ä½¿ç”¨ C++ å®ç°çš„å…³é”®è·¯å¾„
   - æ›´é«˜æ•ˆçš„ KV cache ç®¡ç†

4. è‡ªå®šä¹‰å®ç°çš„ä»·å€¼:
   - å¯ä»¥ä¸ StreamingLLM ç»“åˆç”¨äºé•¿åºåˆ—
   - æ”¯æŒæ›´çµæ´»çš„å®šåˆ¶ (å¦‚ Tree-based)
   - é€‚åˆç ”ç©¶å’Œæ•™å­¦ç›®çš„
""")


if __name__ == "__main__":
    main()






