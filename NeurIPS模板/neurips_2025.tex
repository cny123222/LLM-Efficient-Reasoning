\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025
\PassOptionsToPackage{numbers,sort&compress}{natbib}

% The authors should use one of these tracks.
% Before accepting by the NeurIPS conference, select one of the options below.
% 0. "default" for submission
% \usepackage{neurips_2025}
% Using "final" option to show author names (not for actual submission)
\usepackage[preprint]{neurips_2025}
% the "default" option is equal to the "main" option, which is used for the Main Track with double-blind reviewing.
% 1. "main" option is used for the Main Track
%  \usepackage[main]{neurips_2025}
% 2. "position" option is used for the Position Paper Track
%  \usepackage[position]{neurips_2025}
% 3. "dandb" option is used for the Datasets & Benchmarks Track
 % \usepackage[dandb]{neurips_2025}
% 4. "creativeai" option is used for the Creative AI Track
%  \usepackage[creativeai]{neurips_2025}
% 5. "sglblindworkshop" option is used for the Workshop with single-blind reviewing
 % \usepackage[sglblindworkshop]{neurips_2025}
% 6. "dblblindworkshop" option is used for the Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop]{neurips_2025}

% After being accepted, the authors should add "final" behind the track to compile a camera-ready version.
% 1. Main Track
 % \usepackage[main, final]{neurips_2025}
% 2. Position Paper Track
%  \usepackage[position, final]{neurips_2025}
% 3. Datasets & Benchmarks Track
 % \usepackage[dandb, final]{neurips_2025}
% 4. Creative AI Track
%  \usepackage[creativeai, final]{neurips_2025}
% 5. Workshop with single-blind reviewing
%  \usepackage[sglblindworkshop, final]{neurips_2025}
% 6. Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop, final]{neurips_2025}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote.
% For workshops (5., 6.), the authors should add the name of the workshop, "\workshoptitle" command is used to set the workshop title.
% \workshoptitle{WORKSHOP TITLE}

% "preprint" option is used for arXiv or other preprint submissions
 % \usepackage[preprint]{neurips_2025}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{multirow}       % multirow cells in tables
\usepackage{graphicx}       % figures
\usepackage{amsmath}        % math
\usepackage{amssymb}        % math symbols
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{algorithm}     % float wrapper
\usepackage{algpseudocode} % algorithmicx style (supports \Call)

% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote. 
\title{DynaTree: Confidence-Aware Adaptive Tree Speculative Decoding for Efficient LLM Inference}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Nuoyan Chen$^*$ \quad
  Jiamin Liu$^*$ \quad
  Zhaocheng Li\thanks{Equal contribution.} \\
  School of Computer Science\\
  Shanghai Jiao Tong University\\
  \texttt{\{cny123222, logic-1.0, lzc050419\}@sjtu.edu.cn}
}


\begin{document}


\maketitle


\begin{abstract}
Autoregressive decoding in large language models (LLMs) is fundamentally sequential and therefore underutilizes modern accelerator parallelism during token generation. Speculative decoding mitigates this bottleneck by letting a lightweight draft model propose multiple tokens that are verified in parallel by the target model; however, linear variants explore only a single draft chain per step and can waste substantial computation when early tokens are rejected, while existing tree-based approaches employ \emph{fixed} structures that cannot adapt to varying draft model confidence. We propose \textbf{DynaTree}, a tree-based speculative decoding framework with \emph{confidence-aware adaptive branching} that dynamically adjusts tree structure based on draft model uncertainty through adaptive per-node branching, dynamic depth control, and historical acceptance tuning, combined with probability-threshold pruning under an explicit node budget. Experiments on Pythia models demonstrate that DynaTree achieves 210.8~tokens/sec on WikiText-2 (1.61$\times$ speedup, 94.7\% acceptance rate), outperforming fixed tree structures by 16.3\% and consistently surpassing linear speculative decoding baselines across diverse datasets.
\end{abstract}


\section{Introduction}

Large language models (LLMs) are typically deployed with autoregressive decoding, where each output token is generated after conditioning on all previously generated tokens. While transformer inference can exploit parallelism during the prefill stage, the decode stage remains inherently sequential and requires a full forward pass per token, leading to poor hardware utilization and high latency~\cite{flashattention,vllm}.

Speculative decoding alleviates this bottleneck by separating \emph{proposal} and \emph{verification}~\cite{leviathan2023fast}. A small draft model proposes several candidate tokens, and the target model verifies them in parallel; when the proposal matches the target distribution, multiple tokens can be committed per iteration. Importantly, with rejection sampling, speculative decoding preserves the exact output distribution of the target model~\cite{decoding_speculative}.

In practice, most speculative decoding systems employ \emph{linear} drafting: the draft model proposes a single chain of $K$ tokens. This design is brittle under draft--target mismatch. A rejection at an early position forces all subsequent drafted tokens to be discarded, wasting both draft computation and target-model verification work. This single-path exploration fundamentally limits achievable speedups, as the system cannot recover from early errors without restarting the drafting process~\cite{decoding_speculative}.

Tree-based speculation offers a natural remedy. When multiple plausible next tokens compete, exploring several continuations in parallel increases the chance that at least one path aligns with the target model, thereby improving the expected number of accepted tokens per verification step. The draft expands multiple candidates via top-$k$ branching, and the target verifies the resulting token tree in parallel with a structured, causality-preserving attention mask. This multi-path exploration addresses the fundamental brittleness of linear drafting.

While tree-based drafting addresses the single-path limitation of linear methods, existing approaches typically employ \emph{fixed} tree configurations with predetermined depth and branching factor~\cite{specinfer,medusa}. This rigid structure cannot adapt to the draft model's varying prediction confidence, creating an \emph{efficiency gap}: high-confidence predictions waste compute exploring unnecessary branches, while uncertain predictions suffer from insufficient exploration. Recent adaptive methods~\cite{cm_asd,adaeagle,cas_spec} adjust draft length or employ learned predictors, yet most focus on linear speculation rather than fundamentally restructuring the tree itself. We hypothesize that \emph{confidence-aware} tree construction---dynamically adjusting branching per node based on draft uncertainty---can bridge this gap while maintaining robust exploration.

We present \textbf{DynaTree}, which addresses the efficiency gap through confidence-aware adaptive branching that dynamically adjusts tree structure based on draft model uncertainty. Our three-phase mechanism adapts per-node branching (1--3 branches), implements dynamic depth control via early stopping and deep expansion, and tunes parameters based on historical acceptance rates. Combined with probability-threshold pruning under an explicit node budget, DynaTree verifies candidate paths in a single forward pass using tree attention. Empirically, DynaTree achieves 210.8~tokens/sec on WikiText-2 (1.61$\times$ speedup, 94.7\% acceptance rate), outperforming fixed tree structures by 16.3\%.

In summary, our contributions are:
\begin{itemize}
  \item We propose a tree-based speculative decoding framework with \textbf{confidence-aware adaptive branching} that dynamically adjusts tree structure based on draft model uncertainty, directly addressing the efficiency gap in static configurations.
  
  \item We introduce a three-phase adaptive mechanism combining confidence-based branching, dynamic depth control, and historical parameter tuning. Our ablation study reveals that dynamic depth contributes most to performance gains.
  
  \item Experiments demonstrate that DynaTree achieves 1.61$\times$ speedup with 94.7\% acceptance rate, outperforming fixed tree baselines by 16.3\% across WikiText-2 and PG-19.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../figures/decode-v1.png}
  \caption{\textbf{Comparison of three decoding paradigms.} \textbf{Left:} Autoregressive (AR) decoding generates tokens sequentially, requiring one LLM forward pass per token. \textbf{Middle:} Linear speculative decoding drafts a single token chain; early rejection wastes all subsequent drafted tokens. \textbf{Right:} Tree-based speculative decoding (DynaTree) explores multiple paths in parallel, enabling recovery from draft errors and committing longer sequences per iteration. The multi-path exploration fundamentally addresses the brittleness of linear drafting while maintaining output correctness through structured tree attention verification.}
  \label{fig:decode-comparison}
\end{figure}

\section{Related Work}

\subsection{Speculative Decoding}

Speculative decoding accelerates autoregressive generation by decoupling \emph{proposal} and \emph{verification}: a lightweight draft model proposes multiple tokens, and the target model verifies these candidates in parallel while preserving the exact output distribution~\cite{leviathan2023fast,decoding_speculative,draft_tradeoff}. The memory-bound nature of LLM inference (approximately 1 FLOP/byte operational intensity) makes parallel verification particularly valuable~\cite{flashattention,vllm}. Recent work highlights robustness challenges across heterogeneous workloads and long-context inputs~\cite{spin,owl,judge_decoding}, but the dominant \emph{linear} drafting paradigm suffers from a fundamental inefficiency: when early tokens are rejected, all downstream draft tokens are discarded, wasting computation.

\subsection{Tree-Based Speculative Decoding}

To overcome single-path inefficiency, tree-based methods draft multiple candidate continuations and verify them in one target-model forward pass using structured attention masks~\cite{specinfer,opt_tree,medusa,traversal_verification}. SpecInfer~\cite{specinfer} pioneered token tree verification with expansion-based and merge-based construction mechanisms. OPT-Tree~\cite{opt_tree} algorithmically searches for tree structures that maximize expected acceptance length, while Medusa~\cite{medusa} augments models with multiple decoding heads to eliminate separate draft models. However, these methods predominantly use \emph{fixed} tree configurations with static depth $D$ and branching factor $B$, creating an efficiency gap when draft confidence varies~\cite{specinfer,medusa}.

Recent adaptive approaches address this rigidity through dynamic parameter adjustment. CM-ASD~\cite{cm_asd} modulates drafting length and verification thresholds based on entropy, logit margin, and softmax margin, achieving 4--5$\times$ speedups. AdaEAGLE~\cite{adaeagle} employs a lightweight MLP to predict optimal draft length per iteration. CAS-Spec~\cite{cas_spec} introduces dynamic tree cascades with acceptance rate heuristics, improving throughput by 47\%. While promising, these methods focus on linear speculation or require learned predictors. In contrast, DynaTree directly restructures trees via confidence-aware branching, dynamic depth control, and historical adjustment—achieving 16.3\% gains over fixed configurations while remaining training-free.

\subsection{Dynamic Pruning Strategies}

Exponential candidate growth necessitates pruning to balance exploration and verification cost. ProPD~\cite{propd} employs top-$k$ early prediction heads and weighted regression to remove low-utility branches, reducing computation by 2$\times$. CAST~\cite{cast} formalizes cost-aware breadth and depth pruning, explicitly modeling verification overhead per layer. DySpec~\cite{dyspec} uses greedy confidence-guided expansion, while RASD~\cite{rasd} prunes retrieval candidates outside the draft model's top-$k$ predictions. AdaSD~\cite{adasd} introduces hyperparameter-free thresholds based on entropy and Jensen-Shannon distance, achieving 49\% speedups. These methods differ in adaptation mechanisms—from offline heuristics to online predictors. DynaTree adopts probability-threshold pruning with explicit node budgets, prioritizing simplicity and training-free integration while maintaining strong empirical performance.


\section{Methodology}
\label{method}

\subsection{Problem Setup and Notation}
Let \(M_T\) denote a target autoregressive language model and \(M_D\) a smaller draft model. Given a prefix (prompt) \(x_{1:t}\), greedy decoding with \(M_T\) produces tokens \(y_{t+1}, y_{t+2}, \dots\) where
\[
y_{i} \;=\; \arg\max_{v \in \mathcal{V}} p_T(v \mid x_{1:i-1}).
\]
Speculative decoding accelerates generation by proposing candidate tokens with \(M_D\) and verifying them with \(M_T\), while preserving the greedy output when the verification rule only commits tokens that match the target greedy predictions.

\subsection{Overview of DynaTree}
DynaTree generalizes linear speculative decoding from a single draft chain to a \emph{draft token tree}. In each iteration, it first performs \emph{adaptive tree drafting} with \(M_D\), where the effective tree breadth and depth are determined on-the-fly from the draft distribution and the cumulative path probability. Concretely, for a draft node \(u\) with draft logits \(\mathbf{h}(u)\), we define the draft confidence \(c(u)=\max_{v\in\mathcal{V}}\mathrm{softmax}(\mathbf{h}(u))_v\) and use it to select a per-node branching factor \(B(u)\in\{B_{\min},B_{\mathrm{mid}},B_{\max}\}\). We further control expansion depth via the cumulative probability \(p(u)=\exp(\bar{\ell}_u)\), combining early stopping for low-probability branches with deeper expansion along high-probability paths. The resulting candidate tree is then \textbf{verified in parallel} in a single forward pass of \(M_T\) using tree attention, followed by \textbf{greedy path selection} and \textbf{KV-cache update} for committed tokens. Finally, DynaTree maintains a short window of recent acceptance statistics to adjust drafting thresholds for subsequent iterations.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../figures/dynatree-v9.5.png}
  \caption{\textbf{One iteration of DynaTree decoding.} The process consists of six main stages: (1)~\emph{Adaptive Tree Drafting:} The draft model expands a candidate tree with three adaptive mechanisms: confidence-aware branching adjusts the number of child nodes (1--3) per expansion based on draft model confidence; dynamic depth control implements early stopping for low cumulative probability branches and deep expansion for high-probability paths; adaptive pruning removes branches below probability threshold $\tau$ or exceeding node budget $N_{\max}$. (2)~\emph{Flattening \& Masking:} The pruned tree is serialized in breadth-first order, and a causal attention mask is constructed to ensure each node attends only to its ancestors. (3)~\emph{Parallel Verification:} The target model verifies all candidates in a single forward pass. (4)~\emph{Path Selection:} The longest path where drafted tokens match the target model's greedy predictions is identified. (5)~\emph{Cache Update:} The committed tokens are used to update the context and key-value cache for the next iteration. (6)~\emph{Historical Adjustment:} Acceptance rate from recent rounds feeds back to adjust confidence thresholds and base depth for the next iteration. This three-phase adaptive mechanism enables efficient multi-path exploration while maintaining correctness guarantees for greedy decoding.}
  \label{fig:arch}
\end{figure}

\subsection{Adaptive Tree Drafting}
We maintain a token tree \(\mathcal{T}=(\mathcal{N}, \mathcal{E})\) whose nodes \(u \in \mathcal{N}\) correspond to drafted tokens. Each node \(u\) is associated with: (i)~\emph{token} \(z_u \in \mathcal{V}\); (ii)~\emph{parent} \(\pi(u)\); (iii)~\emph{depth} \(d(u)\) from root; (iv)~\emph{draft log-probability} \(\ell_u = \log p_D(z_u \mid \text{prefix}(\pi(u)))\); and (v)~\emph{cumulative log-probability} \(\bar{\ell}_u = \sum_{v \in \text{path}(u)} \ell_v\), where \(\text{path}(u)\) denotes all nodes from root to \(u\) along the tree.

\paragraph{Tree expansion.}
Starting from the current prefix \(x_{1:t}\), we construct \(\mathcal{T}\) in a breadth-first manner under a strict node budget \(N_{\max}\). For any expandable node \(u\), let \(q_D(\cdot \mid u)\) denote the draft distribution conditioned on the unique root-to-\(u\) prefix, and define the local confidence
\[
c(u)\;=\;\max_{v\in\mathcal{V}} q_D(v\mid u).
\]
We select a \emph{per-node} branching factor via a confidence rule
\[
B(u)=
\begin{cases}
B_{\min}, & c(u)\ge \tau_h,\\
B_{\mathrm{mid}}, & \tau_\ell \le c(u) < \tau_h,\\
B_{\max}, & c(u)<\tau_\ell,
\end{cases}
\]
where \(0<\tau_\ell<\tau_h<1\) are confidence thresholds and \(1\le B_{\min}\le B_{\mathrm{mid}}\le B_{\max}\) are integer branch bounds.
and expand \(u\) by adding the \(B(u)\) highest-probability children under \(q_D(\cdot\mid u)\). To adapt depth, we use the cumulative path probability \(p(u)=\exp(\bar{\ell}_u)\): low-probability branches are terminated early, while high-probability paths may be expanded beyond a base depth. Concretely, a node at depth \(d(u)\) is eligible for expansion only if it satisfies the depth-gating rule (defined in the next paragraph) and \(|\mathcal{N}|<N_{\max}\). Implementation details for cache reuse during expansion are deferred to Appendix~\ref{app:algo}.

\paragraph{Dynamic depth control.}
Let \(D_0\) denote a base depth and \(D_{\max}\) a hard maximum depth. We gate expansion using two probability thresholds \(\rho_{\mathrm{stop}} < \rho_{\mathrm{deep}}\) on the cumulative path probability \(p(u)\). A node \(u\) at depth \(d(u)\) is expandable if and only if
\[
d(u) < D_{\max}
\quad\wedge\quad
p(u)\ge \rho_{\mathrm{stop}}
\quad\wedge\quad
\Bigl(d(u)<D_0 \;\;\vee\;\; p(u)\ge \rho_{\mathrm{deep}}\Bigr).
\]
We assume \(1\le D_0 < D_{\max}\) and \(0<\rho_{\mathrm{stop}}<\rho_{\mathrm{deep}}<1\).
The first condition enforces a hard depth limit; the second implements \emph{early stopping} by terminating branches whose joint draft probability is too small; and the third allows \emph{deep expansion} beyond \(D_0\) only along sufficiently likely paths. In practice, \(\rho_{\mathrm{stop}}\) and \(\rho_{\mathrm{deep}}\) are tuned on a held-out set (Section~\ref{experiments}).

\paragraph{Adaptive pruning under a node budget.}
To reduce wasted verification on unlikely branches, we further prune any leaf \(u\) whose cumulative probability falls below a global threshold \(\tau\in(0,1)\):
\[
p(u) \;<\; \tau \quad \Longrightarrow \quad \text{prune } u.
\]
This rule focuses the target-model verification budget on paths that are jointly plausible under the draft model. Additionally, we enforce a strict node budget \(N_{\max}\) during construction; when \(|\mathcal{N}|=N_{\max}\), expansion stops and remaining frontier nodes are treated as leaves.

\paragraph{Historical adjustment.}
Finally, DynaTree adapts drafting thresholds online using recent verification outcomes. Let \(a_r\in[0,1]\) denote the per-iteration acceptance statistic at iteration \(r\) (i.e., the fraction of drafted tokens committed in that iteration), and let \(\bar{a}_t\) be the sliding-window mean over the last \(W\) iterations:
\[
\bar{a}_t \;=\; \frac{1}{W}\sum_{i=0}^{W-1} a_{t-i}.
\]
Here \(W\) is a fixed window size.
When \(\bar{a}_t\) is high, we make drafting more aggressive (e.g., increasing \(D_0\) or lowering \(\tau_h\)); when \(\bar{a}_t\) is low, we become more conservative to avoid verification waste. We defer the exact update schedule to Appendix~\ref{app:algo}.

% (Moved long pseudocode to the appendix to save space in the main paper.)
We provide full pseudocode for one DynaTree iteration in Appendix~\ref{app:algo}.

\subsection{Tree Attention for Parallel Verification}
To verify all drafted tokens in one target-model forward pass, we \emph{flatten} the tree in breadth-first order (BFS), producing a sequence \(z_{1:n}\) where each token corresponds to one node and all ancestors appear earlier than descendants. We then construct a boolean attention mask \(\mathbf{A} \in \{0,1\}^{n \times (t+n)}\) such that each drafted token attends to: (i) all prefix tokens \(x_{1:t}\), and (ii) only its ancestors (including itself) in the flattened tree:
\[
\mathbf{A}_{i,j} =
\begin{cases}
1, & 1 \le j \le t,\\
1, & j=t+\mathrm{pos}(v) \text{ for some ancestor } v \in \mathrm{Anc}(u_i)\cup\{u_i\},\\
0, & \text{otherwise.}
\end{cases}
\]
This mask ensures the conditional distribution computed at each node matches the distribution of sequential decoding along its unique root-to-node path, while enabling parallel verification across different branches~\cite{specinfer,opt_tree}.

\subsection{Greedy Path Selection and Cache Update}
\paragraph{Verification signals.}
Let \(\hat{y}_{t+1} = \arg\max p_T(\cdot \mid x_{1:t})\) be the target model's greedy next token from the prefix (available from the prefix logits). For each tree node \(u\) with flattened position \(i\), the target forward pass outputs logits \(\mathbf{s}_i\), whose argmax \(\hat{y}(u)=\arg\max \mathbf{s}_i\) corresponds to the greedy \emph{next-token} prediction after consuming the path to \(u\).

\paragraph{Longest valid path.}
DynaTree commits the longest path \(u_0 \rightarrow u_1 \rightarrow \cdots \rightarrow u_m\) such that the drafted token at each node matches the target greedy prediction from its parent context:
\[
z_{u_0}=\hat{y}_{t+1},\quad
z_{u_{k}}=\hat{y}(u_{k-1}) \;\; \text{for } k=1,\dots,m.
\]
If no drafted token matches the first greedy prediction, we fall back to committing \(\hat{y}_{t+1}\) (one token progress). After committing the matched draft tokens, we append one \emph{bonus} token \(\hat{y}(u_m)\) from the target model, mirroring the greedy speculative decoding convention and ensuring steady progress.

\paragraph{KV-cache management.}
Tree verification may populate key-value states for branches that are ultimately not committed. To maintain consistency with sequential decoding, we must restore the cache to the state corresponding to the committed prefix. Concretely, after identifying the committed path, we: (i)~discard all cached key-value pairs beyond the original prefix length \(t\); and (ii)~perform a forward pass of the committed tokens through the target model to populate the cache correctly for the next iteration. This ensures that subsequent iterations start from an identical cache state as sequential greedy decoding would produce.

\subsection{Correctness for Greedy Decoding}
We sketch the correctness argument for greedy decoding (the setting used throughout our experiments). The tree attention mask guarantees that for any node \(u\), the target logits at \(u\) are computed from exactly the same conditioning context as in sequential decoding along the root-to-\(u\) path. DynaTree commits a drafted token \emph{only if} it equals the target greedy argmax under that context. Therefore, every committed token matches the token that greedy decoding with \(M_T\) would produce at that position. The cache rollback-and-rebuild step ensures the subsequent iteration starts from an identical KV state. Consequently, DynaTree generates exactly the same token sequence as greedy decoding with the target model, while reducing the number of expensive target-model forward passes by verifying many candidate tokens in parallel.

\subsection{Complexity Discussion}
Let \(n=|\mathcal{N}|\le N_{\max}\) be the number of drafted nodes. Drafting requires \(O(n)\) one-token forward passes of the draft model (with cache reuse across expansions). Verification requires a single target-model forward pass over \(n\) tokens with a structured attention mask. Dynamic pruning reduces \(n\) in uncertain regions by discarding low-probability branches, improving the trade-off between draft overhead and verification parallelism.


\section{Experiments}
\label{experiments}

\subsection{Experimental Setup}
\paragraph{Models.}
We evaluate DynaTree using models from the Pythia family~\cite{pythia}. Our target model \(M_T\) is Pythia-2.8B (2.8 billion parameters), and our draft model \(M_D\) is Pythia-70M (70 million parameters). All experiments use deterministic greedy decoding, ensuring that the output sequence is uniquely determined by the model and prefix.

\paragraph{Hardware and software.}
All experiments are conducted on a single NVIDIA GPU with sufficient memory to accommodate both models simultaneously. We implement DynaTree in PyTorch~\cite{pytorch} using the HuggingFace Transformers library~\cite{transformers} for model loading and inference, leveraging dynamic key-value cache structures to minimize memory overhead during tree verification.

\paragraph{Workloads and data preprocessing.}
Our primary benchmarks use WikiText-2~\citep{merity2016pointer} and PG-19~\citep{rae2019compressive}. For each sampled prompt, we generate \(T\) new tokens using greedy decoding, where \(T=1500\) for the main results (Section~\ref{main-results}). We truncate prompts to a maximum length \(L_{\max}\) to control prefill cost, using \(L_{\max}=800\) for WikiText-2 and \(L_{\max}=1000\) for PG-19. We evaluate \(N=10\) prompts and discard the first \(W=2\) runs as warmup; we report the mean and standard deviation over the remaining runs. To ensure fair comparison, we synchronize GPU execution and reset cached states between methods. Full experimental configurations are summarized in Appendix~\ref{app:exp-config}.

\subsection{Evaluation Metrics}
We report \textbf{throughput} (tokens per second) as the primary metric, computed as \(T\) divided by the wall-clock decoding time (excluding warmup). We additionally report \textbf{speedup} relative to autoregressive decoding, i.e., \(\mathrm{speedup}=\mathrm{TPS}/\mathrm{TPS}_{\mathrm{AR}}\). To characterize verification efficiency, we measure the \textbf{acceptance rate} \(a\), defined as the fraction of drafted tokens that match the target model's greedy predictions under the corresponding conditioning contexts, and the average \textbf{tokens per iteration} \(\bar{L}\), i.e., the number of committed tokens per verification round. We also report \textbf{average path length} \(\bar{\ell}\) (the mean depth of the greedy-consistent committed path before the bonus token) to reflect how far each verification step progresses. For latency, we include \textbf{time-to-first-token} (TTFT) and \textbf{time-per-output-token} (TPOT), averaged over prompts.

\subsection{Baselines}
We compare the proposed confidence-aware adaptive decoding against the following baselines and variants under identical greedy decoding settings:
\begin{itemize}
  \item \textbf{Autoregressive (AR):} Standard greedy decoding with the target model, serving as the performance baseline.
  \item \textbf{Linear speculative decoding:} A linear-chain speculative decoder that drafts \(K\) tokens with the draft model and verifies them with the target model in parallel, committing the longest greedy-consistent prefix~\citep{leviathan2023fast}.
  \item \textbf{Fixed Tree:} A static tree speculative decoder with fixed depth \(D\), fixed branching factor \(B\), and node budget \(N_{\max}\), representing a non-adaptive tree baseline.
  \item \textbf{DynaTree:} Our final method, which adds three adaptive components on top of the fixed-tree backbone: Phase~1 (Dynamic Breadth), Phase~2 (Dynamic Depth), and Phase~3 (History Adaptation).
\end{itemize}

\subsection{Main Results}
\label{main-results}
Table~\ref{tab:main-results} reports end-to-end throughput on WikiText-2 and PG-19 for \(T=1500\) token generation. Across both datasets, DynaTree yields the highest throughput and improves over both autoregressive decoding and linear speculative decoding. Compared with a fixed tree, DynaTree provides an additional gain on both datasets (e.g., 218.5~t/s vs.\ 188.0~t/s on WikiText-2), indicating that adapting the draft structure to local model uncertainty improves the draft--verify trade-off beyond a static configuration.

\begin{table}[t]
\centering
\caption{\textbf{Main results (\(T=1500\)).} Throughput (t/s) and speedup relative to autoregressive decoding on WikiText-2 and PG-19. Values are mean\(\pm\)std over prompts (excluding warmup).}
\label{tab:main-results}
\small
\begin{tabular}{lcccc}
\toprule
Method & \multicolumn{2}{c}{WikiText-2} & \multicolumn{2}{c}{PG-19} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& Throughput (t/s) & Speedup & Throughput (t/s) & Speedup \\
\midrule
AR & 132.8\(\pm\)0.7 & 1.00\(\times\) & 133.7\(\pm\)0.7 & 1.00\(\times\) \\
Linear Spec (\(K=5\)) & 169.1\(\pm\)19.1 & 1.27\(\times\) & 154.1\(\pm\)25.4 & 1.15\(\times\) \\
Fixed Tree (\(D=5,B=2\)) & 188.0\(\pm\)16.5 & 1.42\(\times\) & 183.7\(\pm\)24.8 & 1.37\(\times\) \\
\textbf{DynaTree} & \textbf{218.5\(\pm\)22.2} & \textbf{1.65\(\times\)} & \textbf{192.1\(\pm\)34.2} & \textbf{1.44\(\times\)} \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:main-results} visualizes the throughput and speedup comparison. Linear speculative decoding can achieve high acceptance but is fragile to early mismatches: once a divergence occurs, the remaining drafted tokens in the chain cannot be reused. Tree-based drafting mitigates this failure mode by exploring multiple continuations in parallel and committing the longest greedy-consistent prefix. DynaTree further concentrates the verification budget on high-confidence regions while limiting wasted expansion in uncertain regions, improving throughput across both datasets.

\paragraph{Latency breakdown analysis.}
Table~\ref{tab:latency-metrics} reports TTFT and TPOT on both datasets for \(T=1500\). Across datasets, speculative decoding reduces TTFT relative to autoregressive decoding, and DynaTree achieves the lowest TPOT by committing longer prefixes per verification step.

\begin{table}[t]
\centering
\caption{\textbf{Latency metrics (\(T=1500\)).} We report TTFT (latency to first output token) and TPOT (average per-token latency) on WikiText-2 and PG-19. Values are mean\(\pm\)std over prompts (excluding warmup).}
\label{tab:latency-metrics}
\small
\begin{tabular}{lcccc}
\toprule
Method & \multicolumn{2}{c}{WikiText-2} & \multicolumn{2}{c}{PG-19} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& TTFT (ms) & TPOT (ms) & TTFT (ms) & TPOT (ms) \\
\midrule
AR & 18.9\(\pm\)6.1 & 7.52\(\pm\)0.04 & 20.0\(\pm\)4.5 & 7.46\(\pm\)0.04 \\
Linear Spec (\(K=5\)) & 14.9\(\pm\)4.0 & 5.98\(\pm\)0.65 & 10.5\(\pm\)1.9 & 6.71\(\pm\)1.42 \\
Fixed Tree (\(D=5,B=2\)) & 15.0\(\pm\)3.9 & 5.35\(\pm\)0.50 & 10.4\(\pm\)0.4 & 5.54\(\pm\)0.75 \\
\textbf{DynaTree} & 14.6\(\pm\)4.1 & \textbf{4.61\(\pm\)0.47} & 9.9\(\pm\)0.6 & \textbf{5.38\(\pm\)1.03} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Verification efficiency.}
To contextualize throughput gains, Table~\ref{tab:verification-efficiency} reports auxiliary verification statistics on both datasets: tokens committed per verification round, average committed path length, and the number of verification rounds required to generate \(T=1500\) tokens. Tree-based methods commit longer prefixes per step and therefore require fewer rounds than autoregressive decoding; DynaTree further reduces rounds by allocating more verification budget to high-confidence regions.

\begin{table}[t]
\centering
\caption{\textbf{Verification efficiency metrics (\(T=1500\)).} Tokens per round (\(\bar{L}\)) and average path length (\(\bar{\ell}\)) reflect per-iteration progress; \#Rounds is the total number of verification iterations needed to generate \(T\) tokens. Values are mean across prompts (excluding warmup).}
\label{tab:verification-efficiency}
\setlength{\tabcolsep}{3pt}
\scriptsize
\begin{tabular}{lcccccc}
\toprule
Method & \multicolumn{3}{c}{WikiText-2} & \multicolumn{3}{c}{PG-19} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
& \(\bar{L}\) & \(\bar{\ell}\) & \#Rounds & \(\bar{L}\) & \(\bar{\ell}\) & \#Rounds \\
\midrule
AR & 1.00 & -- & 1500 & 1.00 & -- & 1500 \\
Linear Spec (\(K=5\)) & 4.84 & 4.88 & 310 & 4.56 & 4.63 & 329 \\
Fixed Tree (\(D=5,B=2\)) & 5.75 & 5.78 & 261 & 5.75 & 5.80 & 261 \\
\textbf{DynaTree} & \textbf{7.08} & \textbf{7.15} & \textbf{212} & \textbf{6.17} & \textbf{6.45} & \textbf{243} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{../figures/main_results_bars.pdf}
  \caption{\textbf{Throughput and speedup across datasets (\(T=1500\)).} Each method is shown with two bars (WikiText-2 vs.\ PG-19). DynaTree consistently improves over autoregressive and linear speculative decoding on both datasets.}
  \label{fig:main-results}
\end{figure}

\subsection{Ablation Study}
\label{sec:ablation}
We provide a progressive ablation on WikiText-2 under the same \(T=1500\) setting as the main benchmark, isolating the contribution of each adaptive component relative to a fixed-tree backbone. In this regime, adapting breadth alone can introduce control overhead without reliably increasing the committed prefix length, whereas dynamic depth control more directly increases per-iteration progress and yields the dominant throughput gain. Table~\ref{tab:ablation} therefore reports the combined effect of \emph{Dynamic Breadth \& Depth}, followed by the additional improvement from \emph{History Adaptation}, which stabilizes the draft configuration by reacting to recent verification outcomes.

\begin{table}[t]
\centering
\caption{\textbf{Ablation-style progressive comparison on WikiText-2 (\(T=1500\)).} We compare a fixed tree (\(D_0=5,B=2\)) with variants that add Dynamic Breadth \& Depth and History Adaptation. Speedup is computed relative to the autoregressive baseline.}
\label{tab:ablation}
\small
\begin{tabular}{lccc}
    \toprule
Variant & Throughput (t/s) & Speedup & \(\Delta\) vs Fixed \\
    \midrule
Fixed Tree (\(D_0=5,B=2\)) & 188.0\(\pm\)16.5 & 1.42\(\times\) & 0.0\% \\
+ Dynamic Breadth \& Depth & 213.2\(\pm\)26.1 & 1.61\(\times\) & +13.4\% \\
\textbf{+ History Adaptation} & \textbf{218.5\(\pm\)22.2} & \textbf{1.65\(\times\)} & \textbf{+16.2\%} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Sequence Length Scaling}
\label{sec:length-scaling}
We evaluate how throughput varies with generation length \(T\) on WikiText-2, comparing autoregressive decoding, linear speculative decoding, a fixed tree configuration, and DynaTree. Figure~\ref{fig:length-scaling} summarizes the throughput trends across \(T\).

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\linewidth]{../figures/length_scaling.pdf}
  \caption{\textbf{Throughput across generation lengths on WikiText-2.} Throughput (tokens/s) as a function of \(T\) for AR, linear speculative decoding, a fixed tree baseline, and DynaTree.}
  \label{fig:length-scaling}
\end{figure}

\paragraph{Additional analyses.}
We place supplementary parameter sensitivity analyses in Appendix~\ref{app:additional-analyses}.


\section{Conclusion}
We presented DynaTree, a greedy-consistent tree-based speculative decoding method that drafts a candidate token tree and verifies all nodes in a single target-model pass via a structured attention mask. The key contribution is an adaptive drafting strategy that allocates verification budget to likely regions of the search space through dynamic breadth and depth control, and further stabilizes the configuration through lightweight history-based adjustment. On WikiText-2 and PG-19 with \(T=1500\), DynaTree improves end-to-end throughput over autoregressive decoding, linear speculative decoding, and a fixed-tree baseline, while reducing per-token latency by committing longer prefixes per verification step. Future work includes extending the evaluation to broader serving conditions (e.g., varying prompt lengths, batch sizes, and longer contexts), and reducing system overhead through kernel-level optimizations and hardware-aware tree construction.

\bibliographystyle{unsrtnat}
\bibliography{references}

\appendix
\section{Experimental Configuration Details}
\label{app:exp-config}
\paragraph{Common settings.}
Unless stated otherwise, we use WikiText-2 prompts, truncate the prompt length to \(L_{\max}=800\), and generate \(T\) new tokens with greedy decoding. We evaluate \(N=10\) prompts and discard the first \(W=2\) runs as warmup.
\paragraph{PG-19 setting.}
For PG-19, we use the same model pair and greedy decoding, with a maximum prompt length \(L_{\max}=1000\).
\paragraph{Fixed-tree baseline.}
For the static-tree baseline, we use depth \(D=5\), branching factor \(B=2\), and a node budget \(N_{\max}=256\).
\paragraph{DynaTree.}
Unless stated otherwise, the adaptive configuration uses base depth \(D_0=5\), maximum depth \(D_{\max}=8\), branch bounds \((B_{\min},B_{\max})=(1,3)\), and confidence thresholds \((\tau_h,\tau_\ell)=(0.9,0.4)\).

\section{Additional Experimental Analyses}
\label{app:additional-analyses}
\paragraph{Speedup computation.}
Some auxiliary result files include a placeholder \texttt{speedup} field; throughout this appendix we report speedup as the ratio between the method throughput and the corresponding autoregressive throughput under the same setting.

\subsection{Parameter Sensitivity}
We study the sensitivity of DynaTree to confidence thresholds \((\tau_h,\tau_\ell)\) and branch bounds \((B_{\min},B_{\max})\) on WikiText-2 at \(T=500\). Table~\ref{tab:appendix-sensitivity} reports representative configurations with the highest throughput in \texttt{results/adaptive/sensitivity/paper\_benchmark\_sensitivity.json}.

\begin{table}[t]
\centering
\caption{\textbf{Parameter sensitivity on WikiText-2 (\(T=500\)).} Results are computed from the experiment logs released with this work. Speedup is relative to AR under the same setting.}
\label{tab:appendix-sensitivity}
\small
\begin{tabular}{lccc}
\toprule
Setting & Throughput (t/s) & Speedup & Accept. \\
\midrule
AR & 99.2\(\pm\)22.3 & 1.00\(\times\) & 0.0\% \\
\((\tau_h,\tau_\ell)=(0.9,0.4),\ (B_{\min},B_{\max})=(1,4)\) & 180.5\(\pm\)29.6 & 1.82\(\times\) & 81.1\% \\
\((\tau_h,\tau_\ell)=(0.8,0.3),\ (B_{\min},B_{\max})=(1,3)\) & 179.0\(\pm\)27.3 & 1.80\(\times\) & 79.7\% \\
\((\tau_h,\tau_\ell)=(0.8,0.3),\ (B_{\min},B_{\max})=(1,2)\) & 178.3\(\pm\)29.2 & 1.80\(\times\) & 78.6\% \\
\((\tau_h,\tau_\ell)=(0.8,0.3),\ (B_{\min},B_{\max})=(1,4)\) & 174.8\(\pm\)31.3 & 1.76\(\times\) & 77.3\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Memory Footprint Analysis}
\label{app:memory}
An important practical consideration for speculative decoding methods is their memory overhead. Table~\ref{tab:memory-footprint} reports peak GPU memory consumption across methods on PG-19 and WikiText-2 datasets during 500-token generation with Pythia models. All measurements are taken using PyTorch's memory profiler during steady-state generation (excluding initial model loading).

Key observations: (i)~DynaTree incurs minimal memory overhead (+0.45\% on average) compared to the autoregressive baseline, adding only~26~MB to accommodate the draft model's KV cache and intermediate tree structures; (ii)~Linear speculative methods show similarly negligible overhead ($<$1\%), as they maintain only a small fixed-size buffer of candidate tokens. Across all methods, memory overhead remains well within 1\% of the baseline, confirming that speculative decoding's primary cost is computational rather than memory-related. This makes DynaTree suitable for memory-constrained deployment scenarios where the target and draft models can already fit in GPU memory.

\begin{table}[t]
\centering
\caption{\textbf{Peak GPU memory consumption comparison.} We measure peak memory usage during 500-token generation on PG-19 (long-form fiction) and WikiText-2 (structured articles) with Pythia-2.8B and Pythia-70M models. All speculative methods incur minimal memory overhead ($<$1\% relative to baseline), with DynaTree adding only 0.45\% on average to accommodate tree structures and draft KV cache. Values show mean peak memory across 10 runs; relative change is computed against the autoregressive baseline.}
\label{tab:memory-footprint}
\small
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Peak Memory (MB)} & \multirow{2}{*}{Average (MB)} & \multirow{2}{*}{Rel. Change} \\
\cmidrule(lr){2-3}
& PG-19 & WikiText-2 & & \\
\midrule
AR (baseline) & 5855.1 & 5798.6 & 5826.9 & 0.00\% \\
Linear K=6 & 5817.3 & 5786.3 & 5801.8 & $-$0.43\% \\
Linear K=7 & 5817.7 & 5786.2 & 5801.9 & $-$0.43\% \\
\textbf{DynaTree (D=6, B=2)} & \textbf{5883.7} & \textbf{5822.9} & \textbf{5853.3} & \textbf{+0.45\%} \\
DynaTree (D=7, B=2) & 5883.7 & 5822.9 & 5853.3 & +0.45\% \\
\bottomrule
\end{tabular}
\end{table}

\section{DynaTree Iteration Pseudocode}
\label{app:algo}
\begin{algorithm}[t]
\caption{\textbf{DynaTree: one iteration (greedy-consistent).}}
\label{alg:dynatree}
\small
\begin{algorithmic}[1]
\Require Prefix tokens \(x_{1:t}\); target KV cache \(\mathcal{K}_T\); prefix next-token logits \(\mathbf{s}_{\text{last}}\);
branch bounds \(B_{\min}\le B_{\mathrm{mid}}\le B_{\max}\); confidence thresholds \(0<\tau_\ell<\tau_h<1\); base depth \(D_0\); max depth \(D_{\max}\); depth thresholds \(0<\rho_{\mathrm{stop}}<\rho_{\mathrm{deep}}<1\); pruning threshold \(\tau\); node budget \(N_{\max}\); history window \(W\).
\Ensure Committed tokens \(y_{t+1:t+L}\) and updated \(\mathcal{K}_T\).

\State \(\ell \gets \Call{SeqLen}{\mathcal{K}_T}\) \Comment{record prefix cache length}
\State \(\mathcal{T} \gets \Call{DraftTree}{x_{1:t}, B_{\min}, B_{\mathrm{mid}}, B_{\max}, \tau_\ell, \tau_h, D_0, D_{\max}, \rho_{\mathrm{stop}}, \rho_{\mathrm{deep}}, \tau, N_{\max}}\)
\State \(\mathbf{z}_{1:n} \gets \Call{BFSFlatten}{\mathcal{T}}\); \(\mathbf{A} \gets \Call{TreeMask}{\mathcal{T}, \ell}\) \Comment{prefix + ancestors only}
\State \(\mathbf{s}_{1:n} \gets M_T(\mathbf{z}_{1:n}; \mathbf{A}, \mathcal{K}_T)\); \(\hat{\mathbf{y}} \gets \arg\max \mathbf{s}_{1:n}\)
\State \(y_{t+1:t+L} \gets \Call{SelectCommit}{\mathcal{T}, \hat{\mathbf{y}}, \mathbf{s}_{\text{last}}}\)
\State \(\mathcal{K}_T \gets \Call{Crop}{\mathcal{K}_T,\ell}\); \(\mathcal{K}_T \gets M_T(y_{t+1:t+L}; \mathcal{K}_T)\) \Comment{rollback + rebuild}
\State \Call{UpdateHistory}{$y_{t+1:t+L}, \mathcal{T}, W$}; \Call{Adjust}{$\tau_\ell,\tau_h,D_0$} \Comment{historical adjustment}
\State \Return \(y_{t+1:t+L}\)

\Statex
\Function{\textproc{DraftTree}}{$x_{1:t}, B_{\min}, B_{\mathrm{mid}}, B_{\max}, \tau_\ell, \tau_h, D_0, D_{\max}, \rho_{\mathrm{stop}}, \rho_{\mathrm{deep}}, \tau, N_{\max}$}
  \Comment{Draft a candidate token tree with adaptive branching, dynamic depth control, and pruning under a node budget.}
  \State Run \(M_D\) on \(x_{1:t}\); let \(u_0\) be the \(\top 1\) token; initialize \(\mathcal{T}\) with root \(u_0\)
  \State \(\mathcal{A} \gets \{u_0\}\) \Comment{active frontier}
  \While{$|\mathcal{A}|>0$ \textbf{and} $|\mathcal{T}|<N_{\max}$}
    \State Pop an element \(u\) from \(\mathcal{A}\)
    \If{$\exp(\bar{\ell}_u) < \tau$} \State \textbf{continue} \EndIf \Comment{probability-threshold pruning}
    \If{$d(u)\ge D_{\max}$} \State \textbf{continue} \EndIf
    \If{$\exp(\bar{\ell}_u) < \rho_{\mathrm{stop}}$} \State \textbf{continue} \EndIf \Comment{early stopping}
    \If{$d(u)\ge D_0$ \textbf{and} $\exp(\bar{\ell}_u) < \rho_{\mathrm{deep}}$} \State \textbf{continue} \EndIf \Comment{depth gating}
    \State Do one cached step of \(M_D\) from \(u\); compute confidence \(c(u)=\max \mathrm{softmax}(\mathbf{h}(u))\)
    \State Set \(B(u)\gets B_{\min}\) if \(c(u)\ge\tau_h\), \(B_{\max}\) if \(c(u)<\tau_\ell\), else \(B_{\mathrm{mid}}\)
    \State Take \(\top B(u)\) next-token candidates; add each child \(v\) to \(\mathcal{T}\) and push \(v\) into \(\mathcal{A}\) until \(N_{\max}\)
  \EndWhile
  \State \Return \(\mathcal{T}\)
\EndFunction

\Statex
\Function{\textproc{SelectCommit}}{$\mathcal{T}, \hat{\mathbf{y}}, \mathbf{s}_{\text{last}}$}
  \Comment{Select the longest greedy-consistent path (plus one bonus token).}
  \State \(first \gets \arg\max \mathbf{s}_{\text{last}}\)
  \State Find the longest path \(P\) where root token \(=\) \(first\), and for each edge \((u\!\rightarrow\!v)\), token\((v)=\hat{\mathbf{y}}[\text{pos}(u)]\)
  \If{$P=\emptyset$}
    \State \Return \([first]\)
  \Else
    \State \(y \gets\) tokens on \(P\)
    \State Append one bonus token \(\hat{\mathbf{y}}[\text{pos}(\text{last}(P))]\)
    \State \Return \(y\)
  \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\end{document}