\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025
\PassOptionsToPackage{numbers,sort&compress}{natbib}

% The authors should use one of these tracks.
% Before accepting by the NeurIPS conference, select one of the options below.
% 0. "default" for submission
% \usepackage{neurips_2025}
% Using "final" option to show author names (not for actual submission)
\usepackage[preprint]{neurips_2025}
% the "default" option is equal to the "main" option, which is used for the Main Track with double-blind reviewing.
% 1. "main" option is used for the Main Track
%  \usepackage[main]{neurips_2025}
% 2. "position" option is used for the Position Paper Track
%  \usepackage[position]{neurips_2025}
% 3. "dandb" option is used for the Datasets & Benchmarks Track
 % \usepackage[dandb]{neurips_2025}
% 4. "creativeai" option is used for the Creative AI Track
%  \usepackage[creativeai]{neurips_2025}
% 5. "sglblindworkshop" option is used for the Workshop with single-blind reviewing
 % \usepackage[sglblindworkshop]{neurips_2025}
% 6. "dblblindworkshop" option is used for the Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop]{neurips_2025}

% After being accepted, the authors should add "final" behind the track to compile a camera-ready version.
% 1. Main Track
 % \usepackage[main, final]{neurips_2025}
% 2. Position Paper Track
%  \usepackage[position, final]{neurips_2025}
% 3. Datasets & Benchmarks Track
 % \usepackage[dandb, final]{neurips_2025}
% 4. Creative AI Track
%  \usepackage[creativeai, final]{neurips_2025}
% 5. Workshop with single-blind reviewing
%  \usepackage[sglblindworkshop, final]{neurips_2025}
% 6. Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop, final]{neurips_2025}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote.
% For workshops (5., 6.), the authors should add the name of the workshop, "\workshoptitle" command is used to set the workshop title.
% \workshoptitle{WORKSHOP TITLE}

% "preprint" option is used for arXiv or other preprint submissions
 % \usepackage[preprint]{neurips_2025}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{multirow}       % multirow cells in tables
\usepackage{graphicx}       % figures
\usepackage{amsmath}        % math
\usepackage{amssymb}        % math symbols
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{algorithm}     % float wrapper
\usepackage{algpseudocode} % algorithmicx style (supports \Call)

% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote. 
\title{DynaTree: Confidence-Aware Adaptive Tree Speculative Decoding for Efficient LLM Inference}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Nuoyan Chen$^*$ \quad
  Jiamin Liu$^*$ \quad
  Zhaocheng Li\thanks{Equal contribution.} \\
  School of Computer Science\\
  Shanghai Jiao Tong University\\
  \texttt{\{cny123222, logic-1.0, lzc050419\}@sjtu.edu.cn}
}


\begin{document}


\maketitle


\begin{abstract}
Autoregressive decoding in large language models (LLMs) is fundamentally sequential and therefore underutilizes modern accelerator parallelism during token generation. Speculative decoding mitigates this bottleneck by letting a lightweight draft model propose multiple tokens that are verified in parallel by the target model; however, linear variants explore only a single draft chain per step and can waste substantial computation when early tokens are rejected, while existing tree-based approaches employ \emph{fixed} structures that cannot adapt to varying draft model confidence. We propose \textbf{DynaTree}, a tree-based speculative decoding framework with \emph{confidence-aware adaptive branching} that dynamically adjusts tree structure based on draft model uncertainty through adaptive per-node branching, dynamic depth control, and historical acceptance tuning, combined with probability-threshold pruning under an explicit node budget. Experiments on Pythia models demonstrate that DynaTree achieves 210.8~tokens/sec on WikiText-2 (1.61$\times$ speedup, 94.7\% acceptance rate), outperforming fixed tree structures by 16.3\% and consistently surpassing linear speculative decoding baselines across diverse datasets.
\end{abstract}


\section{Introduction}

Large language models (LLMs) are typically deployed with autoregressive decoding, where each output token is generated after conditioning on all previously generated tokens. While transformer inference can exploit parallelism during the prefill stage, the decode stage remains inherently sequential and requires a full forward pass per token, leading to poor hardware utilization and high latency~\cite{flashattention,vllm}.

Speculative decoding alleviates this bottleneck by separating \emph{proposal} and \emph{verification}~\cite{leviathan2023fast}. A small draft model proposes several candidate tokens, and the target model verifies them in parallel; when the proposal matches the target distribution, multiple tokens can be committed per iteration. Importantly, with rejection sampling, speculative decoding preserves the exact output distribution of the target model~\cite{decoding_speculative}.

In practice, most speculative decoding systems employ \emph{linear} drafting: the draft model proposes a single chain of $K$ tokens. This design is brittle under draft--target mismatch. A rejection at an early position forces all subsequent drafted tokens to be discarded, wasting both draft computation and target-model verification work. This single-path exploration fundamentally limits achievable speedups, as the system cannot recover from early errors without restarting the drafting process~\cite{decoding_speculative}.

Tree-based speculation offers a natural remedy. When multiple plausible next tokens compete, exploring several continuations in parallel increases the chance that at least one path aligns with the target model, thereby improving the expected number of accepted tokens per verification step. The draft expands multiple candidates via top-$k$ branching, and the target verifies the resulting token tree in parallel with a structured, causality-preserving attention mask. This multi-path exploration addresses the fundamental brittleness of linear drafting.

While tree-based drafting addresses the single-path limitation of linear methods, existing approaches typically employ \emph{fixed} tree configurations with predetermined depth and branching factor~\cite{specinfer,medusa}. This rigid structure cannot adapt to the draft model's varying prediction confidence, creating an \emph{efficiency gap}: high-confidence predictions waste compute exploring unnecessary branches, while uncertain predictions suffer from insufficient exploration. Recent adaptive methods~\cite{cm_asd,adaeagle,cas_spec} adjust draft length or employ learned predictors, yet most focus on linear speculation rather than fundamentally restructuring the tree itself. We hypothesize that \emph{confidence-aware} tree construction---dynamically adjusting branching per node based on draft uncertainty---can bridge this gap while maintaining robust exploration.

We present \textbf{DynaTree}, which addresses the efficiency gap through confidence-aware adaptive branching that dynamically adjusts tree structure based on draft model uncertainty. Our three-phase mechanism adapts per-node branching (1--3 branches), implements dynamic depth control via early stopping and deep expansion, and tunes parameters based on historical acceptance rates. Combined with probability-threshold pruning under an explicit node budget, DynaTree verifies candidate paths in a single forward pass using tree attention. Empirically, DynaTree achieves 210.8~tokens/sec on WikiText-2 (1.61$\times$ speedup, 94.7\% acceptance rate), outperforming fixed tree structures by 16.3\%.

In summary, our contributions are:
\begin{itemize}
  \item We propose a tree-based speculative decoding framework with \textbf{confidence-aware adaptive branching} that dynamically adjusts tree structure based on draft model uncertainty, directly addressing the efficiency gap in static configurations.
  
  \item We introduce a three-phase adaptive mechanism combining confidence-based branching, dynamic depth control, and historical parameter tuning. Our ablation study reveals that dynamic depth contributes most to performance gains.
  
  \item Experiments demonstrate that DynaTree achieves 1.61$\times$ speedup with 94.7\% acceptance rate, outperforming fixed tree baselines by 16.3\% with robust cross-dataset performance.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../figures/decode-v1.png}
  \caption{\textbf{Comparison of three decoding paradigms.} \textbf{Left:} Autoregressive (AR) decoding generates tokens sequentially, requiring one LLM forward pass per token. \textbf{Middle:} Linear speculative decoding drafts a single token chain; early rejection wastes all subsequent drafted tokens. \textbf{Right:} Tree-based speculative decoding (DynaTree) explores multiple paths in parallel, enabling recovery from draft errors and committing longer sequences per iteration. The multi-path exploration fundamentally addresses the brittleness of linear drafting while maintaining output correctness through structured tree attention verification.}
  \label{fig:decode-comparison}
\end{figure}

\section{Related Work}

\subsection{Speculative Decoding}

Speculative decoding accelerates autoregressive generation by decoupling \emph{proposal} and \emph{verification}: a lightweight draft model proposes multiple tokens, and the target model verifies these candidates in parallel while preserving the exact output distribution~\cite{leviathan2023fast,decoding_speculative,draft_tradeoff}. The memory-bound nature of LLM inference (approximately 1 FLOP/byte operational intensity) makes parallel verification particularly valuable~\cite{flashattention,vllm}. Recent work highlights robustness challenges across heterogeneous workloads and long-context inputs~\cite{spin,owl,judge_decoding}, but the dominant \emph{linear} drafting paradigm suffers from a fundamental inefficiency: when early tokens are rejected, all downstream draft tokens are discarded, wasting computation.

\subsection{Tree-Based Speculative Decoding}

To overcome single-path inefficiency, tree-based methods draft multiple candidate continuations and verify them in one target-model forward pass using structured attention masks~\cite{specinfer,opt_tree,medusa,traversal_verification}. SpecInfer~\cite{specinfer} pioneered token tree verification with expansion-based and merge-based construction mechanisms. OPT-Tree~\cite{opt_tree} algorithmically searches for tree structures that maximize expected acceptance length, while Medusa~\cite{medusa} augments models with multiple decoding heads to eliminate separate draft models. However, these methods predominantly use \emph{fixed} tree configurations with static depth $D$ and branching factor $B$, creating an efficiency gap when draft confidence varies~\cite{specinfer,medusa}.

Recent adaptive approaches address this rigidity through dynamic parameter adjustment. CM-ASD~\cite{cm_asd} modulates drafting length and verification thresholds based on entropy, logit margin, and softmax margin, achieving 4--5$\times$ speedups. AdaEAGLE~\cite{adaeagle} employs a lightweight MLP to predict optimal draft length per iteration. CAS-Spec~\cite{cas_spec} introduces dynamic tree cascades with acceptance rate heuristics, improving throughput by 47\%. While promising, these methods focus on linear speculation or require learned predictors. In contrast, DynaTree directly restructures trees via confidence-aware branching, dynamic depth control, and historical adjustment—achieving 16.3\% gains over fixed configurations while remaining training-free.

\subsection{Dynamic Pruning Strategies}

Exponential candidate growth necessitates pruning to balance exploration and verification cost. ProPD~\cite{propd} employs top-$k$ early prediction heads and weighted regression to remove low-utility branches, reducing computation by 2$\times$. CAST~\cite{cast} formalizes cost-aware breadth and depth pruning, explicitly modeling verification overhead per layer. DySpec~\cite{dyspec} uses greedy confidence-guided expansion, while RASD~\cite{rasd} prunes retrieval candidates outside the draft model's top-$k$ predictions. AdaSD~\cite{adasd} introduces hyperparameter-free thresholds based on entropy and Jensen-Shannon distance, achieving 49\% speedups. These methods differ in adaptation mechanisms—from offline heuristics to online predictors. DynaTree adopts probability-threshold pruning with explicit node budgets, prioritizing simplicity and training-free integration while maintaining strong empirical performance.


\section{Methodology}
\label{method}

\subsection{Problem Setup and Notation}
Let \(M_T\) denote a target autoregressive language model and \(M_D\) a smaller draft model. Given a prefix (prompt) \(x_{1:t}\), greedy decoding with \(M_T\) produces tokens \(y_{t+1}, y_{t+2}, \dots\) where
\[
y_{i} \;=\; \arg\max_{v \in \mathcal{V}} p_T(v \mid x_{1:i-1}).
\]
Speculative decoding accelerates generation by proposing candidate tokens with \(M_D\) and verifying them with \(M_T\), while preserving the greedy output when the verification rule only commits tokens that match the target greedy predictions.

\subsection{Overview of DynaTree}
DynaTree generalizes linear speculative decoding from a single draft chain to a \emph{draft token tree}. In each iteration, it first performs \emph{adaptive tree drafting} with \(M_D\), where the effective tree breadth and depth are determined on-the-fly from the draft distribution and the cumulative path probability. Concretely, for a draft node \(u\) with draft logits \(\mathbf{h}(u)\), we define the draft confidence \(c(u)=\max_{v\in\mathcal{V}}\mathrm{softmax}(\mathbf{h}(u))_v\) and use it to select a per-node branching factor \(B(u)\in\{B_{\min},B_{\mathrm{mid}},B_{\max}\}\). We further control expansion depth via the cumulative probability \(p(u)=\exp(\bar{\ell}_u)\), combining early stopping for low-probability branches with deeper expansion along high-probability paths. The resulting candidate tree is then \textbf{verified in parallel} in a single forward pass of \(M_T\) using tree attention, followed by \textbf{greedy path selection} and \textbf{KV-cache update} for committed tokens. Finally, DynaTree maintains a short window of recent acceptance statistics to adjust drafting thresholds for subsequent iterations.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../figures/dynatree-v9.5.png}
  \caption{\textbf{One iteration of DynaTree decoding.} The process consists of six main stages: (1)~\emph{Adaptive Tree Drafting:} The draft model expands a candidate tree with three adaptive mechanisms: confidence-aware branching adjusts the number of child nodes (1--3) per expansion based on draft model confidence; dynamic depth control implements early stopping for low cumulative probability branches and deep expansion for high-probability paths; adaptive pruning removes branches below probability threshold $\tau$ or exceeding node budget $N_{\max}$. (2)~\emph{Flattening \& Masking:} The pruned tree is serialized in breadth-first order, and a causal attention mask is constructed to ensure each node attends only to its ancestors. (3)~\emph{Parallel Verification:} The target model verifies all candidates in a single forward pass. (4)~\emph{Path Selection:} The longest path where drafted tokens match the target model's greedy predictions is identified. (5)~\emph{Cache Update:} The committed tokens are used to update the context and key-value cache for the next iteration. (6)~\emph{Historical Adjustment:} Acceptance rate from recent rounds feeds back to adjust confidence thresholds and base depth for the next iteration. This three-phase adaptive mechanism enables efficient multi-path exploration while maintaining correctness guarantees for greedy decoding.}
  \label{fig:arch}
\end{figure}

\subsection{Adaptive Tree Drafting}
We maintain a token tree \(\mathcal{T}=(\mathcal{N}, \mathcal{E})\) whose nodes \(u \in \mathcal{N}\) correspond to drafted tokens. Each node \(u\) is associated with: (i)~\emph{token} \(z_u \in \mathcal{V}\); (ii)~\emph{parent} \(\pi(u)\); (iii)~\emph{depth} \(d(u)\) from root; (iv)~\emph{draft log-probability} \(\ell_u = \log p_D(z_u \mid \text{prefix}(\pi(u)))\); and (v)~\emph{cumulative log-probability} \(\bar{\ell}_u = \sum_{v \in \text{path}(u)} \ell_v\), where \(\text{path}(u)\) denotes all nodes from root to \(u\) along the tree.

\paragraph{Tree expansion.}
Starting from the current prefix \(x_{1:t}\), we construct \(\mathcal{T}\) in a breadth-first manner under a strict node budget \(N_{\max}\). For any expandable node \(u\), let \(q_D(\cdot \mid u)\) denote the draft distribution conditioned on the unique root-to-\(u\) prefix, and define the local confidence
\[
c(u)\;=\;\max_{v\in\mathcal{V}} q_D(v\mid u).
\]
We select a \emph{per-node} branching factor via a confidence rule
\[
B(u)=
\begin{cases}
B_{\min}, & c(u)\ge \tau_h,\\
B_{\mathrm{mid}}, & \tau_\ell \le c(u) < \tau_h,\\
B_{\max}, & c(u)<\tau_\ell,
\end{cases}
\]
where \(0<\tau_\ell<\tau_h<1\) are confidence thresholds and \(1\le B_{\min}\le B_{\mathrm{mid}}\le B_{\max}\) are integer branch bounds.
and expand \(u\) by adding the \(B(u)\) highest-probability children under \(q_D(\cdot\mid u)\). To adapt depth, we use the cumulative path probability \(p(u)=\exp(\bar{\ell}_u)\): low-probability branches are terminated early, while high-probability paths may be expanded beyond a base depth. Concretely, a node at depth \(d(u)\) is eligible for expansion only if it satisfies the depth-gating rule (defined in the next paragraph) and \(|\mathcal{N}|<N_{\max}\). Implementation details for cache reuse during expansion are deferred to Appendix~\ref{app:algo}.

\paragraph{Dynamic depth control.}
Let \(D_0\) denote a base depth and \(D_{\max}\) a hard maximum depth. We gate expansion using two probability thresholds \(\rho_{\mathrm{stop}} < \rho_{\mathrm{deep}}\) on the cumulative path probability \(p(u)\). A node \(u\) at depth \(d(u)\) is expandable if and only if
\[
d(u) < D_{\max}
\quad\wedge\quad
p(u)\ge \rho_{\mathrm{stop}}
\quad\wedge\quad
\Bigl(d(u)<D_0 \;\;\vee\;\; p(u)\ge \rho_{\mathrm{deep}}\Bigr).
\]
We assume \(1\le D_0 < D_{\max}\) and \(0<\rho_{\mathrm{stop}}<\rho_{\mathrm{deep}}<1\).
The first condition enforces a hard depth limit; the second implements \emph{early stopping} by terminating branches whose joint draft probability is too small; and the third allows \emph{deep expansion} beyond \(D_0\) only along sufficiently likely paths. In practice, \(\rho_{\mathrm{stop}}\) and \(\rho_{\mathrm{deep}}\) are tuned on a held-out set (Section~\ref{experiments}).

\paragraph{Adaptive pruning under a node budget.}
To reduce wasted verification on unlikely branches, we further prune any leaf \(u\) whose cumulative probability falls below a global threshold \(\tau\in(0,1)\):
\[
p(u) \;<\; \tau \quad \Longrightarrow \quad \text{prune } u.
\]
This rule focuses the target-model verification budget on paths that are jointly plausible under the draft model. Additionally, we enforce a strict node budget \(N_{\max}\) during construction; when \(|\mathcal{N}|=N_{\max}\), expansion stops and remaining frontier nodes are treated as leaves.

\paragraph{Historical adjustment.}
Finally, DynaTree adapts drafting thresholds online using recent verification outcomes. Let \(a_r\in[0,1]\) denote the per-iteration acceptance statistic at iteration \(r\) (i.e., the fraction of drafted tokens committed in that iteration), and let \(\bar{a}_t\) be the sliding-window mean over the last \(W\) iterations:
\[
\bar{a}_t \;=\; \frac{1}{W}\sum_{i=0}^{W-1} a_{t-i}.
\]
Here \(W\) is a fixed window size.
When \(\bar{a}_t\) is high, we make drafting more aggressive (e.g., increasing \(D_0\) or lowering \(\tau_h\)); when \(\bar{a}_t\) is low, we become more conservative to avoid verification waste. We defer the exact update schedule to Appendix~\ref{app:algo}.

% (Moved long pseudocode to the appendix to save space in the main paper.)
We provide full pseudocode for one DynaTree iteration in Appendix~\ref{app:algo}.

\subsection{Tree Attention for Parallel Verification}
To verify all drafted tokens in one target-model forward pass, we \emph{flatten} the tree in breadth-first order (BFS), producing a sequence \(z_{1:n}\) where each token corresponds to one node and all ancestors appear earlier than descendants. We then construct a boolean attention mask \(\mathbf{A} \in \{0,1\}^{n \times (t+n)}\) such that each drafted token attends to: (i) all prefix tokens \(x_{1:t}\), and (ii) only its ancestors (including itself) in the flattened tree:
\[
\mathbf{A}_{i,j} =
\begin{cases}
1, & 1 \le j \le t,\\
1, & j=t+\mathrm{pos}(v) \text{ for some ancestor } v \in \mathrm{Anc}(u_i)\cup\{u_i\},\\
0, & \text{otherwise.}
\end{cases}
\]
This mask ensures the conditional distribution computed at each node matches the distribution of sequential decoding along its unique root-to-node path, while enabling parallel verification across different branches~\cite{specinfer,opt_tree}.

\subsection{Greedy Path Selection and Cache Update}
\paragraph{Verification signals.}
Let \(\hat{y}_{t+1} = \arg\max p_T(\cdot \mid x_{1:t})\) be the target model's greedy next token from the prefix (available from the prefix logits). For each tree node \(u\) with flattened position \(i\), the target forward pass outputs logits \(\mathbf{s}_i\), whose argmax \(\hat{y}(u)=\arg\max \mathbf{s}_i\) corresponds to the greedy \emph{next-token} prediction after consuming the path to \(u\).

\paragraph{Longest valid path.}
DynaTree commits the longest path \(u_0 \rightarrow u_1 \rightarrow \cdots \rightarrow u_m\) such that the drafted token at each node matches the target greedy prediction from its parent context:
\[
z_{u_0}=\hat{y}_{t+1},\quad
z_{u_{k}}=\hat{y}(u_{k-1}) \;\; \text{for } k=1,\dots,m.
\]
If no drafted token matches the first greedy prediction, we fall back to committing \(\hat{y}_{t+1}\) (one token progress). After committing the matched draft tokens, we append one \emph{bonus} token \(\hat{y}(u_m)\) from the target model, mirroring the greedy speculative decoding convention and ensuring steady progress.

\paragraph{KV-cache management.}
Tree verification may populate key-value states for branches that are ultimately not committed. To maintain consistency with sequential decoding, we must restore the cache to the state corresponding to the committed prefix. Concretely, after identifying the committed path, we: (i)~discard all cached key-value pairs beyond the original prefix length \(t\); and (ii)~perform a forward pass of the committed tokens through the target model to populate the cache correctly for the next iteration. This ensures that subsequent iterations start from an identical cache state as sequential greedy decoding would produce.

\subsection{Correctness for Greedy Decoding}
We sketch the correctness argument for greedy decoding (the setting used throughout our experiments). The tree attention mask guarantees that for any node \(u\), the target logits at \(u\) are computed from exactly the same conditioning context as in sequential decoding along the root-to-\(u\) path. DynaTree commits a drafted token \emph{only if} it equals the target greedy argmax under that context. Therefore, every committed token matches the token that greedy decoding with \(M_T\) would produce at that position. The cache rollback-and-rebuild step ensures the subsequent iteration starts from an identical KV state. Consequently, DynaTree generates exactly the same token sequence as greedy decoding with the target model, while reducing the number of expensive target-model forward passes by verifying many candidate tokens in parallel.

\subsection{Complexity Discussion}
Let \(n=|\mathcal{N}|\le N_{\max}\) be the number of drafted nodes. Drafting requires \(O(n)\) one-token forward passes of the draft model (with cache reuse across expansions). Verification requires a single target-model forward pass over \(n\) tokens with a structured attention mask. Dynamic pruning reduces \(n\) in uncertain regions by discarding low-probability branches, improving the trade-off between draft overhead and verification parallelism.


\section{Experiments}
\label{experiments}

\subsection{Experimental Setup}
\paragraph{Models.}
We evaluate DynaTree using models from the Pythia family~\cite{pythia}. Our target model \(M_T\) is Pythia-2.8B (2.8 billion parameters), and our draft model \(M_D\) is Pythia-70M (70 million parameters). All experiments use deterministic greedy decoding, ensuring that the output sequence is uniquely determined by the model and prefix.

\paragraph{Hardware and software.}
All experiments are conducted on a single NVIDIA GPU with sufficient memory to accommodate both models simultaneously. We implement DynaTree in PyTorch~\cite{pytorch} using the HuggingFace Transformers library~\cite{transformers} for model loading and inference, leveraging dynamic key-value cache structures to minimize memory overhead during tree verification.

\paragraph{Workloads and data preprocessing.}
Our primary benchmark uses WikiText-2~\citep{merity2016pointer}. For each sampled prompt, we generate \(T\) new tokens using greedy decoding, where \(T=1000\) for the main results (Section~\ref{main-results}). We truncate prompts to a maximum length \(L_{\max}\) to control prefill cost, and use a fixed \(L_{\max}=800\) throughout WikiText-2 experiments. We evaluate \(N=10\) prompts and discard the first \(W=2\) runs as warmup; we report the mean and standard deviation over the remaining runs. To ensure fair comparison, we synchronize GPU execution and reset cached states between methods. We additionally report cross-dataset results on PG-19~\citep{rae2019compressive} (Section~\ref{dataset-robustness}) to validate robustness beyond Wikipedia-style text; full experimental configurations are summarized in Appendix~\ref{app:exp-config}.

\subsection{Evaluation Metrics}
We report \textbf{throughput} (tokens per second) as the primary metric, computed as \(T\) divided by the wall-clock decoding time (excluding warmup). We additionally report \textbf{speedup} relative to autoregressive decoding, i.e., \(\mathrm{speedup}=\mathrm{TPS}/\mathrm{TPS}_{\mathrm{AR}}\). To characterize verification efficiency, we measure the \textbf{acceptance rate} \(a\), defined as the fraction of drafted tokens that match the target model's greedy predictions under the corresponding conditioning contexts, and the average \textbf{tokens per iteration} \(\bar{L}\), i.e., the number of committed tokens per verification round. We also report \textbf{average path length} \(\bar{\ell}\) (the mean depth of the greedy-consistent committed path before the bonus token) to reflect how far each verification step progresses. For latency, we include \textbf{time-to-first-token} (TTFT) and \textbf{time-per-output-token} (TPOT), averaged over prompts.

\subsection{Baselines}
We compare the proposed confidence-aware adaptive decoding against the following baselines and variants under identical greedy decoding settings:
\begin{itemize}
  \item \textbf{Autoregressive (AR):} Standard greedy decoding with the target model, serving as the performance baseline.
  \item \textbf{Linear speculative decoding:} A linear-chain speculative decoder that drafts \(K\) tokens with the draft model and verifies them with the target model in parallel, committing the longest greedy-consistent prefix~\citep{leviathan2023fast}.
  \item \textbf{Fixed Tree:} A static tree speculative decoder with fixed depth \(D\), fixed branching factor \(B\), and node budget \(N_{\max}\), representing a non-adaptive tree baseline.
  \item \textbf{DynaTree:} Our final method, which adds three adaptive components on top of the fixed-tree backbone: Phase~1 (Dynamic Breadth), Phase~2 (Dynamic Depth), and Phase~3 (History Adaptation).
\end{itemize}

\subsection{Main Results}
\label{main-results}
Table~\ref{tab:main-results} reports the end-to-end performance on WikiText-2 for \(T=1000\) token generation. Compared with autoregressive decoding, linear speculative decoding improves throughput to 156.6~t/s (1.15\(\times\)) by verifying short draft chains. A fixed tree further increases throughput to 191.9~t/s (1.41\(\times\)) by exploring multiple candidates per verification step. \textbf{DynaTree} achieves the best throughput, reaching \textbf{198.7~t/s} (\textbf{1.46\(\times\)}), improving over linear speculative decoding and the static-tree baseline under identical greedy decoding.

\begin{table}[t]
\centering
\caption{\textbf{Main results on WikiText-2 (greedy decoding, \(T=1000\)).} Throughput is measured in tokens per second (t/s). Speedup is relative to autoregressive decoding. Accept.\ rate is the fraction of drafted tokens that match the target model's greedy predictions; Tokens/Iter reports the average number of tokens committed per verification round; Avg.\ Path Len is the mean committed path length (before the bonus token). We report mean\(\pm\)std over prompts (excluding warmup).}
\label{tab:main-results}
\small
\begin{tabular}{lccccc}
\toprule
Method & Throughput (t/s) & Speedup & Accept. (\%) & Tokens/Iter & Avg.\ Path Len \\
\midrule
AR & 135.6\(\pm\)0.8 & 1.00\(\times\) & -- & 1.00 & -- \\
Linear Spec (\(K=5\)) & 156.6\(\pm\)20.2 & 1.15\(\times\) & 93.5 & 4.61 & 4.67 \\
Fixed Tree (\(D=5,B=2\)) & 191.9\(\pm\)14.0 & 1.41\(\times\) & 80.8 & 5.65 & 5.65 \\
\textbf{DynaTree} & \textbf{198.7\(\pm\)42.2} & \textbf{1.46\(\times\)} & \textbf{94.7} & \textbf{6.49} & \textbf{6.63} \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:main-results} visualizes the throughput and speedup comparison. Although linear speculative decoding achieves high per-token acceptance, it discards the remainder of the drafted chain after an early mismatch. In contrast, tree-based methods can commit longer greedy-consistent prefixes per iteration by exploring multiple candidates in parallel. DynaTree's confidence-aware adaptation further improves this trade-off, yielding the best end-to-end throughput.

\paragraph{Latency breakdown analysis.}
To understand fine-grained latency characteristics, Table~\ref{tab:latency-metrics} reports Time-To-First-Token (TTFT) and Time-Per-Output-Token (TPOT) on WikiText-2 for \(T=1000\). Speculative decoding substantially reduces TTFT relative to autoregressive decoding. Moreover, DynaTree reduces TPOT to 5.30~ms while maintaining greedy consistency, consistent with its higher tokens-per-iteration progress.

\begin{table}[t]
\centering
\caption{\textbf{Latency metrics on WikiText-2 (\(T=1000\)).} We report TTFT (latency to first output token) and TPOT (average per-token latency). Values are mean\(\pm\)std over prompts (excluding warmup).}
\label{tab:latency-metrics}
\small
\begin{tabular}{lcc}
\toprule
Method & TTFT (ms) & TPOT (ms) \\
\midrule
AR & 18.8\(\pm\)6.0 & 7.35\(\pm\)0.04 \\
Linear Spec (\(K=5\)) & 14.7\(\pm\)5.7 & 6.48\(\pm\)0.88 \\
Fixed Tree (\(D=5,B=2\)) & 12.6\(\pm\)2.7 & 5.23\(\pm\)0.37 \\
\textbf{DynaTree} & 13.8\(\pm\)3.6 & \textbf{5.30\(\pm\)1.39} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{../figures/main_results_bars.pdf}
  \caption{\textbf{Throughput and speedup on WikiText-2 (\(T=1000\)).} Left: absolute throughput (tokens/s). Right: speedup relative to autoregressive decoding. DynaTree achieves the highest throughput (198.7~t/s, 1.46\(\times\)), improving over linear speculative decoding (156.6~t/s, 1.15\(\times\)) and the fixed-tree baseline (191.9~t/s, 1.41\(\times\)).}
  \label{fig:main-results}
\end{figure}

\subsection{Ablation Study}
\label{sec:ablation}
We perform a progressive ablation study on WikiText-2 (\(T=500\)) to isolate the contribution of each adaptive component. We start from a fixed-tree baseline and incrementally enable Phase~1 (Dynamic Breadth), Phase~2 (Dynamic Depth), and Phase~3 (History Adaptation). Table~\ref{tab:ablation} reports results under a matched base depth \(D_0=5\), showing that dynamic depth provides the largest marginal gain, while history adaptation further improves stability and acceptance.

\begin{table}[t]
\centering
\caption{\textbf{Progressive ablation on WikiText-2 (\(T=500\)).} We compare a fixed tree (\(D_0=5,B=2\)) with Phase~1/2/3 variants that incrementally add Dynamic Breadth, Dynamic Depth, and History Adaptation. Speedup is computed relative to the autoregressive baseline.}
\label{tab:ablation}
\small
\begin{tabular}{lcccc}
\toprule
Variant & Throughput (t/s) & Speedup & \(\Delta\) vs Fixed & Accept. (\%) \\
\midrule
AR & 132.7\(\pm\)0.9 & 1.00\(\times\) & -- & -- \\
Fixed Tree (\(D_0=5,B=2\)) & 177.0\(\pm\)21.4 & 1.33\(\times\) & 0.0\% & 73.8 \\
Phase~1: Dynamic Breadth & 174.0\(\pm\)26.0 & 1.31\(\times\) & $-$1.7\% & 72.9 \\
Phase~2: Dynamic Depth & 183.5\(\pm\)34.2 & 1.38\(\times\) & +3.7\% & 75.9 \\
\textbf{Phase~3: History Adaptation} & \textbf{187.2\(\pm\)35.1} & \textbf{1.41\(\times\)} & \textbf{+5.8\%} & \textbf{80.3} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Hyperparameter Sensitivity}
To understand the impact of design choices, we perform a comprehensive grid search over tree depth \(D \in \{3,4,5,6,7,8\}\), branching factor \(B \in \{2,3,4\}\), and pruning threshold \(\tau \in \{0.01,0.02,0.03,0.05,0.1\}\) across generation lengths ranging from 100 to 1000 tokens, totaling 450 configurations. Figure~\ref{fig:tree-config} illustrates the throughput trends across these parameters for 500-token generation. Key findings include: (i)~\emph{Depth} exhibits diminishing returns beyond D=8, as verification overhead grows faster than the expected path length (Figure~\ref{fig:tree-config}b); (ii)~\emph{Branching factor} B=3 provides the best throughput-exploration trade-off, with B=4 introducing too much verification cost (Figure~\ref{fig:tree-config}a); (iii)~\emph{Pruning threshold} \(\tau=0.03\) is optimal, balancing aggressive pruning (which may discard valid paths) and loose thresholds (which waste computation on unlikely branches) (Figure~\ref{fig:tree-config}c). These results confirm that adaptive probability-threshold pruning is essential to maintain an effective tree size within the verification budget. Additional detailed visualizations across all generation lengths are provided in Appendix~\ref{app:sweep} (Figure~\ref{fig:param-sweep}).

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../figures/tree_config_comparison.pdf}
  \caption{\textbf{Impact of tree configuration on throughput (500-token generation).} (a)~Branch factor impact for different depths (fixed \(\tau=0.03\)): B=3 achieves optimal throughput across all depths, with deeper trees benefiting more from branching. (b)~Depth impact for different branch factors (fixed \(\tau=0.03\)): throughput increases with depth up to D=8, after which verification overhead dominates. (c)~Pruning threshold impact for different depths (fixed B=3): \(\tau=0.03\) balances exploration and cost across all depths. The optimal configuration (D=8, B=3, \(\tau=0.03\), highlighted in terra cotta) achieves 221.4~tokens/s.}
  \label{fig:tree-config}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.65\linewidth]{../figures/tree_config_heatmap.pdf}
  \caption{\textbf{Tree configuration heatmap (500-token generation, \(\tau\)=0.05).} Heatmap visualization of throughput across depth and branch factor combinations. Each cell shows the achieved throughput (tokens/s). The optimal configuration (D=7, B=2, highlighted with dashed border) achieves 172.3~tokens/s. Darker colors indicate higher throughput. The heatmap clearly shows that (i)~deeper trees (D=6,7) consistently outperform shallow trees, and (ii)~branch factor B=2 provides better throughput than B=3 across all depths, suggesting that wider exploration incurs verification overhead that outweighs the benefit of additional paths at this threshold setting.}
  \label{fig:heatmap}
\end{figure}

\subsection{Sequence Length Scaling}
Figure~\ref{fig:length-scaling} and Table~\ref{tab:length-scaling} examine how DynaTree's performance varies with generation length. For each target length, we report the best-performing configuration identified in our parameter sweep, along with the corresponding baseline and DynaTree throughput. Several trends emerge: (i)~DynaTree achieves strong speedups across all lengths, ranging from 1.32\(\times\) at 200 tokens to 1.57\(\times\) at 1000 tokens, with highest relative gain of 1.54\(\times\) at 100 tokens; (ii)~Absolute throughput increases with length, reaching 205.6 tokens/sec at 1000 tokens, as the amortized cost per token decreases; (iii)~The optimal tree depth varies with length (D=5 for 100 tokens, D=6 for 200-500 tokens, D=7 for 750-1000 tokens), reflecting the need to balance exploration breadth with verification overhead as sequence length grows. As shown in Figure~\ref{fig:length-scaling}, DynaTree consistently outperforms Linear Speculative Decoding across all tested generation lengths, demonstrating robust performance scaling.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\linewidth]{../figures/length_scaling.pdf}
  \caption{\textbf{Throughput across different generation lengths.} DynaTree consistently outperforms all baselines across generation lengths from 100 to 1000 tokens. Speedup ranges from 1.32\(\times\) at 200 tokens to 1.57\(\times\) at 1000 tokens, with a peak of 1.54\(\times\) at 100 tokens. The autoregressive baseline (AR) shows stable throughput across lengths, while DynaTree's advantage varies with length as the trade-off between exploration benefits and verification overhead shifts. Linear methods show decreasing speedups at longer lengths, while DynaTree maintains robust acceleration.}
  \label{fig:length-scaling}
\end{figure}

\begin{table}[t]
\centering
\caption{\textbf{Performance across different generation lengths.} For each target length, we report the optimal hyperparameter configuration and the resulting throughput and speedup. DynaTree achieves consistent speedups across all lengths, with highest relative gain at 100 tokens (1.54\(\times\)) and highest absolute throughput at 1000 tokens (205.6 t/s). The optimal depth increases with generation length.}
\label{tab:length-scaling}
\begin{tabular}{lccccc}
    \toprule
Length & Optimal Config & Baseline (t/s) & DynaTree (t/s) & Speedup & Accept. \\
    \midrule
100  & D=5, B=2, \(\tau\)=0.05 & 73.5  & 113.4 & 1.54\(\times\) & 50.3\% \\
200  & D=6, B=2, \(\tau\)=0.05 & 125.7 & 166.2 & 1.32\(\times\) & 72.0\% \\
500  & D=6, B=2, \(\tau\)=0.05 & 133.2 & 185.3 & 1.39\(\times\) & 79.5\% \\
750  & D=7, B=2, \(\tau\)=0.05 & 126.3 & 186.7 & 1.48\(\times\) & 75.1\% \\
1000 & D=7, B=2, \(\tau\)=0.05 & 131.0 & 205.6 & 1.57\(\times\) & 73.3\% \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Cross-Dataset Robustness}
\label{dataset-robustness}
To evaluate robustness across text domains, we compare performance on WikiText-2~\citep{merity2016pointer} (encyclopedic articles) and PG-19~\citep{rae2019compressive} (long-form fiction) using the same models and greedy decoding with \(T=1000\). Table~\ref{tab:dataset-comparison} and Figure~\ref{fig:dataset-comparison} summarize results. Across both datasets, tree-based speculation provides substantially higher throughput than linear chains by committing longer greedy-consistent prefixes per verification step. On PG-19, DynaTree remains competitive with the static-tree baseline while consistently outperforming linear speculative decoding, indicating that confidence-aware adaptation transfers beyond Wikipedia-style text.

\begin{table}[t]
\centering
\caption{\textbf{Cross-dataset performance comparison (\(T=1000\)).} We compare AR, linear speculative decoding, a fixed-tree baseline, and DynaTree on WikiText-2 and PG-19. Values are mean throughput (t/s) with speedup relative to AR; both datasets use the same model pair and greedy decoding.}
\label{tab:dataset-comparison}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{PG-19} & \multicolumn{2}{c}{WikiText-2} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& Throughput (t/s) & Speedup & Throughput (t/s) & Speedup \\
\midrule
AR & 131.4 & 1.00\(\times\) & 135.6 & 1.00\(\times\) \\
Linear Spec (\(K=5\)) & 153.3 & 1.17\(\times\) & 156.6 & 1.15\(\times\) \\
Fixed Tree (\(D=5,B=2\)) & \textbf{179.1} & \textbf{1.36\(\times\)} & 191.9 & 1.41\(\times\) \\
DynaTree & 176.5 & 1.34\(\times\) & \textbf{198.7} & \textbf{1.46\(\times\)} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{../figures/dataset_comparison.pdf}
  \caption{\textbf{Cross-dataset performance comparison (\(T=1000\)).} (a)~Throughput and (b)~speedup on PG-19 and WikiText-2 for AR, linear speculative decoding, a fixed-tree baseline, and DynaTree.}
  \label{fig:dataset-comparison}
\end{figure}

\subsection{Prompt Length Sensitivity}
The length of the input prompt can significantly impact decoding performance due to prefill overhead and context dependencies. To assess DynaTree's sensitivity to prompt length, we evaluate all methods on WikiText-2 with varying maximum prompt lengths: 100, 200, 800, and 1000 tokens. As shown in Table~\ref{tab:prompt-length} and Figure~\ref{fig:prompt-length}, several trends emerge: (i)~All methods achieve peak performance at moderate prompt lengths (200 tokens), where the prefill cost is amortized without excessive context overhead. DynaTree D=6 reaches 197.9 tokens/sec (1.55\(\times\) speedup) at this length; (ii)~Performance degrades slightly at very long prompts (1000 tokens), with DynaTree maintaining 162.8 tokens/sec (1.21\(\times\) speedup), as prefill overhead increases; (iii)~DynaTree's relative advantage remains consistent across prompt lengths, with speedups ranging from 1.21\(\times\) to 1.55\(\times\), demonstrating robustness to varying context sizes. These results confirm that DynaTree's tree-based exploration strategy provides stable acceleration benefits across diverse prompt length regimes, making it suitable for applications with varying context requirements.

\begin{table}[t]
\centering
\caption{\textbf{Performance across different prompt lengths (WikiText-2, 500-token generation).} We evaluate each method with varying maximum prompt lengths from 100 to 1000 tokens. DynaTree maintains consistent speedups across all prompt lengths, with peak performance at 200 tokens. All methods show slight degradation at very long prompts (1000 tokens) due to increased prefill overhead.}
\label{tab:prompt-length}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Prompt Length (tokens)} \\
\cmidrule(lr){2-5}
& 100 & 200 & 800 & 1000 \\
\midrule
AR (baseline) & 132.8 & 127.4 & 133.2 & 135.0 \\
Linear K=6 & 158.4 (1.19\(\times\)) & 175.6 (1.38\(\times\)) & 167.7 (1.26\(\times\)) & 139.8 (1.04\(\times\)) \\
Linear K=7 & 161.7 (1.22\(\times\)) & 178.2 (1.40\(\times\)) & 173.4 (1.30\(\times\)) & 143.1 (1.06\(\times\)) \\
\textbf{DynaTree D=6} & \textbf{181.2 (1.36\(\times\))} & \textbf{197.9 (1.55\(\times\))} & \textbf{185.3 (1.39\(\times\))} & \textbf{162.8 (1.21\(\times\))} \\
DynaTree D=7 & 177.0 (1.33\(\times\)) & 198.0 (1.55\(\times\)) & 184.5 (1.39\(\times\)) & 172.6 (1.28\(\times\)) \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{../figures/prompt_length_impact.pdf}
  \caption{\textbf{Performance across different prompt lengths.} (a)~Throughput vs.\ prompt length: all methods achieve peak performance at moderate prompt lengths (200 tokens), with DynaTree D=6 reaching 197.9~t/s. Performance degrades slightly at very long prompts (1000 tokens) due to prefill overhead. (b)~Speedup vs.\ prompt length: DynaTree maintains consistent relative gains (1.21--1.55\(\times\)) across all prompt lengths, demonstrating robustness to varying context sizes. Linear methods show stronger degradation at 1000 tokens, with speedups dropping to 1.04--1.06\(\times\).}
  \label{fig:prompt-length}
\end{figure}


\section{Conclusion}
We introduced DynaTree, a tree-based speculative decoding framework that drafts multiple candidate continuations and verifies them in parallel using tree attention, while controlling verification cost via probability-threshold pruning and an explicit node budget. Across Pythia models, DynaTree improves decoding throughput over autoregressive decoding and consistently outperforms strong speculative decoding baselines. Our results suggest that multi-branch exploration, coupled with lightweight pruning, is an effective way to better utilize target-model verification compute under strict budget constraints. A key direction for future work is improving robustness across diverse prompts and long-context settings, and reducing overhead via kernel-level optimizations and hardware-aware tree construction.

\bibliographystyle{unsrtnat}
\bibliography{references}

\appendix
\section{Experimental Configuration Details}
\label{app:exp-config}
\paragraph{Common settings.}
Unless stated otherwise, we use WikiText-2 prompts, truncate the prompt length to \(L_{\max}=800\), and generate \(T\) new tokens with greedy decoding. We evaluate \(N=10\) prompts and discard the first \(W=2\) runs as warmup.
\paragraph{PG-19 cross-dataset setting.}
For PG-19, we use the same model pair and greedy decoding, with a maximum prompt length \(L_{\max}=1000\).
\paragraph{Fixed-tree baseline.}
For the static-tree baseline, we use depth \(D=5\), branching factor \(B=2\), and a node budget \(N_{\max}=256\).
\paragraph{DynaTree.}
Unless stated otherwise, the adaptive configuration uses base depth \(D_0=5\), maximum depth \(D_{\max}=8\), branch bounds \((B_{\min},B_{\max})=(1,3)\), and confidence thresholds \((\tau_h,\tau_\ell)=(0.9,0.4)\).
\section{Hyperparameter Sweep Details}
\label{app:sweep}
We perform an exhaustive grid search over tree depth $D \in \{3,4,5,6,7,8\}$, branching factor $B \in \{2,3,4\}$, and pruning threshold $\tau \in \{0.01,0.02,0.03,0.05,0.1\}$ across generation lengths 100--1000 tokens, totaling 450 distinct configurations. Each configuration is evaluated with 2 independent runs (excluding warmup) to estimate average throughput and speedup. Figure~\ref{fig:param-sweep} visualizes the relationships between hyperparameters and performance, revealing the complex trade-offs between tree exploration, verification cost, and effective path length. The results confirm that no single configuration dominates across all generation lengths, motivating adaptive hyperparameter selection based on the target workload.

\section{Memory Footprint Analysis}
\label{app:memory}
An important practical consideration for speculative decoding methods is their memory overhead. Table~\ref{tab:memory-footprint} reports peak GPU memory consumption across methods on PG-19 and WikiText-2 datasets during 500-token generation with Pythia models. All measurements are taken using PyTorch's memory profiler during steady-state generation (excluding initial model loading).

Key observations: (i)~DynaTree incurs minimal memory overhead (+0.45\% on average) compared to the autoregressive baseline, adding only~26~MB to accommodate the draft model's KV cache and intermediate tree structures; (ii)~Linear speculative methods show similarly negligible overhead ($<$1\%), as they maintain only a small fixed-size buffer of candidate tokens. Across all methods, memory overhead remains well within 1\% of the baseline, confirming that speculative decoding's primary cost is computational rather than memory-related. This makes DynaTree suitable for memory-constrained deployment scenarios where the target and draft models can already fit in GPU memory.

\begin{table}[t]
\centering
\caption{\textbf{Peak GPU memory consumption comparison.} We measure peak memory usage during 500-token generation on PG-19 (long-form fiction) and WikiText-2 (structured articles) with Pythia-2.8B and Pythia-70M models. All speculative methods incur minimal memory overhead ($<$1\% relative to baseline), with DynaTree adding only 0.45\% on average to accommodate tree structures and draft KV cache. Values show mean peak memory across 10 runs; relative change is computed against the autoregressive baseline.}
\label{tab:memory-footprint}
\small
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Peak Memory (MB)} & \multirow{2}{*}{Average (MB)} & \multirow{2}{*}{Rel. Change} \\
\cmidrule(lr){2-3}
& PG-19 & WikiText-2 & & \\
\midrule
AR (baseline) & 5855.1 & 5798.6 & 5826.9 & 0.00\% \\
Linear K=6 & 5817.3 & 5786.3 & 5801.8 & $-$0.43\% \\
Linear K=7 & 5817.7 & 5786.2 & 5801.9 & $-$0.43\% \\
\textbf{DynaTree (D=6, B=2)} & \textbf{5883.7} & \textbf{5822.9} & \textbf{5853.3} & \textbf{+0.45\%} \\
DynaTree (D=7, B=2) & 5883.7 & 5822.9 & 5853.3 & +0.45\% \\
\bottomrule
\end{tabular}
\end{table}

\section{DynaTree Iteration Pseudocode}
\label{app:algo}
\begin{algorithm}[t]
\caption{\textbf{DynaTree: one iteration (greedy-consistent).}}
\label{alg:dynatree}
\small
\begin{algorithmic}[1]
\Require Prefix tokens \(x_{1:t}\); target KV cache \(\mathcal{K}_T\); prefix next-token logits \(\mathbf{s}_{\text{last}}\);
branch bounds \(B_{\min}\le B_{\mathrm{mid}}\le B_{\max}\); confidence thresholds \(0<\tau_\ell<\tau_h<1\); base depth \(D_0\); max depth \(D_{\max}\); depth thresholds \(0<\rho_{\mathrm{stop}}<\rho_{\mathrm{deep}}<1\); pruning threshold \(\tau\); node budget \(N_{\max}\); history window \(W\).
\Ensure Committed tokens \(y_{t+1:t+L}\) and updated \(\mathcal{K}_T\).

\State \(\ell \gets \Call{SeqLen}{\mathcal{K}_T}\) \Comment{record prefix cache length}
\State \(\mathcal{T} \gets \Call{DraftTree}{x_{1:t}, B_{\min}, B_{\mathrm{mid}}, B_{\max}, \tau_\ell, \tau_h, D_0, D_{\max}, \rho_{\mathrm{stop}}, \rho_{\mathrm{deep}}, \tau, N_{\max}}\)
\State \(\mathbf{z}_{1:n} \gets \Call{BFSFlatten}{\mathcal{T}}\); \(\mathbf{A} \gets \Call{TreeMask}{\mathcal{T}, \ell}\) \Comment{prefix + ancestors only}
\State \(\mathbf{s}_{1:n} \gets M_T(\mathbf{z}_{1:n}; \mathbf{A}, \mathcal{K}_T)\); \(\hat{\mathbf{y}} \gets \arg\max \mathbf{s}_{1:n}\)
\State \(y_{t+1:t+L} \gets \Call{SelectCommit}{\mathcal{T}, \hat{\mathbf{y}}, \mathbf{s}_{\text{last}}}\)
\State \(\mathcal{K}_T \gets \Call{Crop}{\mathcal{K}_T,\ell}\); \(\mathcal{K}_T \gets M_T(y_{t+1:t+L}; \mathcal{K}_T)\) \Comment{rollback + rebuild}
\State \Call{UpdateHistory}{$y_{t+1:t+L}, \mathcal{T}, W$}; \Call{Adjust}{$\tau_\ell,\tau_h,D_0$} \Comment{historical adjustment}
\State \Return \(y_{t+1:t+L}\)

\Statex
\Function{\textproc{DraftTree}}{$x_{1:t}, B_{\min}, B_{\mathrm{mid}}, B_{\max}, \tau_\ell, \tau_h, D_0, D_{\max}, \rho_{\mathrm{stop}}, \rho_{\mathrm{deep}}, \tau, N_{\max}$}
  \Comment{Draft a candidate token tree with adaptive branching, dynamic depth control, and pruning under a node budget.}
  \State Run \(M_D\) on \(x_{1:t}\); let \(u_0\) be the \(\top 1\) token; initialize \(\mathcal{T}\) with root \(u_0\)
  \State \(\mathcal{A} \gets \{u_0\}\) \Comment{active frontier}
  \While{$|\mathcal{A}|>0$ \textbf{and} $|\mathcal{T}|<N_{\max}$}
    \State Pop an element \(u\) from \(\mathcal{A}\)
    \If{$\exp(\bar{\ell}_u) < \tau$} \State \textbf{continue} \EndIf \Comment{probability-threshold pruning}
    \If{$d(u)\ge D_{\max}$} \State \textbf{continue} \EndIf
    \If{$\exp(\bar{\ell}_u) < \rho_{\mathrm{stop}}$} \State \textbf{continue} \EndIf \Comment{early stopping}
    \If{$d(u)\ge D_0$ \textbf{and} $\exp(\bar{\ell}_u) < \rho_{\mathrm{deep}}$} \State \textbf{continue} \EndIf \Comment{depth gating}
    \State Do one cached step of \(M_D\) from \(u\); compute confidence \(c(u)=\max \mathrm{softmax}(\mathbf{h}(u))\)
    \State Set \(B(u)\gets B_{\min}\) if \(c(u)\ge\tau_h\), \(B_{\max}\) if \(c(u)<\tau_\ell\), else \(B_{\mathrm{mid}}\)
    \State Take \(\top B(u)\) next-token candidates; add each child \(v\) to \(\mathcal{T}\) and push \(v\) into \(\mathcal{A}\) until \(N_{\max}\)
  \EndWhile
  \State \Return \(\mathcal{T}\)
\EndFunction

\Statex
\Function{\textproc{SelectCommit}}{$\mathcal{T}, \hat{\mathbf{y}}, \mathbf{s}_{\text{last}}$}
  \Comment{Select the longest greedy-consistent path (plus one bonus token).}
  \State \(first \gets \arg\max \mathbf{s}_{\text{last}}\)
  \State Find the longest path \(P\) where root token \(=\) \(first\), and for each edge \((u\!\rightarrow\!v)\), token\((v)=\hat{\mathbf{y}}[\text{pos}(u)]\)
  \If{$P=\emptyset$}
    \State \Return \([first]\)
  \Else
    \State \(y \gets\) tokens on \(P\)
    \State Append one bonus token \(\hat{\mathbf{y}}[\text{pos}(\text{last}(P))]\)
    \State \Return \(y\)
  \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.75\linewidth]{../figures/param_sweep.pdf}
  \caption{\textbf{Comprehensive parameter sweep analysis across 450 configurations.} (a)~Speedup vs.\ tree depth $D$ (fixing $B=3$, $\tau=0.03$, 500 tokens): deeper trees improve speedup up to $D=8$, after which verification overhead dominates. (b)~Speedup vs.\ branching factor $B$ (fixing $D=8$, $\tau=0.03$, 500 tokens): $B=3$ achieves the best balance between exploration and cost. (c)~Speedup vs.\ pruning threshold $\tau$ (fixing $D=8$, $B=3$, 500 tokens, log scale): $\tau=0.03$ is optimal, balancing aggressive and loose pruning. (d)~Best speedup across generation lengths with corresponding optimal $(D,B,\tau)$ configurations: speedup peaks at 500 tokens and remains strong at other lengths. (e)~Average path length heatmap over $(D,B)$ (fixing $\tau=0.03$, 500 tokens): larger trees enable longer committed paths. (f)~Acceptance rate distribution vs.\ pruning threshold (fixing $D=8$, $B=3$, 500 tokens): tighter pruning slightly reduces acceptance but improves overall throughput. Stars mark optimal configurations in each subplot.}
  \label{fig:param-sweep}
\end{figure}

\end{document}