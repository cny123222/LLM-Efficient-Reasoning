\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025
\PassOptionsToPackage{numbers,sort&compress}{natbib}

% The authors should use one of these tracks.
% Before accepting by the NeurIPS conference, select one of the options below.
% 0. "default" for submission
% \usepackage{neurips_2025}
% Using "final" option to show author names (not for actual submission)
\usepackage[preprint]{neurips_2025}
% the "default" option is equal to the "main" option, which is used for the Main Track with double-blind reviewing.
% 1. "main" option is used for the Main Track
%  \usepackage[main]{neurips_2025}
% 2. "position" option is used for the Position Paper Track
%  \usepackage[position]{neurips_2025}
% 3. "dandb" option is used for the Datasets & Benchmarks Track
 % \usepackage[dandb]{neurips_2025}
% 4. "creativeai" option is used for the Creative AI Track
%  \usepackage[creativeai]{neurips_2025}
% 5. "sglblindworkshop" option is used for the Workshop with single-blind reviewing
 % \usepackage[sglblindworkshop]{neurips_2025}
% 6. "dblblindworkshop" option is used for the Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop]{neurips_2025}

% After being accepted, the authors should add "final" behind the track to compile a camera-ready version.
% 1. Main Track
 % \usepackage[main, final]{neurips_2025}
% 2. Position Paper Track
%  \usepackage[position, final]{neurips_2025}
% 3. Datasets & Benchmarks Track
 % \usepackage[dandb, final]{neurips_2025}
% 4. Creative AI Track
%  \usepackage[creativeai, final]{neurips_2025}
% 5. Workshop with single-blind reviewing
%  \usepackage[sglblindworkshop, final]{neurips_2025}
% 6. Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop, final]{neurips_2025}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote.
% For workshops (5., 6.), the authors should add the name of the workshop, "\workshoptitle" command is used to set the workshop title.
% \workshoptitle{WORKSHOP TITLE}

% "preprint" option is used for arXiv or other preprint submissions
 % \usepackage[preprint]{neurips_2025}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hypertexnames=false]{hyperref}       % hyperlinks (avoid duplicate destination warnings)
\usepackage{placeins}       % \FloatBarrier to keep floats out of references
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{multirow}       % multirow cells in tables
\usepackage{graphicx}       % figures
\usepackage{amsmath}        % math
\usepackage{amssymb}        % math symbols
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{algorithm}     % float wrapper
\usepackage{algpseudocode} % algorithmicx style (supports \Call)

% --- Float layout tuning (NeurIPS-style: reduce large whitespace & avoid float-only pages) ---
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.90}
\renewcommand{\bottomfraction}{0.80}
\renewcommand{\textfraction}{0.07}
\renewcommand{\floatpagefraction}{0.85}
\setlength{\textfloatsep}{8pt plus 2pt minus 2pt}
\setlength{\floatsep}{6pt plus 2pt minus 2pt}
\setlength{\intextsep}{6pt plus 2pt minus 2pt}

% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote. 
\title{DynaTree: Confidence-Aware Adaptive Tree Speculative Decoding for Efficient LLM Inference}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Nuoyan Chen$^*$ \quad
  Jiamin Liu$^*$ \quad
  Zhaocheng Li\thanks{Equal contribution.} \\
  School of Computer Science\\
  Shanghai Jiao Tong University\\
  \texttt{\{cny123222, logic-1.0, lzc050419\}@sjtu.edu.cn}
}


\begin{document}


\maketitle


\begin{abstract}
Autoregressive decoding in large language models (LLMs) is fundamentally sequential and therefore underutilizes modern accelerator parallelism during token generation. Speculative decoding mitigates this bottleneck by letting a lightweight draft model propose multiple tokens that are verified in parallel by the target model; however, linear variants explore only a single draft chain per step and can waste substantial computation when early tokens are rejected, while existing tree-based approaches employ \emph{fixed} structures that cannot adapt to varying draft model confidence. We propose \textbf{DynaTree}, a tree-based speculative decoding framework with \emph{confidence-aware adaptive branching} that dynamically adjusts tree structure based on draft model uncertainty through adaptive per-node branching, dynamic depth control, and history-based adjustment, combined with probability-threshold pruning under an explicit node budget. Experiments on Pythia models show that DynaTree reaches 218.5~tokens/s on WikiText-2 (1.65$\times$ speedup) and improves throughput over a fixed-tree baseline by 16.2\%, while consistently outperforming linear speculative decoding across both WikiText-2 and PG-19.
\end{abstract}


\section{Introduction}

Large language models (LLMs) are typically deployed with autoregressive decoding, where each output token is generated after conditioning on all previously generated tokens. While transformer inference can exploit parallelism during the prefill stage, the decode stage remains inherently sequential and requires a full forward pass per token, leading to poor hardware utilization and high latency~\cite{flashattention,vllm}.

Speculative decoding alleviates this bottleneck by separating \emph{proposal} and \emph{verification}~\cite{leviathan2023fast}. A small draft model proposes several candidate tokens, and the target model verifies them in parallel; when the proposal matches the target distribution, multiple tokens can be committed per iteration. Importantly, with rejection sampling, speculative decoding preserves the exact output distribution of the target model~\cite{decoding_speculative}.

In practice, most speculative decoding systems employ \emph{linear} drafting: the draft model proposes a single chain of $K$ tokens. This design is brittle under draft--target mismatch. A rejection at an early position forces all subsequent drafted tokens to be discarded, wasting both draft computation and target-model verification work. This single-path exploration fundamentally limits achievable speedups, as the system cannot recover from early errors without restarting the drafting process~\cite{decoding_speculative}.

Tree-based speculation offers a natural remedy. When multiple plausible next tokens compete, exploring several continuations in parallel increases the chance that at least one path aligns with the target model, thereby improving the expected number of accepted tokens per verification step. The draft expands multiple candidates via top-$k$ branching, and the target verifies the resulting token tree in parallel with a structured, causality-preserving attention mask. This multi-path exploration addresses the fundamental brittleness of linear drafting.

While tree-based drafting addresses the single-path limitation of linear methods, existing approaches typically employ \emph{fixed} tree configurations with predetermined depth and branching factor~\cite{specinfer,medusa}. This rigid structure cannot adapt to the draft model's varying prediction confidence, creating an \emph{efficiency gap}: high-confidence predictions waste compute exploring unnecessary branches, while uncertain predictions suffer from insufficient exploration. Recent adaptive methods~\cite{cm_asd,adaeagle,cas_spec} adjust draft length or employ learned predictors, yet most focus on linear speculation rather than fundamentally restructuring the tree itself. We hypothesize that \emph{confidence-aware} tree construction---dynamically adjusting branching per node based on draft uncertainty---can bridge this gap while maintaining robust exploration.

We present \textbf{DynaTree}, which addresses the efficiency gap through confidence-aware adaptive branching that dynamically adjusts tree structure based on draft model uncertainty. Our three-phase mechanism adapts per-node branching (1--3 branches), implements dynamic depth control via early stopping and deep expansion, and tunes parameters based on recent verification outcomes. Combined with probability-threshold pruning under an explicit node budget, DynaTree verifies candidate paths in a single forward pass using tree attention. Empirically, DynaTree achieves 218.5~tokens/s on WikiText-2 (1.65$\times$ speedup), outperforming a fixed-tree baseline by 16.2\%.

In summary, our contributions are:
\begin{itemize}
  \item We propose a tree-based speculative decoding framework with \textbf{confidence-aware adaptive branching} that dynamically adjusts tree structure based on draft model uncertainty, directly addressing the efficiency gap in static configurations.
  
  \item We introduce a three-phase adaptive mechanism combining confidence-based branching, dynamic depth control, and historical parameter tuning. Our ablation study reveals that dynamic depth contributes most to performance gains.
  
  \item Experiments demonstrate that DynaTree improves throughput over autoregressive decoding, linear speculative decoding, and a fixed-tree baseline across WikiText-2 and PG-19, with a 1.65$\times$ speedup on WikiText-2 at \(T=1500\).
\end{itemize}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.92\linewidth]{../figures/decode-v1.png}
  \caption{\textbf{Comparison of three decoding paradigms.} \textbf{Left:} Autoregressive (AR) decoding generates tokens sequentially, requiring one LLM forward pass per token. \textbf{Middle:} Linear speculative decoding drafts a single token chain; early rejection wastes all subsequent drafted tokens. \textbf{Right:} Tree-based speculative decoding (DynaTree) explores multiple paths in parallel, enabling recovery from draft errors and committing longer sequences per iteration. The multi-path exploration fundamentally addresses the brittleness of linear drafting while maintaining output correctness through structured tree attention verification.}
  \label{fig:decode-comparison}
\end{figure}

\section{Related Work}

\subsection{Speculative Decoding}

Speculative decoding accelerates autoregressive generation by decoupling \emph{proposal} and \emph{verification}: a lightweight draft model proposes multiple tokens, and the target model verifies these candidates in parallel while preserving the exact output distribution~\cite{leviathan2023fast,decoding_speculative,draft_tradeoff}. The memory-bound nature of LLM inference (approximately 1 FLOP/byte operational intensity) makes parallel verification particularly valuable~\cite{flashattention,vllm}. Recent work highlights robustness challenges across heterogeneous workloads and long-context inputs~\cite{spin,owl,judge_decoding}, but the dominant \emph{linear} drafting paradigm suffers from a fundamental inefficiency: when early tokens are rejected, all downstream draft tokens are discarded, wasting computation.

\subsection{Tree-Based Speculative Decoding}

To overcome single-path inefficiency, tree-based methods draft multiple candidate continuations and verify them in one target-model forward pass using structured attention masks~\cite{specinfer,opt_tree,medusa,traversal_verification}. SpecInfer~\cite{specinfer} pioneered token tree verification with expansion-based and merge-based construction mechanisms. OPT-Tree~\cite{opt_tree} algorithmically searches for tree structures that maximize expected acceptance length, while Medusa~\cite{medusa} augments models with multiple decoding heads to eliminate separate draft models. However, these methods predominantly use \emph{fixed} tree configurations with static depth $D$ and branching factor $B$, creating an efficiency gap when draft confidence varies~\cite{specinfer,medusa}.

Recent adaptive approaches address this rigidity through dynamic parameter adjustment. CM-ASD~\cite{cm_asd} modulates drafting length and verification thresholds based on entropy, logit margin, and softmax margin, achieving 4--5$\times$ speedups. AdaEAGLE~\cite{adaeagle} employs a lightweight MLP to predict optimal draft length per iteration. CAS-Spec~\cite{cas_spec} introduces dynamic tree cascades with acceptance rate heuristics, improving throughput by 47\%. While promising, these methods focus on linear speculation or require learned predictors. In contrast, DynaTree directly restructures trees via confidence-aware branching, dynamic depth control, and history-based adjustment, yielding a 16.2\% throughput gain over a fixed-tree baseline in our setup while remaining training-free.

\subsection{Dynamic Pruning Strategies}

Exponential candidate growth necessitates pruning to balance exploration and verification cost. ProPD~\cite{propd} employs top-$k$ early prediction heads and weighted regression to remove low-utility branches, reducing computation by 2$\times$. CAST~\cite{cast} formalizes cost-aware breadth and depth pruning, explicitly modeling verification overhead per layer. DySpec~\cite{dyspec} uses greedy confidence-guided expansion, while RASD~\cite{rasd} prunes retrieval candidates outside the draft model's top-$k$ predictions. AdaSD~\cite{adasd} introduces hyperparameter-free thresholds based on entropy and Jensen-Shannon distance, achieving 49\% speedups. These methods differ in adaptation mechanismsâ€”from offline heuristics to online predictors. DynaTree adopts probability-threshold pruning with explicit node budgets, prioritizing simplicity and training-free integration while maintaining strong empirical performance.


\section{Methodology}
\label{method}

\subsection{Problem Setup and Notation}
Let \(M_T\) denote a target autoregressive language model and \(M_D\) a smaller draft model. Given a prefix (prompt) \(x_{1:t}\), greedy decoding with \(M_T\) produces tokens \(y_{t+1}, y_{t+2}, \dots\) where
\[
y_{i} \;=\; \arg\max_{v \in \mathcal{V}} p_T(v \mid x_{1:i-1}).
\]
Speculative decoding accelerates generation by proposing candidate tokens with \(M_D\) and verifying them with \(M_T\), while preserving the greedy output when the verification rule only commits tokens that match the target greedy predictions.

\subsection{Overview of DynaTree}
DynaTree generalizes linear speculative decoding from a single draft chain to a \emph{draft token tree}. In each iteration, it first performs \emph{adaptive tree drafting} with \(M_D\), where the effective tree breadth and depth are determined on-the-fly from the draft distribution and the cumulative path probability. Concretely, for a draft node \(u\) with draft logits \(\mathbf{h}(u)\), we define the draft confidence \(c(u)=\max_{v\in\mathcal{V}}\mathrm{softmax}(\mathbf{h}(u))_v\) and use it to select a per-node branching factor \(B(u)\in\{B_{\min},B_{\mathrm{mid}},B_{\max}\}\). We further control expansion depth via the cumulative probability \(p(u)=\exp(\bar{\ell}_u)\), combining early stopping for low-probability branches with deeper expansion along high-probability paths. The resulting candidate tree is then \textbf{verified in parallel} in a single forward pass of \(M_T\) using tree attention, followed by \textbf{greedy path selection} and \textbf{KV-cache update} for committed tokens. Finally, DynaTree maintains a short window of recent acceptance statistics to adjust drafting thresholds for subsequent iterations.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.92\linewidth]{../figures/dynatree-v9.5.png}
  \caption{\textbf{One iteration of DynaTree decoding.} The process consists of six main stages: (1)~\emph{Adaptive Tree Drafting:} The draft model expands a candidate tree with three adaptive mechanisms: confidence-aware branching adjusts the number of child nodes (1--3) per expansion based on draft model confidence; dynamic depth control implements early stopping for low cumulative probability branches and deep expansion for high-probability paths; adaptive pruning removes branches below probability threshold $\tau$ or exceeding node budget $N_{\max}$. (2)~\emph{Flattening \& Masking:} The pruned tree is serialized in breadth-first order, and a causal attention mask is constructed to ensure each node attends only to its ancestors. (3)~\emph{Parallel Verification:} The target model verifies all candidates in a single forward pass. (4)~\emph{Path Selection:} The longest path where drafted tokens match the target model's greedy predictions is identified. (5)~\emph{Cache Update:} The committed tokens are used to update the context and key-value cache for the next iteration. (6)~\emph{Historical Adjustment:} Acceptance rate from recent rounds feeds back to adjust confidence thresholds and base depth for the next iteration. This three-phase adaptive mechanism enables efficient multi-path exploration while maintaining correctness guarantees for greedy decoding.}
  \label{fig:arch}
\end{figure}

\subsection{Adaptive Tree Drafting}
We maintain a token tree \(\mathcal{T}=(\mathcal{N}, \mathcal{E})\) whose nodes \(u \in \mathcal{N}\) correspond to drafted tokens. Each node \(u\) is associated with: (i)~\emph{token} \(z_u \in \mathcal{V}\); (ii)~\emph{parent} \(\pi(u)\); (iii)~\emph{depth} \(d(u)\) from root; (iv)~\emph{draft log-probability} \(\ell_u = \log p_D(z_u \mid \text{prefix}(\pi(u)))\); and (v)~\emph{cumulative log-probability} \(\bar{\ell}_u = \sum_{v \in \text{path}(u)} \ell_v\), where \(\text{path}(u)\) denotes all nodes from root to \(u\) along the tree.

\paragraph{Tree expansion.}
Starting from the current prefix \(x_{1:t}\), we construct \(\mathcal{T}\) in a breadth-first manner under a strict node budget \(N_{\max}\). For any expandable node \(u\), let \(q_D(\cdot \mid u)\) denote the draft distribution conditioned on the unique root-to-\(u\) prefix, and define the local confidence
\[
c(u)\;=\;\max_{v\in\mathcal{V}} q_D(v\mid u).
\]
We select a \emph{per-node} branching factor via a confidence rule
\[
B(u)=
\begin{cases}
B_{\min}, & c(u)\ge \tau_h,\\
B_{\mathrm{mid}}, & \tau_\ell \le c(u) < \tau_h,\\
B_{\max}, & c(u)<\tau_\ell,
\end{cases}
\]
where \(0<\tau_\ell<\tau_h<1\) are confidence thresholds and \(1\le B_{\min}\le B_{\mathrm{mid}}\le B_{\max}\) are integer branch bounds.
and expand \(u\) by adding the \(B(u)\) highest-probability children under \(q_D(\cdot\mid u)\). To adapt depth, we use the cumulative path probability \(p(u)=\exp(\bar{\ell}_u)\): low-probability branches are terminated early, while high-probability paths may be expanded beyond a base depth. Concretely, a node at depth \(d(u)\) is eligible for expansion only if it satisfies the depth-gating rule (defined in the next paragraph) and \(|\mathcal{N}|<N_{\max}\). Implementation details for cache reuse during expansion are deferred to Appendix~\ref{app:algo}.

\paragraph{Dynamic depth control.}
Let \(D_0\) denote a base depth and \(D_{\max}\) a hard maximum depth. We gate expansion using two probability thresholds \(\rho_{\mathrm{stop}} < \rho_{\mathrm{deep}}\) on the cumulative path probability \(p(u)\). A node \(u\) at depth \(d(u)\) is expandable if and only if
\[
d(u) < D_{\max}
\quad\wedge\quad
p(u)\ge \rho_{\mathrm{stop}}
\quad\wedge\quad
\Bigl(d(u)<D_0 \;\;\vee\;\; p(u)\ge \rho_{\mathrm{deep}}\Bigr).
\]
We assume \(1\le D_0 < D_{\max}\) and \(0<\rho_{\mathrm{stop}}<\rho_{\mathrm{deep}}<1\).
The first condition enforces a hard depth limit; the second implements \emph{early stopping} by terminating branches whose joint draft probability is too small; and the third allows \emph{deep expansion} beyond \(D_0\) only along sufficiently likely paths. In practice, \(\rho_{\mathrm{stop}}\) and \(\rho_{\mathrm{deep}}\) are tuned on a held-out set (Section~\ref{experiments}).

\paragraph{Adaptive pruning under a node budget.}
To reduce wasted verification on unlikely branches, we further prune any leaf \(u\) whose cumulative probability falls below a global threshold \(\tau\in(0,1)\):
\[
p(u) \;<\; \tau \quad \Longrightarrow \quad \text{prune } u.
\]
This rule focuses the target-model verification budget on paths that are jointly plausible under the draft model. Additionally, we enforce a strict node budget \(N_{\max}\) during construction; when \(|\mathcal{N}|=N_{\max}\), expansion stops and remaining frontier nodes are treated as leaves.

\paragraph{Historical adjustment.}
Finally, DynaTree adapts drafting thresholds online using recent verification outcomes. Let \(a_r\in[0,1]\) denote the per-iteration acceptance statistic at iteration \(r\) (i.e., the fraction of drafted tokens committed in that iteration), and let \(\bar{a}_t\) be the sliding-window mean over the last \(W\) iterations:
\[
\bar{a}_t \;=\; \frac{1}{W}\sum_{i=0}^{W-1} a_{t-i}.
\]
Here \(W\) is a fixed window size.
When \(\bar{a}_t\) is high, we make drafting more aggressive (e.g., increasing \(D_0\) or lowering \(\tau_h\)); when \(\bar{a}_t\) is low, we become more conservative to avoid verification waste. We defer the exact update schedule to Appendix~\ref{app:algo}.

% (Moved long pseudocode to the appendix to save space in the main paper.)
We provide full pseudocode for one DynaTree iteration in Appendix~\ref{app:algo}.

\subsection{Tree Attention for Parallel Verification}
To verify all drafted tokens in one target-model forward pass, we \emph{flatten} the tree in breadth-first order (BFS), producing a sequence \(z_{1:n}\) where each token corresponds to one node and all ancestors appear earlier than descendants. We then construct a boolean attention mask \(\mathbf{A} \in \{0,1\}^{n \times (t+n)}\) such that each drafted token attends to: (i) all prefix tokens \(x_{1:t}\), and (ii) only its ancestors (including itself) in the flattened tree:
\[
\mathbf{A}_{i,j} =
\begin{cases}
1, & 1 \le j \le t,\\
1, & j=t+\mathrm{pos}(v) \text{ for some ancestor } v \in \mathrm{Anc}(u_i)\cup\{u_i\},\\
0, & \text{otherwise.}
\end{cases}
\]
This mask ensures the conditional distribution computed at each node matches the distribution of sequential decoding along its unique root-to-node path, while enabling parallel verification across different branches~\cite{specinfer,opt_tree}.

\subsection{Greedy Path Selection and Cache Update}
\paragraph{Verification signals.}
Let \(\hat{y}_{t+1} = \arg\max p_T(\cdot \mid x_{1:t})\) be the target model's greedy next token from the prefix (available from the prefix logits). For each tree node \(u\) with flattened position \(i\), the target forward pass outputs logits \(\mathbf{s}_i\), whose argmax \(\hat{y}(u)=\arg\max \mathbf{s}_i\) corresponds to the greedy \emph{next-token} prediction after consuming the path to \(u\).

\paragraph{Longest valid path.}
DynaTree commits the longest path \(u_0 \rightarrow u_1 \rightarrow \cdots \rightarrow u_m\) such that the drafted token at each node matches the target greedy prediction from its parent context:
\[
z_{u_0}=\hat{y}_{t+1},\quad
z_{u_{k}}=\hat{y}(u_{k-1}) \;\; \text{for } k=1,\dots,m.
\]
If no drafted token matches the first greedy prediction, we fall back to committing \(\hat{y}_{t+1}\) (one token progress). After committing the matched draft tokens, we append one \emph{bonus} token \(\hat{y}(u_m)\) from the target model, mirroring the greedy speculative decoding convention and ensuring steady progress.

\paragraph{KV-cache management.}
Tree verification may populate key-value states for branches that are ultimately not committed. To maintain consistency with sequential decoding, we must restore the cache to the state corresponding to the committed prefix. Concretely, after identifying the committed path, we: (i)~discard all cached key-value pairs beyond the original prefix length \(t\); and (ii)~perform a forward pass of the committed tokens through the target model to populate the cache correctly for the next iteration. This ensures that subsequent iterations start from an identical cache state as sequential greedy decoding would produce.

\subsection{Correctness for Greedy Decoding}
We sketch the correctness argument for greedy decoding (the setting used throughout our experiments). The tree attention mask guarantees that for any node \(u\), the target logits at \(u\) are computed from exactly the same conditioning context as in sequential decoding along the root-to-\(u\) path. DynaTree commits a drafted token \emph{only if} it equals the target greedy argmax under that context. Therefore, every committed token matches the token that greedy decoding with \(M_T\) would produce at that position. The cache rollback-and-rebuild step ensures the subsequent iteration starts from an identical KV state. Consequently, DynaTree generates exactly the same token sequence as greedy decoding with the target model, while reducing the number of expensive target-model forward passes by verifying many candidate tokens in parallel.

\subsection{Complexity Discussion}
Let \(n=|\mathcal{N}|\le N_{\max}\) be the number of drafted nodes. Drafting requires \(O(n)\) one-token forward passes of the draft model (with cache reuse across expansions). Verification requires a single target-model forward pass over \(n\) tokens with a structured attention mask. Dynamic pruning reduces \(n\) in uncertain regions by discarding low-probability branches, improving the trade-off between draft overhead and verification parallelism.


\section{Experiments}
\label{experiments}

\subsection{Experimental Setup}
\paragraph{Models.}
We evaluate DynaTree using models from the Pythia family~\cite{pythia}. Our target model \(M_T\) is Pythia-2.8B (2.8B parameters) and our draft model \(M_D\) is Pythia-70M (70M parameters). Throughout, we use deterministic greedy decoding so the generated sequence is uniquely determined by the model and prefix.

\paragraph{Hardware and software.}
All experiments run on a single NVIDIA GPU with sufficient memory to host both models. We implement DynaTree in PyTorch~\cite{pytorch} on top of HuggingFace Transformers~\cite{transformers}. Across methods, we reuse KV caches where appropriate and synchronize GPU execution for timing to ensure a consistent measurement protocol.

\paragraph{Workloads and data preprocessing.}
We report results on WikiText-2~\citep{merity2016pointer} and PG-19~\citep{rae2019compressive}. For each sampled prompt, we generate \(T\) new tokens with greedy decoding; unless stated otherwise, \(T=1500\). To control prefill cost, prompts are truncated to a maximum length \(L_{\max}\) (\(L_{\max}=800\) for WikiText-2 and \(L_{\max}=1000\) for PG-19). We evaluate \(N=10\) prompts and discard the first \(W=2\) runs as warmup, reporting mean and standard deviation over the remaining runs. Appendix~\ref{app:exp-config} summarizes the common configurations.

\subsection{Evaluation Metrics}
We use \textbf{throughput} (tokens/s) as the primary metric, computed as \(T\) divided by the wall-clock decoding time (excluding warmup). We report \textbf{speedup} relative to autoregressive decoding, \(\mathrm{speedup}=\mathrm{TPS}/\mathrm{TPS}_{\mathrm{AR}}\), under the same dataset and prompt-length cap. To characterize verification efficiency, we report the \textbf{acceptance rate} \(a\) (fraction of drafted tokens matching the target model's greedy predictions under the corresponding conditioning contexts) and the average \textbf{tokens per iteration} \(\bar{L}\) (tokens committed per verification round). For tree-based methods, we additionally report the \textbf{average committed path length} \(\bar{\ell}\) (mean depth of the greedy-consistent committed path before the bonus token). For latency, we include \textbf{time-to-first-token} (TTFT) and \textbf{time-per-output-token} (TPOT), averaged over prompts.

\subsection{Baselines}
We compare against the following baselines under identical greedy decoding settings:
\begin{itemize}
  \item \textbf{Autoregressive (AR):} Standard greedy decoding with the target model, serving as the performance baseline.
  \item \textbf{Linear speculative decoding:} A linear-chain speculative decoder that drafts \(K\) tokens with the draft model and verifies them with the target model in parallel, committing the longest greedy-consistent prefix~\citep{leviathan2023fast}.
  \item \textbf{Fixed Tree:} A static tree speculative decoder with fixed depth \(D\), fixed branching factor \(B\), and node budget \(N_{\max}\), representing a non-adaptive tree baseline.
  \item \textbf{DynaTree:} Our full method that augments the fixed-tree backbone with (i)~\emph{Dynamic Breadth}, (ii)~\emph{Dynamic Depth}, and (iii)~\emph{History Adaptation}. We ablate these components in Section~\ref{sec:ablation}.
\end{itemize}

\subsection{Main Results}
\label{main-results}
Table~\ref{tab:main-results} reports end-to-end throughput on WikiText-2 and PG-19 for \(T=1500\). DynaTree achieves the highest throughput on both datasets, improving over autoregressive decoding, linear speculative decoding, and a fixed-tree baseline. On WikiText-2, DynaTree reaches 219.5~t/s and outperforms a tuned static tree (200.7~t/s), indicating that allocating verification budget according to local model uncertainty can improve the draft--verify trade-off beyond a fixed configuration.

\begin{table}[tbp]
\centering
\caption{\textbf{Main results (\(T=1500\)).} Throughput (t/s) and speedup relative to autoregressive decoding on WikiText-2 and PG-19. Values are mean\(\pm\)std over prompts (excluding warmup).}
\label{tab:main-results}
\small
\begin{tabular}{lcccc}
\toprule
Method & \multicolumn{2}{c}{WikiText-2} & \multicolumn{2}{c}{PG-19} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& Throughput (t/s) & Speedup & Throughput (t/s) & Speedup \\
\midrule
AR & 133.4\(\pm\)0.5 & 1.00\(\times\) & 133.7\(\pm\)0.7 & 1.00\(\times\) \\
Linear Spec (\(K=8\)) & 196.1\(\pm\)37.8 & 1.47\(\times\) & 154.1\(\pm\)25.4 & 1.15\(\times\) \\
Fixed Tree (\(D=8,B=3,\tau=0.1\)) & 200.7\(\pm\)41.7 & 1.50\(\times\) & 183.7\(\pm\)24.8 & 1.37\(\times\) \\
\textbf{DynaTree} & \textbf{219.5\(\pm\)22.2} & \textbf{1.64\(\times\)} & \textbf{192.1\(\pm\)34.2} & \textbf{1.44\(\times\)} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.80\linewidth]{../figures/main_results_bars.pdf}
  \caption{\textbf{Throughput and speedup across datasets (\(T=1500\)).} Each method is shown with two bars (WikiText-2 vs.\ PG-19). DynaTree consistently improves over autoregressive and linear speculative decoding on both datasets.}
  \label{fig:main-results}
\end{figure}

Figure~\ref{fig:main-results} visualizes throughput and speedup. Linear speculative decoding can attain high acceptance when the draft closely matches the target; however, a mismatch truncates the reusable prefix and wastes the remaining drafted tokens in the chain. Tree-based drafting mitigates this failure mode by exploring multiple continuations in parallel and committing the longest greedy-consistent prefix. DynaTree further concentrates the node budget on high-confidence regions while limiting expansion in uncertain regions, improving throughput across both datasets.

\paragraph{Latency breakdown analysis.}
Table~\ref{tab:latency-metrics} reports TTFT and TPOT for \(T=1500\). Across datasets, speculative decoding reduces TTFT relative to autoregressive decoding by amortizing verification over multiple tokens, and DynaTree achieves the lowest TPOT by committing longer prefixes per verification step.

\begin{table}[tbp]
\centering
\caption{\textbf{Latency metrics (\(T=1500\)).} We report TTFT (latency to first output token) and TPOT (average per-token latency) on WikiText-2 and PG-19. Values are mean\(\pm\)std over prompts (excluding warmup).}
\label{tab:latency-metrics}
\small
\begin{tabular}{lcccc}
\toprule
Method & \multicolumn{2}{c}{WikiText-2} & \multicolumn{2}{c}{PG-19} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& TTFT (ms) & TPOT (ms) & TTFT (ms) & TPOT (ms) \\
\midrule
AR & 18.9\(\pm\)6.1 & 7.48\(\pm\)0.02 & 20.0\(\pm\)4.5 & 7.46\(\pm\)0.04 \\
Linear Spec (\(K=8\)) & 14.6\(\pm\)4.0 & 5.32\(\pm\)1.24 & 10.5\(\pm\)1.9 & 6.71\(\pm\)1.42 \\
Fixed Tree (\(D=8,B=3,\tau=0.1\)) & 14.6\(\pm\)4.2 & 5.30\(\pm\)1.63 & 10.4\(\pm\)0.4 & 5.54\(\pm\)0.75 \\
\textbf{DynaTree} & 14.4\(\pm\)4.0 & \textbf{4.59\(\pm\)0.47} & 9.9\(\pm\)0.6 & \textbf{5.38\(\pm\)1.03} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Verification efficiency.}
To contextualize throughput gains, Table~\ref{tab:verification-efficiency} reports auxiliary verification statistics: tokens committed per verification round (\(\bar{L}\)), average committed path length (\(\bar{\ell}\)), and the number of verification rounds required to generate \(T=1500\) tokens. Tree-based methods commit longer prefixes per step and thus require fewer rounds than autoregressive decoding; DynaTree further reduces the number of rounds by adapting the draft structure to recent verification outcomes.

\begin{table}[tbp]
\centering
\caption{\textbf{Verification efficiency metrics (\(T=1500\)).} We report acceptance rate (\textbf{Accept.}), tokens committed per verification iteration (\(\bar{L}\)), average committed path length before the bonus token (\(\bar{\ell}\)), and the total number of verification iterations (\#Iter.) needed to generate \(T\) tokens. Due to the bonus-token convention in tree verification, \textbf{Accept.} can slightly exceed 100\% for tree-based methods. Values are mean across prompts (excluding warmup).}
\label{tab:verification-efficiency}
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccccccc}
\toprule
Method & \multicolumn{4}{c}{WikiText-2} & \multicolumn{4}{c}{PG-19} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
& Accept. & \(\bar{L}\) & \(\bar{\ell}\) & \#Iter. & Accept. & \(\bar{L}\) & \(\bar{\ell}\) & \#Iter. \\
\midrule
AR & -- & 1.00 & -- & 1500 & -- & 1.00 & -- & 1500 \\
Linear Spec & 88.2\% & 6.82 & 7.05 & 220 & 92.5\% & 4.56 & 4.63 & 329 \\
Fixed Tree & 71.0\% & 6.79 & 7.10 & 221 & 82.9\% & 5.75 & 5.80 & 261 \\
\textbf{DynaTree} & \textbf{102.2\%} & \textbf{7.08} & \textbf{7.15} & \textbf{212} & \textbf{92.2\%} & \textbf{6.17} & \textbf{6.45} & \textbf{243} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Discussion.}
Two trends help explain the observed speedups. First, compared with linear drafting, tree-based methods are less sensitive to early mismatches: when the draft model diverges, alternative branches can still contain a greedy-consistent continuation, which increases the expected committed prefix length per verification step. Second, compared with a fixed tree, DynaTree allocates more of the node budget to regions where the draft model is confident while curtailing wasteful expansion in uncertain regions. This adaptivity improves verification efficiency (Table~\ref{tab:verification-efficiency}) and translates into higher end-to-end throughput (Table~\ref{tab:main-results}).

\subsection{Ablation Study}
\label{sec:ablation}
We provide a progressive ablation on WikiText-2 under the same \(T=1500\) setting as the main benchmark, isolating the contribution of each adaptive component relative to a fixed-tree backbone. In this regime, adapting breadth alone may introduce control overhead without reliably increasing the committed prefix length, whereas dynamic depth control more directly increases per-iteration progress and yields the dominant throughput gain. Table~\ref{tab:ablation} therefore reports the combined effect of \emph{Dynamic Breadth \& Depth}, followed by the additional improvement from \emph{History Adaptation}, which stabilizes the configuration by reacting to recent verification outcomes.

\begin{table}[tbp]
\centering
\caption{\textbf{Ablation-style progressive comparison on WikiText-2 (\(T=1500\)).} We compare a fixed tree (\(D_0=5,B=2\)) with variants that add Dynamic Breadth \& Depth and History Adaptation. Speedup is computed relative to the autoregressive baseline.}
\label{tab:ablation}
\small
\begin{tabular}{lccc}
    \toprule
Variant & Throughput (t/s) & Speedup & \(\Delta\) vs Fixed \\
    \midrule
Fixed Tree (\(D_0=5,B=2\)) & 188.0\(\pm\)16.5 & 1.42\(\times\) & 0.0\% \\
+ Dynamic Breadth \& Depth & 213.2\(\pm\)26.1 & 1.61\(\times\) & +13.4\% \\
\textbf{+ History Adaptation} & \textbf{218.5\(\pm\)22.2} & \textbf{1.65\(\times\)} & \textbf{+16.2\%} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.85\linewidth]{../figures/ablation_progression.pdf}
  \caption{\textbf{Ablation progression on WikiText-2 (\(T=1500\)).} Each panel combines two complementary metrics. Left: throughput (bars) alongside the number of verification iterations (\#Iter.). Right: per-iteration progress (\(\bar{L}\), tokens/iter) alongside acceptance rate (separate axis). Dynamic Breadth \& Depth increases per-step progress and reduces iterations; History Adaptation further improves per-step progress and iteration count.}
  \label{fig:ablation-progression}
\end{figure}

\paragraph{Interpretation.}
Dynamic depth control primarily improves throughput by increasing per-iteration progress (\(\bar{L}\)) and reducing the number of verification rounds required to reach \(T\) tokens. History Adaptation then refines the draft aggressiveness based on recent outcomes, stabilizing verification efficiency and yielding a further throughput gain without increasing peak memory (Appendix~\ref{app:memory}).

\subsection{Sequence Length Scaling}
\label{sec:length-scaling}
We evaluate how performance varies with generation length \(T\) on WikiText-2, comparing autoregressive decoding, linear speculative decoding, a fixed tree baseline, and DynaTree. Figure~\ref{fig:length-scaling} summarizes both end-to-end throughput and auxiliary verification statistics across \(T\).

\begin{figure}[tbp]
  \centering
  \includegraphics[width=\linewidth]{../figures/length_scaling_fourpanel.pdf}
  \caption{\textbf{Sequence length scaling on WikiText-2.} (a) Throughput (tokens/s) as a function of \(T\) for AR, linear speculative decoding, a fixed tree baseline, and DynaTree. (b--d) Auxiliary verification statistics: tokens per iteration (\(\bar{L}\)), average committed path length (\(\bar{\ell}\)), and acceptance rate.}
  \label{fig:length-scaling}
\end{figure}

\paragraph{Analysis.}
Across \(T\), DynaTree maintains strong throughput by sustaining a larger committed prefix per verification round while keeping acceptance rates stable. In contrast, linear speculation is more sensitive to occasional early mismatches: as the generation proceeds, a single rejection can truncate the reusable drafted chain and reduce effective progress per round. The efficiency view (right panel) highlights how changes in \(\bar{L}\), \(\bar{\ell}\), and acceptance jointly determine throughput, providing a mechanistic explanation of the observed scaling curves.

\paragraph{Additional analyses.}
We place supplementary parameter sensitivity analyses in Appendix~\ref{app:additional-analyses}.


\section{Conclusion}
We presented DynaTree, a greedy-consistent tree-based speculative decoding method that drafts a candidate token tree with a lightweight model and verifies all nodes in a single target-model pass using a structured attention mask. DynaTree adaptively allocates the verification budget via dynamic breadth and depth control and stabilizes the configuration through history-based adjustment. Across WikiText-2 and PG-19 at \(T=1500\), DynaTree improves throughput and reduces per-token latency relative to autoregressive decoding, linear speculative decoding, and a fixed-tree baseline by committing longer greedy-consistent prefixes per verification step. Future work includes evaluating broader serving regimes (e.g., longer contexts, different prompt lengths and batch sizes) and reducing system overhead via kernel-level optimizations and hardware-aware tree construction.

\FloatBarrier % ensure all floats are placed before references
\bibliographystyle{unsrtnat}
\bibliography{references}

\appendix
% Avoid duplicate hyperref anchors when float counters reset/behave differently in the appendix.
% (pdfTeX warning: destination with the same identifier has been already used)
\makeatletter
\renewcommand{\theHfigure}{\theHsection.\arabic{figure}}
\renewcommand{\theHtable}{\theHsection.\arabic{table}}
\makeatother

\section{Experimental Configuration Details}
\label{app:exp-config}
\paragraph{Common settings.}
Unless stated otherwise, we use WikiText-2 prompts, truncate the prompt length to \(L_{\max}=800\), and generate \(T\) new tokens with greedy decoding. We evaluate \(N=10\) prompts and discard the first \(W=2\) runs as warmup. Reported values are mean and standard deviation over the remaining runs.
\paragraph{PG-19 setting.}
For PG-19, we use the same model pair and greedy decoding, with a maximum prompt length \(L_{\max}=1000\).
\paragraph{Fixed-tree baseline.}
For the static-tree baseline, we use depth \(D=5\), branching factor \(B=2\), and a node budget \(N_{\max}=256\). We additionally report a fixed-tree hyperparameter sweep under the paper protocol in Appendix~\ref{app:additional-analyses} to verify that this baseline is reasonably tuned.
\paragraph{DynaTree.}
Unless stated otherwise, the adaptive configuration uses base depth \(D_0=5\), maximum depth \(D_{\max}=8\), branch bounds \((B_{\min},B_{\max})=(1,3)\), and confidence thresholds \((\tau_h,\tau_\ell)=(0.9,0.4)\). History Adaptation updates a small subset of these parameters based on recent verification outcomes.

\section{Additional Experimental Analyses}
\label{app:additional-analyses}
\paragraph{Speedup computation.}
Some auxiliary result files include a placeholder \texttt{speedup} field. Throughout this appendix, we compute speedup as the ratio between the method throughput and the corresponding autoregressive throughput under the same setting.

\subsection{Parameter Sensitivity}
We study the sensitivity of DynaTree to key drafting hyperparameters on WikiText-2 at \(T=1500\) using a comprehensive sweep that varies confidence thresholds, branch bounds, depth ranges, and selected cross-combinations. Since the sweep does not enumerate a full Cartesian grid for every parameter pair, we visualize thresholds as a sparse 2D scatter and use mean\(\pm\)std plots for breadth and depth.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../figures/parameter_sensitivity.pdf}
\caption{\textbf{Sensitivity of DynaTree to drafting hyperparameters (WikiText-2, \(T=1500\)).} \textbf{(a)} Threshold sweep shown as a sparse 2D scatter over \((\tau_h,\tau_\ell)\), colored by throughput (tokens/s). \textbf{(b--c)} Breadth and depth sweeps shown as throughput mean\(\pm\)std across tested pair configurations. Horizontal lines denote the autoregressive and fixed-tree baselines measured in the same sweep run.}
  \label{fig:parameter-sensitivity}
\end{figure}

\subsection{Fixed-tree Hyperparameter Sweep}
We perform a fixed-tree hyperparameter sweep under the same evaluation protocol as our main benchmark to ensure the static-tree baseline is reasonably tuned. Figure~\ref{fig:fixed-tree-sweep} summarizes the sweep over depth \(D\), branching factor \(B\), and pruning threshold \(\tau\) on WikiText-2 at \(T=1500\).

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../figures/fixed_tree_sweep.pdf}
  \caption{\textbf{Fixed-tree hyperparameter sweep under the paper protocol (WikiText-2, \(T=1500\)).} Speedup is computed relative to autoregressive decoding under the same setting. Line plots use widened y-axis ranges to emphasize robustness across depth, branching, and threshold settings.}
  \label{fig:fixed-tree-sweep}
\end{figure}

\subsection{Prompt Length Sensitivity}
We evaluate the impact of input context length by varying the maximum prompt length \(L_{\max}\) on WikiText-2 under the same \(T=1500\) generation setting as the main benchmark. Figure~\ref{fig:prompt-length-sensitivity} reports throughput and speedup as functions of \(L_{\max}\), illustrating how speculative decoding performance changes as prefill cost increases.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../figures/prompt_length_sensitivity.pdf}
  \caption{\textbf{Prompt length sensitivity on WikiText-2 (\(T=1500\)).} Throughput and speedup (relative to AR at the same \(L_{\max}\)) as functions of the maximum prompt length.}
  \label{fig:prompt-length-sensitivity}
\end{figure}

\section{Memory Footprint Analysis}
\label{app:memory}
An important practical consideration for speculative decoding methods is their memory overhead. Table~\ref{tab:memory-footprint} reports peak GPU memory consumption across methods on PG-19 and WikiText-2 during the \(T=1500\) main benchmark. Overall, speculative decoding introduces a modest additional peak allocation (about 3\% on average in this setup) to maintain the draft-model KV cache and intermediate verification structures.

\begin{table}[t]
\centering
\caption{\textbf{Peak GPU memory consumption comparison (\(T=1500\)).} Peak GPU memory (MB) during generation on PG-19 and WikiText-2 with Pythia-2.8B and Pythia-70M. Relative change is computed against the autoregressive baseline using the average of the two datasets.}
\label{tab:memory-footprint}
\small
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Peak Memory (MB)} & \multirow{2}{*}{Average (MB)} & \multirow{2}{*}{Rel. Change} \\
\cmidrule(lr){2-3}
& PG-19 & WikiText-2 & & \\
\midrule
AR (baseline) & 6157.7 & 6110.0 & 6133.9 & 0.00\% \\
Linear Spec (\(K=5\)) & 6360.5 & 6313.8 & 6337.2 & +3.31\% \\
Fixed Tree (\(D=5,B=2\)) & 6358.3 & 6312.8 & 6335.5 & +3.29\% \\
\textbf{DynaTree} & \textbf{6359.4} & \textbf{6316.1} & \textbf{6337.7} & \textbf{+3.32\%} \\
\bottomrule
\end{tabular}
\end{table}

\section{DynaTree Iteration Pseudocode}
\label{app:algo}
\begin{algorithm}[t]
\caption{\textbf{DynaTree: one iteration (greedy-consistent).}}
\label{alg:dynatree}
\small
\begin{algorithmic}[1]
\Require Prefix tokens \(x_{1:t}\); target KV cache \(\mathcal{K}_T\); prefix next-token logits \(\mathbf{s}_{\text{last}}\);
branch bounds \(B_{\min}\le B_{\mathrm{mid}}\le B_{\max}\); confidence thresholds \(0<\tau_\ell<\tau_h<1\); base depth \(D_0\); max depth \(D_{\max}\); depth thresholds \(0<\rho_{\mathrm{stop}}<\rho_{\mathrm{deep}}<1\); pruning threshold \(\tau\); node budget \(N_{\max}\); history window \(W\).
\Ensure Committed tokens \(y_{t+1:t+L}\) and updated \(\mathcal{K}_T\).

\State \(\ell \gets \Call{SeqLen}{\mathcal{K}_T}\) \Comment{record prefix cache length}
\State \(\mathcal{T} \gets \Call{DraftTree}{x_{1:t}, B_{\min}, B_{\mathrm{mid}}, B_{\max}, \tau_\ell, \tau_h, D_0, D_{\max}, \rho_{\mathrm{stop}}, \rho_{\mathrm{deep}}, \tau, N_{\max}}\)
\State \(\mathbf{z}_{1:n} \gets \Call{BFSFlatten}{\mathcal{T}}\); \(\mathbf{A} \gets \Call{TreeMask}{\mathcal{T}, \ell}\) \Comment{prefix + ancestors only}
\State \(\mathbf{s}_{1:n} \gets M_T(\mathbf{z}_{1:n}; \mathbf{A}, \mathcal{K}_T)\); \(\hat{\mathbf{y}} \gets \arg\max \mathbf{s}_{1:n}\)
\State \(y_{t+1:t+L} \gets \Call{SelectCommit}{\mathcal{T}, \hat{\mathbf{y}}, \mathbf{s}_{\text{last}}}\)
\State \(\mathcal{K}_T \gets \Call{Crop}{\mathcal{K}_T,\ell}\); \(\mathcal{K}_T \gets M_T(y_{t+1:t+L}; \mathcal{K}_T)\) \Comment{rollback + rebuild}
\State \Call{UpdateHistory}{$y_{t+1:t+L}, \mathcal{T}, W$}; \Call{Adjust}{$\tau_\ell,\tau_h,D_0$} \Comment{historical adjustment}
\State \Return \(y_{t+1:t+L}\)

\Statex
\Function{\textproc{DraftTree}}{$x_{1:t}, B_{\min}, B_{\mathrm{mid}}, B_{\max}, \tau_\ell, \tau_h, D_0, D_{\max}, \rho_{\mathrm{stop}}, \rho_{\mathrm{deep}}, \tau, N_{\max}$}
  \Comment{Draft a candidate token tree with adaptive branching, dynamic depth control, and pruning under a node budget.}
  \State Run \(M_D\) on \(x_{1:t}\); let \(u_0\) be the \(\top 1\) token; initialize \(\mathcal{T}\) with root \(u_0\)
  \State \(\mathcal{A} \gets \{u_0\}\) \Comment{active frontier}
  \While{$|\mathcal{A}|>0$ \textbf{and} $|\mathcal{T}|<N_{\max}$}
    \State Pop an element \(u\) from \(\mathcal{A}\)
    \If{$\exp(\bar{\ell}_u) < \tau$} \State \textbf{continue} \EndIf \Comment{probability-threshold pruning}
    \If{$d(u)\ge D_{\max}$} \State \textbf{continue} \EndIf
    \If{$\exp(\bar{\ell}_u) < \rho_{\mathrm{stop}}$} \State \textbf{continue} \EndIf \Comment{early stopping}
    \If{$d(u)\ge D_0$ \textbf{and} $\exp(\bar{\ell}_u) < \rho_{\mathrm{deep}}$} \State \textbf{continue} \EndIf \Comment{depth gating}
    \State Do one cached step of \(M_D\) from \(u\); compute confidence \(c(u)=\max \mathrm{softmax}(\mathbf{h}(u))\)
    \State Set \(B(u)\gets B_{\min}\) if \(c(u)\ge\tau_h\), \(B_{\max}\) if \(c(u)<\tau_\ell\), else \(B_{\mathrm{mid}}\)
    \State Take \(\top B(u)\) next-token candidates; add each child \(v\) to \(\mathcal{T}\) and push \(v\) into \(\mathcal{A}\) until \(N_{\max}\)
  \EndWhile
  \State \Return \(\mathcal{T}\)
\EndFunction

\Statex
\Function{\textproc{SelectCommit}}{$\mathcal{T}, \hat{\mathbf{y}}, \mathbf{s}_{\text{last}}$}
  \Comment{Select the longest greedy-consistent path (plus one bonus token).}
  \State \(first \gets \arg\max \mathbf{s}_{\text{last}}\)
  \State Find the longest path \(P\) where root token \(=\) \(first\), and for each edge \((u\!\rightarrow\!v)\), token\((v)=\hat{\mathbf{y}}[\text{pos}(u)]\)
  \If{$P=\emptyset$}
    \State \Return \([first]\)
  \Else
    \State \(y \gets\) tokens on \(P\)
    \State Append one bonus token \(\hat{\mathbf{y}}[\text{pos}(\text{last}(P))]\)
    \State \Return \(y\)
  \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\end{document}